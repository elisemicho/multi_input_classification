############# train @ Fri Mar 23 16:51:15 CET 2018 GPUS=3  HOST=ssaling11 PWD=/home/michon/projects/VarDial2018/to_export/multi_input_modular
Loading data
Data Configurations loaded
Loading data
(13806, 8)
(1509, 8)
EGY    3085
LAV    2940
NOR    2866
GLF    2707
MSA    2208
Name: Class, dtype: int64
NOR    346
LAV    327
EGY    297
MSA    280
GLF    259
Name: Class, dtype: int64
Loading vocabularies
Words
48244 48244
Phones
45 45
39 39
61 61
51 51
Generating ids
Preprocessing data
Padding character sequences
(13806, 6830)
Padding phone sequences
(13806, 5885) (13806, 7329) (13806, 6436) (13806, 6837)
Turning labels in one-hot vectors
(13806, 5)
Taking ready-made acoustic embeddings
(13806, 600)
Padding character sequences
(1509, 6830)
Padding phone sequences
(1509, 5885) (1509, 7329) (1509, 6436) (1509, 6837)
Turning labels in one-hot vectors
(1509, 5)
Taking ready-made acoustic embeddings
(1509, 600)
MultiInputCharCNN Configurations loaded
Building the model
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
sent_input (InputLayer)         (None, 6830)         0                                            
__________________________________________________________________________________________________
phone_CZ_input (InputLayer)     (None, 5885)         0                                            
__________________________________________________________________________________________________
phone_EN_input (InputLayer)     (None, 7329)         0                                            
__________________________________________________________________________________________________
phone_HU_input (InputLayer)     (None, 6436)         0                                            
__________________________________________________________________________________________________
phone_RU_input (InputLayer)     (None, 6837)         0                                            
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 6830, 16)     1616        sent_input[0][0]                 
__________________________________________________________________________________________________
embedding_2 (Embedding)         (None, 5885, 16)     736         phone_CZ_input[0][0]             
__________________________________________________________________________________________________
embedding_3 (Embedding)         (None, 7329, 16)     640         phone_EN_input[0][0]             
__________________________________________________________________________________________________
embedding_4 (Embedding)         (None, 6436, 16)     992         phone_HU_input[0][0]             
__________________________________________________________________________________________________
embedding_5 (Embedding)         (None, 6837, 16)     832         phone_RU_input[0][0]             
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 6830, 16)     0           embedding_1[0][0]                
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 5885, 16)     0           embedding_2[0][0]                
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 7329, 16)     0           embedding_3[0][0]                
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 6436, 16)     0           embedding_4[0][0]                
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 6837, 16)     0           embedding_5[0][0]                
__________________________________________________________________________________________________
zero_padding1d_1 (ZeroPadding1D (None, 6834, 16)     0           dropout_1[0][0]                  
__________________________________________________________________________________________________
zero_padding1d_2 (ZeroPadding1D (None, 6838, 16)     0           dropout_1[0][0]                  
__________________________________________________________________________________________________
zero_padding1d_3 (ZeroPadding1D (None, 5889, 16)     0           dropout_2[0][0]                  
__________________________________________________________________________________________________
zero_padding1d_4 (ZeroPadding1D (None, 5893, 16)     0           dropout_2[0][0]                  
__________________________________________________________________________________________________
zero_padding1d_5 (ZeroPadding1D (None, 7333, 16)     0           dropout_3[0][0]                  
__________________________________________________________________________________________________
zero_padding1d_6 (ZeroPadding1D (None, 7337, 16)     0           dropout_3[0][0]                  
__________________________________________________________________________________________________
zero_padding1d_7 (ZeroPadding1D (None, 6440, 16)     0           dropout_4[0][0]                  
__________________________________________________________________________________________________
zero_padding1d_8 (ZeroPadding1D (None, 6444, 16)     0           dropout_4[0][0]                  
__________________________________________________________________________________________________
zero_padding1d_9 (ZeroPadding1D (None, 6841, 16)     0           dropout_5[0][0]                  
__________________________________________________________________________________________________
zero_padding1d_10 (ZeroPadding1 (None, 6845, 16)     0           dropout_5[0][0]                  
__________________________________________________________________________________________________
conv1d_1 (Conv1D)               (None, 6834, 8)      392         zero_padding1d_1[0][0]           
__________________________________________________________________________________________________
conv1d_2 (Conv1D)               (None, 6838, 8)      648         zero_padding1d_2[0][0]           
__________________________________________________________________________________________________
conv1d_3 (Conv1D)               (None, 5889, 8)      392         zero_padding1d_3[0][0]           
__________________________________________________________________________________________________
conv1d_4 (Conv1D)               (None, 5893, 8)      648         zero_padding1d_4[0][0]           
__________________________________________________________________________________________________
conv1d_5 (Conv1D)               (None, 7333, 8)      392         zero_padding1d_5[0][0]           
__________________________________________________________________________________________________
conv1d_6 (Conv1D)               (None, 7337, 8)      648         zero_padding1d_6[0][0]           
__________________________________________________________________________________________________
conv1d_7 (Conv1D)               (None, 6440, 8)      392         zero_padding1d_7[0][0]           
__________________________________________________________________________________________________
conv1d_8 (Conv1D)               (None, 6444, 8)      648         zero_padding1d_8[0][0]           
__________________________________________________________________________________________________
conv1d_9 (Conv1D)               (None, 6841, 8)      392         zero_padding1d_9[0][0]           
__________________________________________________________________________________________________
conv1d_10 (Conv1D)              (None, 6845, 8)      648         zero_padding1d_10[0][0]          
__________________________________________________________________________________________________
dropout_6 (Dropout)             (None, 6834, 8)      0           conv1d_1[0][0]                   
__________________________________________________________________________________________________2018-03-23 16:51:26.186943: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-03-23 16:51:26.433360: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335
pciBusID: 0000:04:00.0
totalMemory: 7.92GiB freeMemory: 7.81GiB
2018-03-23 16:51:26.433389: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:04:00.0, compute capability: 6.1)

dropout_7 (Dropout)             (None, 6838, 8)      0           conv1d_2[0][0]                   
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 5889, 8)      0           conv1d_3[0][0]                   
__________________________________________________________________________________________________
dropout_9 (Dropout)             (None, 5893, 8)      0           conv1d_4[0][0]                   
__________________________________________________________________________________________________
dropout_10 (Dropout)            (None, 7333, 8)      0           conv1d_5[0][0]                   
__________________________________________________________________________________________________
dropout_11 (Dropout)            (None, 7337, 8)      0           conv1d_6[0][0]                   
__________________________________________________________________________________________________
dropout_12 (Dropout)            (None, 6440, 8)      0           conv1d_7[0][0]                   
__________________________________________________________________________________________________
dropout_13 (Dropout)            (None, 6444, 8)      0           conv1d_8[0][0]                   
__________________________________________________________________________________________________
dropout_14 (Dropout)            (None, 6841, 8)      0           conv1d_9[0][0]                   
__________________________________________________________________________________________________
dropout_15 (Dropout)            (None, 6845, 8)      0           conv1d_10[0][0]                  
__________________________________________________________________________________________________
global_max_pooling1d_1 (GlobalM (None, 8)            0           dropout_6[0][0]                  
__________________________________________________________________________________________________
global_max_pooling1d_2 (GlobalM (None, 8)            0           dropout_7[0][0]                  
__________________________________________________________________________________________________
global_max_pooling1d_3 (GlobalM (None, 8)            0           dropout_8[0][0]                  
__________________________________________________________________________________________________
global_max_pooling1d_4 (GlobalM (None, 8)            0           dropout_9[0][0]                  
__________________________________________________________________________________________________
global_max_pooling1d_5 (GlobalM (None, 8)            0           dropout_10[0][0]                 
__________________________________________________________________________________________________
global_max_pooling1d_6 (GlobalM (None, 8)            0           dropout_11[0][0]                 
__________________________________________________________________________________________________
global_max_pooling1d_7 (GlobalM (None, 8)            0           dropout_12[0][0]                 
__________________________________________________________________________________________________
global_max_pooling1d_8 (GlobalM (None, 8)            0           dropout_13[0][0]                 
__________________________________________________________________________________________________
global_max_pooling1d_9 (GlobalM (None, 8)            0           dropout_14[0][0]                 
__________________________________________________________________________________________________
global_max_pooling1d_10 (Global (None, 8)            0           dropout_15[0][0]                 
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 80)           0           global_max_pooling1d_1[0][0]     
                                                                 global_max_pooling1d_2[0][0]     
                                                                 global_max_pooling1d_3[0][0]     
                                                                 global_max_pooling1d_4[0][0]     
                                                                 global_max_pooling1d_5[0][0]     
                                                                 global_max_pooling1d_6[0][0]     
                                                                 global_max_pooling1d_7[0][0]     
                                                                 global_max_pooling1d_8[0][0]     
                                                                 global_max_pooling1d_9[0][0]     
                                                                 global_max_pooling1d_10[0][0]    
__________________________________________________________________________________________________
embed_input (InputLayer)        (None, 600)          0                                            
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 680)          0           concatenate_1[0][0]              
                                                                 embed_input[0][0]                
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 16)           10896       concatenate_2[0][0]              
__________________________________________________________________________________________________
dropout_16 (Dropout)            (None, 16)           0           dense_1[0][0]                    
__________________________________________________________________________________________________
l_out (Dense)                   (None, 5)            85          dropout_16[0][0]                 
==================================================================================================
Total params: 20,997
Trainable params: 20,997
Non-trainable params: 0
__________________________________________________________________________________________________
Training Configurations loaded
Training the model
no checkpoints available !
Train on 13806 samples, validate on 1509 samples
Epoch 1/15

  128/13806 [..............................] - ETA: 2:18 - loss: 1.9748 - categorical_accuracy: 0.1797
  256/13806 [..............................] - ETA: 1:21 - loss: 1.9253 - categorical_accuracy: 0.2266
  384/13806 [..............................] - ETA: 1:02 - loss: 1.8884 - categorical_accuracy: 0.2682
  512/13806 [>.............................] - ETA: 52s - loss: 1.8593 - categorical_accuracy: 0.2852 
  640/13806 [>.............................] - ETA: 47s - loss: 1.8333 - categorical_accuracy: 0.2969
  768/13806 [>.............................] - ETA: 43s - loss: 1.8041 - categorical_accuracy: 0.3190
  896/13806 [>.............................] - ETA: 40s - loss: 1.7791 - categorical_accuracy: 0.3404
 1024/13806 [=>............................] - ETA: 38s - loss: 1.7587 - categorical_accuracy: 0.3525
 1152/13806 [=>............................] - ETA: 36s - loss: 1.7437 - categorical_accuracy: 0.3620
 1280/13806 [=>............................] - ETA: 34s - loss: 1.7217 - categorical_accuracy: 0.3828
 1408/13806 [==>...........................] - ETA: 33s - loss: 1.6913 - categorical_accuracy: 0.4048
 1536/13806 [==>...........................] - ETA: 32s - loss: 1.6701 - categorical_accuracy: 0.4173
 1664/13806 [==>...........................] - ETA: 31s - loss: 1.6547 - categorical_accuracy: 0.4267
 1792/13806 [==>...........................] - ETA: 30s - loss: 1.6407 - categorical_accuracy: 0.4375
 1920/13806 [===>..........................] - ETA: 29s - loss: 1.6247 - categorical_accuracy: 0.4448
 2048/13806 [===>..........................] - ETA: 29s - loss: 1.6036 - categorical_accuracy: 0.4580
 2176/13806 [===>..........................] - ETA: 28s - loss: 1.5839 - categorical_accuracy: 0.4706
 2304/13806 [====>.........................] - ETA: 27s - loss: 1.5656 - categorical_accuracy: 0.4783
 2432/13806 [====>.........................] - ETA: 27s - loss: 1.5498 - categorical_accuracy: 0.4860
 2560/13806 [====>.........................] - ETA: 26s - loss: 1.5365 - categorical_accuracy: 0.4914
 2688/13806 [====>.........................] - ETA: 26s - loss: 1.5201 - categorical_accuracy: 0.5015
 2816/13806 [=====>........................] - ETA: 25s - loss: 1.5086 - categorical_accuracy: 0.5060
 2944/13806 [=====>........................] - ETA: 25s - loss: 1.4906 - categorical_accuracy: 0.5163
 3072/13806 [=====>........................] - ETA: 24s - loss: 1.4804 - categorical_accuracy: 0.5212
 3200/13806 [=====>........................] - ETA: 24s - loss: 1.4670 - categorical_accuracy: 0.5291
 3328/13806 [======>.......................] - ETA: 23s - loss: 1.4534 - categorical_accuracy: 0.5382
 3456/13806 [======>.......................] - ETA: 23s - loss: 1.4437 - categorical_accuracy: 0.5411
 3584/13806 [======>.......................] - ETA: 23s - loss: 1.4330 - categorical_accuracy: 0.5472
 3712/13806 [=======>......................] - ETA: 22s - loss: 1.4215 - categorical_accuracy: 0.5528
 3840/13806 [=======>......................] - ETA: 22s - loss: 1.4129 - categorical_accuracy: 0.5555
 3968/13806 [=======>......................] - ETA: 21s - loss: 1.4039 - categorical_accuracy: 0.5595
 4096/13806 [=======>......................] - ETA: 21s - loss: 1.3925 - categorical_accuracy: 0.5645
 4224/13806 [========>.....................] - ETA: 21s - loss: 1.3840 - categorical_accuracy: 0.5670
 4352/13806 [========>.....................] - ETA: 20s - loss: 1.3716 - categorical_accuracy: 0.5728
 4480/13806 [========>.....................] - ETA: 20s - loss: 1.3607 - categorical_accuracy: 0.5775
 4608/13806 [=========>....................] - ETA: 20s - loss: 1.3491 - categorical_accuracy: 0.5827
 4736/13806 [=========>....................] - ETA: 19s - loss: 1.3385 - categorical_accuracy: 0.5878
 4864/13806 [=========>....................] - ETA: 19s - loss: 1.3296 - categorical_accuracy: 0.5905
 4992/13806 [=========>....................] - ETA: 19s - loss: 1.3229 - categorical_accuracy: 0.5944
 5120/13806 [==========>...................] - ETA: 18s - loss: 1.3144 - categorical_accuracy: 0.5965
 5248/13806 [==========>...................] - ETA: 18s - loss: 1.3038 - categorical_accuracy: 0.6002
 5376/13806 [==========>...................] - ETA: 18s - loss: 1.2952 - categorical_accuracy: 0.6042
 5504/13806 [==========>...................] - ETA: 17s - loss: 1.2856 - categorical_accuracy: 0.6090
 5632/13806 [===========>..................] - ETA: 17s - loss: 1.2767 - categorical_accuracy: 0.6122
 5760/13806 [===========>..................] - ETA: 17s - loss: 1.2708 - categorical_accuracy: 0.6146
 5888/13806 [===========>..................] - ETA: 17s - loss: 1.2645 - categorical_accuracy: 0.6160
 6016/13806 [============>.................] - ETA: 16s - loss: 1.2574 - categorical_accuracy: 0.6184
 6144/13806 [============>.................] - ETA: 16s - loss: 1.2505 - categorical_accuracy: 0.6203
 6272/13806 [============>.................] - ETA: 16s - loss: 1.2460 - categorical_accuracy: 0.6221
 6400/13806 [============>.................] - ETA: 15s - loss: 1.2377 - categorical_accuracy: 0.6253
 6528/13806 [=============>................] - ETA: 15s - loss: 1.2310 - categorical_accuracy: 0.6278
 6656/13806 [=============>................] - ETA: 15s - loss: 1.2255 - categorical_accuracy: 0.6295
 6784/13806 [=============>................] - ETA: 14s - loss: 1.2201 - categorical_accuracy: 0.6319
 6912/13806 [==============>...............] - ETA: 14s - loss: 1.2146 - categorical_accuracy: 0.6340
 7040/13806 [==============>...............] - ETA: 14s - loss: 1.2095 - categorical_accuracy: 0.6359
 7168/13806 [==============>...............] - ETA: 14s - loss: 1.2035 - categorical_accuracy: 0.6385
 7296/13806 [==============>...............] - ETA: 13s - loss: 1.1974 - categorical_accuracy: 0.6408
 7424/13806 [===============>..............] - ETA: 13s - loss: 1.1914 - categorical_accuracy: 0.6432
 7552/13806 [===============>..............] - ETA: 13s - loss: 1.1865 - categorical_accuracy: 0.6449
 7680/13806 [===============>..............] - ETA: 12s - loss: 1.1815 - categorical_accuracy: 0.6464
 7808/13806 [===============>..............] - ETA: 12s - loss: 1.1785 - categorical_accuracy: 0.6465
 7936/13806 [================>.............] - ETA: 12s - loss: 1.1731 - categorical_accuracy: 0.6482
 8064/13806 [================>.............] - ETA: 12s - loss: 1.1675 - categorical_accuracy: 0.6500
 8192/13806 [================>.............] - ETA: 11s - loss: 1.1639 - categorical_accuracy: 0.6509
 8320/13806 [=================>............] - ETA: 11s - loss: 1.1613 - categorical_accuracy: 0.6518
 8448/13806 [=================>............] - ETA: 11s - loss: 1.1571 - categorical_accuracy: 0.6528
 8576/13806 [=================>............] - ETA: 11s - loss: 1.1529 - categorical_accuracy: 0.6537
 8704/13806 [=================>............] - ETA: 10s - loss: 1.1476 - categorical_accuracy: 0.6562
 8832/13806 [==================>...........] - ETA: 10s - loss: 1.1433 - categorical_accuracy: 0.6578
 8960/13806 [==================>...........] - ETA: 10s - loss: 1.1393 - categorical_accuracy: 0.6592
 9088/13806 [==================>...........] - ETA: 9s - loss: 1.1354 - categorical_accuracy: 0.6604 
 9216/13806 [===================>..........] - ETA: 9s - loss: 1.1306 - categorical_accuracy: 0.6618
 9344/13806 [===================>..........] - ETA: 9s - loss: 1.1271 - categorical_accuracy: 0.6626
 9472/13806 [===================>..........] - ETA: 9s - loss: 1.1228 - categorical_accuracy: 0.6643
 9600/13806 [===================>..........] - ETA: 8s - loss: 1.1181 - categorical_accuracy: 0.6656
 9728/13806 [====================>.........] - ETA: 8s - loss: 1.1141 - categorical_accuracy: 0.6670
 9856/13806 [====================>.........] - ETA: 8s - loss: 1.1087 - categorical_accuracy: 0.6690
 9984/13806 [====================>.........] - ETA: 8s - loss: 1.1059 - categorical_accuracy: 0.6700
10112/13806 [====================>.........] - ETA: 7s - loss: 1.1019 - categorical_accuracy: 0.6718
10240/13806 [=====================>........] - ETA: 7s - loss: 1.0968 - categorical_accuracy: 0.6737
10368/13806 [=====================>........] - ETA: 7s - loss: 1.0926 - categorical_accuracy: 0.6753
10496/13806 [=====================>........] - ETA: 6s - loss: 1.0888 - categorical_accuracy: 0.6768
10624/13806 [======================>.......] - ETA: 6s - loss: 1.0861 - categorical_accuracy: 0.6783
10752/13806 [======================>.......] - ETA: 6s - loss: 1.0820 - categorical_accuracy: 0.6793
10880/13806 [======================>.......] - ETA: 6s - loss: 1.0787 - categorical_accuracy: 0.6802
11008/13806 [======================>.......] - ETA: 5s - loss: 1.0758 - categorical_accuracy: 0.6804
11136/13806 [=======================>......] - ETA: 5s - loss: 1.0728 - categorical_accuracy: 0.6813
11264/13806 [=======================>......] - ETA: 5s - loss: 1.0698 - categorical_accuracy: 0.6824
11392/13806 [=======================>......] - ETA: 5s - loss: 1.0665 - categorical_accuracy: 0.6836
11520/13806 [========================>.....] - ETA: 4s - loss: 1.0627 - categorical_accuracy: 0.6846
11648/13806 [========================>.....] - ETA: 4s - loss: 1.0591 - categorical_accuracy: 0.6860
11776/13806 [========================>.....] - ETA: 4s - loss: 1.0558 - categorical_accuracy: 0.6868
11904/13806 [========================>.....] - ETA: 3s - loss: 1.0532 - categorical_accuracy: 0.6876
12032/13806 [=========================>....] - ETA: 3s - loss: 1.0502 - categorical_accuracy: 0.6883
12160/13806 [=========================>....] - ETA: 3s - loss: 1.0467 - categorical_accuracy: 0.6898
12288/13806 [=========================>....] - ETA: 3s - loss: 1.0428 - categorical_accuracy: 0.6913
12416/13806 [=========================>....] - ETA: 2s - loss: 1.0397 - categorical_accuracy: 0.6923
12544/13806 [==========================>...] - ETA: 2s - loss: 1.0369 - categorical_accuracy: 0.6933
12672/13806 [==========================>...] - ETA: 2s - loss: 1.0340 - categorical_accuracy: 0.6941
12800/13806 [==========================>...] - ETA: 2s - loss: 1.0304 - categorical_accuracy: 0.6952
12928/13806 [===========================>..] - ETA: 1s - loss: 1.0279 - categorical_accuracy: 0.6956
13056/13806 [===========================>..] - ETA: 1s - loss: 1.0252 - categorical_accuracy: 0.6962
13184/13806 [===========================>..] - ETA: 1s - loss: 1.0232 - categorical_accuracy: 0.6967
13312/13806 [===========================>..] - ETA: 1s - loss: 1.0210 - categorical_accuracy: 0.6967
13440/13806 [============================>.] - ETA: 0s - loss: 1.0177 - categorical_accuracy: 0.6977
13568/13806 [============================>.] - ETA: 0s - loss: 1.0153 - categorical_accuracy: 0.6982
13696/13806 [============================>.] - ETA: 0s - loss: 1.0136 - categorical_accuracy: 0.6990
13806/13806 [==============================] - 30s 2ms/step - loss: 1.0122 - categorical_accuracy: 0.6993 - val_loss: 1.4474 - val_categorical_accuracy: 0.5017

Epoch 00001: val_categorical_accuracy improved from -inf to 0.50166, saving model to results/vardial2018/multi_input_middle_with_dropout/model_weights.hdf5
Epoch 2/15

  128/13806 [..............................] - ETA: 26s - loss: 0.6777 - categorical_accuracy: 0.8047
  256/13806 [..............................] - ETA: 26s - loss: 0.7546 - categorical_accuracy: 0.7617
  384/13806 [..............................] - ETA: 26s - loss: 0.7363 - categorical_accuracy: 0.7682
  512/13806 [>.............................] - ETA: 26s - loss: 0.7402 - categorical_accuracy: 0.7656
  640/13806 [>.............................] - ETA: 25s - loss: 0.7298 - categorical_accuracy: 0.7828
  768/13806 [>.............................] - ETA: 25s - loss: 0.7292 - categorical_accuracy: 0.7747
  896/13806 [>.............................] - ETA: 25s - loss: 0.7206 - categorical_accuracy: 0.7812
 1024/13806 [=>............................] - ETA: 25s - loss: 0.7169 - categorical_accuracy: 0.7803
 1152/13806 [=>............................] - ETA: 24s - loss: 0.7281 - categorical_accuracy: 0.7708
 1280/13806 [=>............................] - ETA: 24s - loss: 0.7256 - categorical_accuracy: 0.7680
 1408/13806 [==>...........................] - ETA: 24s - loss: 0.7325 - categorical_accuracy: 0.7642
 1536/13806 [==>...........................] - ETA: 24s - loss: 0.7285 - categorical_accuracy: 0.7689
 1664/13806 [==>...........................] - ETA: 23s - loss: 0.7287 - categorical_accuracy: 0.7704
 1792/13806 [==>...........................] - ETA: 23s - loss: 0.7315 - categorical_accuracy: 0.7729
 1920/13806 [===>..........................] - ETA: 23s - loss: 0.7349 - categorical_accuracy: 0.7661
 2048/13806 [===>..........................] - ETA: 23s - loss: 0.7381 - categorical_accuracy: 0.7656
 2176/13806 [===>..........................] - ETA: 22s - loss: 0.7365 - categorical_accuracy: 0.7675
 2304/13806 [====>.........................] - ETA: 22s - loss: 0.7350 - categorical_accuracy: 0.7713
 2432/13806 [====>.........................] - ETA: 22s - loss: 0.7323 - categorical_accuracy: 0.7726
 2560/13806 [====>.........................] - ETA: 22s - loss: 0.7391 - categorical_accuracy: 0.7691
 2688/13806 [====>.........................] - ETA: 21s - loss: 0.7329 - categorical_accuracy: 0.7723
 2816/13806 [=====>........................] - ETA: 21s - loss: 0.7356 - categorical_accuracy: 0.7695
 2944/13806 [=====>........................] - ETA: 21s - loss: 0.7342 - categorical_accuracy: 0.7673
 3072/13806 [=====>........................] - ETA: 21s - loss: 0.7345 - categorical_accuracy: 0.7682
 3200/13806 [=====>........................] - ETA: 20s - loss: 0.7282 - categorical_accuracy: 0.7703
 3328/13806 [======>.......................] - ETA: 20s - loss: 0.7290 - categorical_accuracy: 0.7680
 3456/13806 [======>.......................] - ETA: 20s - loss: 0.7255 - categorical_accuracy: 0.7697
 3584/13806 [======>.......................] - ETA: 20s - loss: 0.7234 - categorical_accuracy: 0.7676
 3712/13806 [=======>......................] - ETA: 19s - loss: 0.7237 - categorical_accuracy: 0.7678
 3840/13806 [=======>......................] - ETA: 19s - loss: 0.7216 - categorical_accuracy: 0.7682
 3968/13806 [=======>......................] - ETA: 19s - loss: 0.7211 - categorical_accuracy: 0.7676
 4096/13806 [=======>......................] - ETA: 19s - loss: 0.7199 - categorical_accuracy: 0.7666
 4224/13806 [========>.....................] - ETA: 19s - loss: 0.7173 - categorical_accuracy: 0.7673
 4352/13806 [========>.....................] - ETA: 18s - loss: 0.7175 - categorical_accuracy: 0.7668
 4480/13806 [========>.....................] - ETA: 18s - loss: 0.7165 - categorical_accuracy: 0.7679
 4608/13806 [=========>....................] - ETA: 18s - loss: 0.7131 - categorical_accuracy: 0.7689
 4736/13806 [=========>....................] - ETA: 18s - loss: 0.7115 - categorical_accuracy: 0.7703
 4864/13806 [=========>....................] - ETA: 17s - loss: 0.7120 - categorical_accuracy: 0.7697
 4992/13806 [=========>....................] - ETA: 17s - loss: 0.7118 - categorical_accuracy: 0.7690
 5120/13806 [==========>...................] - ETA: 17s - loss: 0.7103 - categorical_accuracy: 0.7701
 5248/13806 [==========>...................] - ETA: 17s - loss: 0.7095 - categorical_accuracy: 0.7685
 5376/13806 [==========>...................] - ETA: 16s - loss: 0.7083 - categorical_accuracy: 0.7675
 5504/13806 [==========>...................] - ETA: 16s - loss: 0.7062 - categorical_accuracy: 0.7685
 5632/13806 [===========>..................] - ETA: 16s - loss: 0.7068 - categorical_accuracy: 0.7679
 5760/13806 [===========>..................] - ETA: 16s - loss: 0.7054 - categorical_accuracy: 0.7684
 5888/13806 [===========>..................] - ETA: 15s - loss: 0.7036 - categorical_accuracy: 0.7690
 6016/13806 [============>.................] - ETA: 15s - loss: 0.7051 - categorical_accuracy: 0.7675
 6144/13806 [============>.................] - ETA: 15s - loss: 0.7047 - categorical_accuracy: 0.7669
 6272/13806 [============>.................] - ETA: 15s - loss: 0.7036 - categorical_accuracy: 0.7671
 6400/13806 [============>.................] - ETA: 14s - loss: 0.7017 - categorical_accuracy: 0.7681
 6528/13806 [=============>................] - ETA: 14s - loss: 0.7026 - categorical_accuracy: 0.7675
 6656/13806 [=============>................] - ETA: 14s - loss: 0.7001 - categorical_accuracy: 0.7686
 6784/13806 [=============>................] - ETA: 13s - loss: 0.6984 - categorical_accuracy: 0.7690
 6912/13806 [==============>...............] - ETA: 13s - loss: 0.6968 - categorical_accuracy: 0.7698
 7040/13806 [==============>...............] - ETA: 13s - loss: 0.6959 - categorical_accuracy: 0.7705
 7168/13806 [==============>...............] - ETA: 13s - loss: 0.6938 - categorical_accuracy: 0.7711
 7296/13806 [==============>...............] - ETA: 12s - loss: 0.6934 - categorical_accuracy: 0.7711
 7424/13806 [===============>..............] - ETA: 12s - loss: 0.6903 - categorical_accuracy: 0.7725
 7552/13806 [===============>..............] - ETA: 12s - loss: 0.6894 - categorical_accuracy: 0.7726
 7680/13806 [===============>..............] - ETA: 12s - loss: 0.6872 - categorical_accuracy: 0.7733
 7808/13806 [===============>..............] - ETA: 11s - loss: 0.6852 - categorical_accuracy: 0.7738
 7936/13806 [================>.............] - ETA: 11s - loss: 0.6845 - categorical_accuracy: 0.7744
 8064/13806 [================>.............] - ETA: 11s - loss: 0.6834 - categorical_accuracy: 0.7749
 8192/13806 [================>.............] - ETA: 11s - loss: 0.6815 - categorical_accuracy: 0.7754
 8320/13806 [=================>............] - ETA: 10s - loss: 0.6794 - categorical_accuracy: 0.7766
 8448/13806 [=================>............] - ETA: 10s - loss: 0.6779 - categorical_accuracy: 0.7777
 8576/13806 [=================>............] - ETA: 10s - loss: 0.6769 - categorical_accuracy: 0.7781
 8704/13806 [=================>............] - ETA: 10s - loss: 0.6760 - categorical_accuracy: 0.7786
 8832/13806 [==================>...........] - ETA: 9s - loss: 0.6750 - categorical_accuracy: 0.7789 
 8960/13806 [==================>...........] - ETA: 9s - loss: 0.6733 - categorical_accuracy: 0.7796
 9088/13806 [==================>...........] - ETA: 9s - loss: 0.6735 - categorical_accuracy: 0.7795
 9216/13806 [===================>..........] - ETA: 9s - loss: 0.6736 - categorical_accuracy: 0.7793
 9344/13806 [===================>..........] - ETA: 8s - loss: 0.6718 - categorical_accuracy: 0.7804
 9472/13806 [===================>..........] - ETA: 8s - loss: 0.6711 - categorical_accuracy: 0.7814
 9600/13806 [===================>..........] - ETA: 8s - loss: 0.6697 - categorical_accuracy: 0.7823
 9728/13806 [====================>.........] - ETA: 8s - loss: 0.6688 - categorical_accuracy: 0.7825
 9856/13806 [====================>.........] - ETA: 7s - loss: 0.6662 - categorical_accuracy: 0.7841
 9984/13806 [====================>.........] - ETA: 7s - loss: 0.6646 - categorical_accuracy: 0.7851
10112/13806 [====================>.........] - ETA: 7s - loss: 0.6622 - categorical_accuracy: 0.7859
10240/13806 [=====================>........] - ETA: 7s - loss: 0.6617 - categorical_accuracy: 0.7856
10368/13806 [=====================>........] - ETA: 6s - loss: 0.6601 - categorical_accuracy: 0.7864
10496/13806 [=====================>........] - ETA: 6s - loss: 0.6586 - categorical_accuracy: 0.7873
10624/13806 [======================>.......] - ETA: 6s - loss: 0.6564 - categorical_accuracy: 0.7884
10752/13806 [======================>.......] - ETA: 6s - loss: 0.6569 - categorical_accuracy: 0.7889
10880/13806 [======================>.......] - ETA: 5s - loss: 0.6560 - categorical_accuracy: 0.7890
11008/13806 [======================>.......] - ETA: 5s - loss: 0.6542 - categorical_accuracy: 0.7894
11136/13806 [=======================>......] - ETA: 5s - loss: 0.6525 - categorical_accuracy: 0.7903
11264/13806 [=======================>......] - ETA: 5s - loss: 0.6515 - categorical_accuracy: 0.7909
11392/13806 [=======================>......] - ETA: 4s - loss: 0.6509 - categorical_accuracy: 0.7916
11520/13806 [========================>.....] - ETA: 4s - loss: 0.6489 - categorical_accuracy: 0.7926
11648/13806 [========================>.....] - ETA: 4s - loss: 0.6477 - categorical_accuracy: 0.7933
11776/13806 [========================>.....] - ETA: 4s - loss: 0.6469 - categorical_accuracy: 0.7936
11904/13806 [========================>.....] - ETA: 3s - loss: 0.6464 - categorical_accuracy: 0.7936
12032/13806 [=========================>....] - ETA: 3s - loss: 0.6450 - categorical_accuracy: 0.7943
12160/13806 [=========================>....] - ETA: 3s - loss: 0.6443 - categorical_accuracy: 0.7946
12288/13806 [=========================>....] - ETA: 3s - loss: 0.6432 - categorical_accuracy: 0.7949
12416/13806 [=========================>....] - ETA: 2s - loss: 0.6423 - categorical_accuracy: 0.7953
12544/13806 [==========================>...] - ETA: 2s - loss: 0.6413 - categorical_accuracy: 0.7958
12672/13806 [==========================>...] - ETA: 2s - loss: 0.6396 - categorical_accuracy: 0.7965
12800/13806 [==========================>...] - ETA: 2s - loss: 0.6387 - categorical_accuracy: 0.7970
12928/13806 [===========================>..] - ETA: 1s - loss: 0.6376 - categorical_accuracy: 0.7970
13056/13806 [===========================>..] - ETA: 1s - loss: 0.6376 - categorical_accuracy: 0.7969
13184/13806 [===========================>..] - ETA: 1s - loss: 0.6388 - categorical_accuracy: 0.7970
13312/13806 [===========================>..] - ETA: 0s - loss: 0.6370 - categorical_accuracy: 0.7978
13440/13806 [============================>.] - ETA: 0s - loss: 0.6369 - categorical_accuracy: 0.7978
13568/13806 [============================>.] - ETA: 0s - loss: 0.6359 - categorical_accuracy: 0.7982
13696/13806 [============================>.] - ETA: 0s - loss: 0.6349 - categorical_accuracy: 0.7986
13806/13806 [==============================] - 28s 2ms/step - loss: 0.6342 - categorical_accuracy: 0.7986 - val_loss: 1.3380 - val_categorical_accuracy: 0.5421

Epoch 00002: val_categorical_accuracy improved from 0.50166 to 0.54208, saving model to results/vardial2018/multi_input_middle_with_dropout/model_weights.hdf5
Epoch 3/15

  128/13806 [..............................] - ETA: 26s - loss: 0.4925 - categorical_accuracy: 0.8438
  256/13806 [..............................] - ETA: 27s - loss: 0.5157 - categorical_accuracy: 0.8125
  384/13806 [..............................] - ETA: 27s - loss: 0.5279 - categorical_accuracy: 0.8151
  512/13806 [>.............................] - ETA: 28s - loss: 0.5357 - categorical_accuracy: 0.8164
  640/13806 [>.............................] - ETA: 27s - loss: 0.5513 - categorical_accuracy: 0.8187
  768/13806 [>.............................] - ETA: 27s - loss: 0.5450 - categorical_accuracy: 0.8255
  896/13806 [>.............................] - ETA: 26s - loss: 0.5391 - categorical_accuracy: 0.8315
 1024/13806 [=>............................] - ETA: 26s - loss: 0.5300 - categorical_accuracy: 0.8350
 1152/13806 [=>............................] - ETA: 26s - loss: 0.5237 - categorical_accuracy: 0.8394
 1280/13806 [=>............................] - ETA: 26s - loss: 0.5278 - categorical_accuracy: 0.8406
 1408/13806 [==>...........................] - ETA: 25s - loss: 0.5235 - categorical_accuracy: 0.8409
 1536/13806 [==>...........................] - ETA: 25s - loss: 0.5208 - categorical_accuracy: 0.8431
 1664/13806 [==>...........................] - ETA: 24s - loss: 0.5225 - categorical_accuracy: 0.8395
 1792/13806 [==>...........................] - ETA: 24s - loss: 0.5239 - categorical_accuracy: 0.8387
 1920/13806 [===>..........................] - ETA: 24s - loss: 0.5271 - categorical_accuracy: 0.8328
 2048/13806 [===>..........................] - ETA: 23s - loss: 0.5312 - categorical_accuracy: 0.8301
 2176/13806 [===>..........................] - ETA: 23s - loss: 0.5358 - categorical_accuracy: 0.8290
 2304/13806 [====>.........................] - ETA: 23s - loss: 0.5360 - categorical_accuracy: 0.8294
 2432/13806 [====>.........................] - ETA: 23s - loss: 0.5361 - categorical_accuracy: 0.8302
 2560/13806 [====>.........................] - ETA: 22s - loss: 0.5361 - categorical_accuracy: 0.8273
 2688/13806 [====>.........................] - ETA: 22s - loss: 0.5351 - categorical_accuracy: 0.8289
 2816/13806 [=====>........................] - ETA: 22s - loss: 0.5352 - categorical_accuracy: 0.8271
 2944/13806 [=====>........................] - ETA: 22s - loss: 0.5374 - categorical_accuracy: 0.8247
 3072/13806 [=====>........................] - ETA: 21s - loss: 0.5396 - categorical_accuracy: 0.8245
 3200/13806 [=====>........................] - ETA: 21s - loss: 0.5365 - categorical_accuracy: 0.8272
 3328/13806 [======>.......................] - ETA: 21s - loss: 0.5319 - categorical_accuracy: 0.8299
 3456/13806 [======>.......................] - ETA: 21s - loss: 0.5325 - categorical_accuracy: 0.8304
 3584/13806 [======>.......................] - ETA: 20s - loss: 0.5355 - categorical_accuracy: 0.8292
 3712/13806 [=======>......................] - ETA: 20s - loss: 0.5333 - categorical_accuracy: 0.8292
 3840/13806 [=======>......................] - ETA: 20s - loss: 0.5327 - categorical_accuracy: 0.8299
 3968/13806 [=======>......................] - ETA: 20s - loss: 0.5308 - categorical_accuracy: 0.8309
 4096/13806 [=======>......................] - ETA: 19s - loss: 0.5309 - categorical_accuracy: 0.8303
 4224/13806 [========>.....................] - ETA: 19s - loss: 0.5291 - categorical_accuracy: 0.8317
 4352/13806 [========>.....................] - ETA: 19s - loss: 0.5267 - categorical_accuracy: 0.8330
 4480/13806 [========>.....................] - ETA: 19s - loss: 0.5251 - categorical_accuracy: 0.8337
 4608/13806 [=========>....................] - ETA: 18s - loss: 0.5295 - categorical_accuracy: 0.8342
 4736/13806 [=========>....................] - ETA: 18s - loss: 0.5309 - categorical_accuracy: 0.8338
 4864/13806 [=========>....................] - ETA: 18s - loss: 0.5283 - categorical_accuracy: 0.8343
 4992/13806 [=========>....................] - ETA: 18s - loss: 0.5300 - categorical_accuracy: 0.8337
 5120/13806 [==========>...................] - ETA: 17s - loss: 0.5306 - categorical_accuracy: 0.8322
 5248/13806 [==========>...................] - ETA: 17s - loss: 0.5286 - categorical_accuracy: 0.8335
 5376/13806 [==========>...................] - ETA: 17s - loss: 0.5287 - categorical_accuracy: 0.8333
 5504/13806 [==========>...................] - ETA: 16s - loss: 0.5280 - categorical_accuracy: 0.8339
 5632/13806 [===========>..................] - ETA: 16s - loss: 0.5271 - categorical_accuracy: 0.8340
 5760/13806 [===========>..................] - ETA: 16s - loss: 0.5293 - categorical_accuracy: 0.8323
 5888/13806 [===========>..................] - ETA: 16s - loss: 0.5288 - categorical_accuracy: 0.8325
 6016/13806 [============>.................] - ETA: 15s - loss: 0.5311 - categorical_accuracy: 0.8313
 6144/13806 [============>.................] - ETA: 15s - loss: 0.5309 - categorical_accuracy: 0.8317
 6272/13806 [============>.................] - ETA: 15s - loss: 0.5326 - categorical_accuracy: 0.8307
 6400/13806 [============>.................] - ETA: 15s - loss: 0.5308 - categorical_accuracy: 0.8314
 6528/13806 [=============>................] - ETA: 14s - loss: 0.5317 - categorical_accuracy: 0.8312
 6656/13806 [=============>................] - ETA: 14s - loss: 0.5317 - categorical_accuracy: 0.8316
 6784/13806 [=============>................] - ETA: 14s - loss: 0.5322 - categorical_accuracy: 0.8308
 6912/13806 [==============>...............] - ETA: 13s - loss: 0.5313 - categorical_accuracy: 0.8316
 7040/13806 [==============>...............] - ETA: 13s - loss: 0.5322 - categorical_accuracy: 0.8308
 7168/13806 [==============>...............] - ETA: 13s - loss: 0.5326 - categorical_accuracy: 0.8312
 7296/13806 [==============>...............] - ETA: 13s - loss: 0.5310 - categorical_accuracy: 0.8317
 7424/13806 [===============>..............] - ETA: 12s - loss: 0.5298 - categorical_accuracy: 0.8327
 7552/13806 [===============>..............] - ETA: 12s - loss: 0.5293 - categorical_accuracy: 0.8328
 7680/13806 [===============>..............] - ETA: 12s - loss: 0.5307 - categorical_accuracy: 0.8326
 7808/13806 [===============>..............] - ETA: 12s - loss: 0.5300 - categorical_accuracy: 0.8330
 7936/13806 [================>.............] - ETA: 11s - loss: 0.5285 - categorical_accuracy: 0.8335
 8064/13806 [================>.............] - ETA: 11s - loss: 0.5276 - categorical_accuracy: 0.8338
 8192/13806 [================>.............] - ETA: 11s - loss: 0.5274 - categorical_accuracy: 0.8339
 8320/13806 [=================>............] - ETA: 11s - loss: 0.5255 - categorical_accuracy: 0.8347
 8448/13806 [=================>............] - ETA: 10s - loss: 0.5251 - categorical_accuracy: 0.8344
 8576/13806 [=================>............] - ETA: 10s - loss: 0.5242 - categorical_accuracy: 0.8354
 8704/13806 [=================>............] - ETA: 10s - loss: 0.5233 - categorical_accuracy: 0.8356
 8832/13806 [==================>...........] - ETA: 10s - loss: 0.5228 - categorical_accuracy: 0.8354
 8960/13806 [==================>...........] - ETA: 9s - loss: 0.5221 - categorical_accuracy: 0.8357 
 9088/13806 [==================>...........] - ETA: 9s - loss: 0.5212 - categorical_accuracy: 0.8355
 9216/13806 [===================>..........] - ETA: 9s - loss: 0.5205 - categorical_accuracy: 0.8351
 9344/13806 [===================>..........] - ETA: 9s - loss: 0.5196 - categorical_accuracy: 0.8353
 9472/13806 [===================>..........] - ETA: 8s - loss: 0.5201 - categorical_accuracy: 0.8347
 9600/13806 [===================>..........] - ETA: 8s - loss: 0.5198 - categorical_accuracy: 0.8351
 9728/13806 [====================>.........] - ETA: 8s - loss: 0.5187 - categorical_accuracy: 0.8357
 9856/13806 [====================>.........] - ETA: 8s - loss: 0.5177 - categorical_accuracy: 0.8360
 9984/13806 [====================>.........] - ETA: 7s - loss: 0.5185 - categorical_accuracy: 0.8356
10112/13806 [====================>.........] - ETA: 7s - loss: 0.5186 - categorical_accuracy: 0.8354
10240/13806 [=====================>........] - ETA: 7s - loss: 0.5171 - categorical_accuracy: 0.8364
10368/13806 [=====================>........] - ETA: 6s - loss: 0.5158 - categorical_accuracy: 0.8370
10496/13806 [=====================>........] - ETA: 6s - loss: 0.5156 - categorical_accuracy: 0.8371
10624/13806 [======================>.......] - ETA: 6s - loss: 0.5145 - categorical_accuracy: 0.8376
10752/13806 [======================>.......] - ETA: 6s - loss: 0.5153 - categorical_accuracy: 0.8378
10880/13806 [======================>.......] - ETA: 5s - loss: 0.5158 - categorical_accuracy: 0.8375
11008/13806 [======================>.......] - ETA: 5s - loss: 0.5155 - categorical_accuracy: 0.8380
11136/13806 [=======================>......] - ETA: 5s - loss: 0.5161 - categorical_accuracy: 0.8380
11264/13806 [=======================>......] - ETA: 5s - loss: 0.5159 - categorical_accuracy: 0.8377
11392/13806 [=======================>......] - ETA: 4s - loss: 0.5157 - categorical_accuracy: 0.8374
11520/13806 [========================>.....] - ETA: 4s - loss: 0.5160 - categorical_accuracy: 0.8372
11648/13806 [========================>.....] - ETA: 4s - loss: 0.5153 - categorical_accuracy: 0.8376
11776/13806 [========================>.....] - ETA: 4s - loss: 0.5149 - categorical_accuracy: 0.8381
11904/13806 [========================>.....] - ETA: 3s - loss: 0.5148 - categorical_accuracy: 0.8379
12032/13806 [=========================>....] - ETA: 3s - loss: 0.5158 - categorical_accuracy: 0.8374
12160/13806 [=========================>....] - ETA: 3s - loss: 0.5159 - categorical_accuracy: 0.8375
12288/13806 [=========================>....] - ETA: 3s - loss: 0.5153 - categorical_accuracy: 0.8376
12416/13806 [=========================>....] - ETA: 2s - loss: 0.5144 - categorical_accuracy: 0.8377
12544/13806 [==========================>...] - ETA: 2s - loss: 0.5140 - categorical_accuracy: 0.8382
12672/13806 [==========================>...] - ETA: 2s - loss: 0.5140 - categorical_accuracy: 0.8381
12800/13806 [==========================>...] - ETA: 2s - loss: 0.5130 - categorical_accuracy: 0.8386
12928/13806 [===========================>..] - ETA: 1s - loss: 0.5115 - categorical_accuracy: 0.8391
13056/13806 [===========================>..] - ETA: 1s - loss: 0.5116 - categorical_accuracy: 0.8392
13184/13806 [===========================>..] - ETA: 1s - loss: 0.5115 - categorical_accuracy: 0.8390
13312/13806 [===========================>..] - ETA: 0s - loss: 0.5114 - categorical_accuracy: 0.8387
13440/13806 [============================>.] - ETA: 0s - loss: 0.5120 - categorical_accuracy: 0.8385
13568/13806 [============================>.] - ETA: 0s - loss: 0.5119 - categorical_accuracy: 0.8381
13696/13806 [============================>.] - ETA: 0s - loss: 0.5117 - categorical_accuracy: 0.8381
13806/13806 [==============================] - 29s 2ms/step - loss: 0.5121 - categorical_accuracy: 0.8375 - val_loss: 1.3320 - val_categorical_accuracy: 0.5434

Epoch 00003: val_categorical_accuracy improved from 0.54208 to 0.54341, saving model to results/vardial2018/multi_input_middle_with_dropout/model_weights.hdf5
Epoch 4/15

  128/13806 [..............................] - ETA: 26s - loss: 0.5045 - categorical_accuracy: 0.8281
  256/13806 [..............................] - ETA: 26s - loss: 0.4932 - categorical_accuracy: 0.8320
  384/13806 [..............................] - ETA: 26s - loss: 0.4619 - categorical_accuracy: 0.8438
  512/13806 [>.............................] - ETA: 25s - loss: 0.4463 - categorical_accuracy: 0.8555
  640/13806 [>.............................] - ETA: 25s - loss: 0.4578 - categorical_accuracy: 0.8547
  768/13806 [>.............................] - ETA: 25s - loss: 0.4584 - categorical_accuracy: 0.8529
  896/13806 [>.............................] - ETA: 25s - loss: 0.4629 - categorical_accuracy: 0.8504
 1024/13806 [=>............................] - ETA: 25s - loss: 0.4710 - categorical_accuracy: 0.8506
 1152/13806 [=>............................] - ETA: 24s - loss: 0.4748 - categorical_accuracy: 0.8524
 1280/13806 [=>............................] - ETA: 24s - loss: 0.4777 - categorical_accuracy: 0.8484
 1408/13806 [==>...........................] - ETA: 24s - loss: 0.4895 - categorical_accuracy: 0.8438
 1536/13806 [==>...........................] - ETA: 24s - loss: 0.4902 - categorical_accuracy: 0.8438
 1664/13806 [==>...........................] - ETA: 24s - loss: 0.4848 - categorical_accuracy: 0.8474
 1792/13806 [==>...........................] - ETA: 23s - loss: 0.4888 - categorical_accuracy: 0.8471
 1920/13806 [===>..........................] - ETA: 23s - loss: 0.4902 - categorical_accuracy: 0.8474
 2048/13806 [===>..........................] - ETA: 23s - loss: 0.4870 - categorical_accuracy: 0.8486
 2176/13806 [===>..........................] - ETA: 23s - loss: 0.4855 - categorical_accuracy: 0.8493
 2304/13806 [====>.........................] - ETA: 22s - loss: 0.4850 - categorical_accuracy: 0.8498
 2432/13806 [====>.........................] - ETA: 22s - loss: 0.4861 - categorical_accuracy: 0.8487
 2560/13806 [====>.........................] - ETA: 22s - loss: 0.4845 - categorical_accuracy: 0.8473
 2688/13806 [====>.........................] - ETA: 22s - loss: 0.4810 - categorical_accuracy: 0.8482
 2816/13806 [=====>........................] - ETA: 22s - loss: 0.4789 - categorical_accuracy: 0.8484
 2944/13806 [=====>........................] - ETA: 21s - loss: 0.4788 - categorical_accuracy: 0.8471
 3072/13806 [=====>........................] - ETA: 21s - loss: 0.4793 - categorical_accuracy: 0.8460
 3200/13806 [=====>........................] - ETA: 21s - loss: 0.4747 - categorical_accuracy: 0.8481
 3328/13806 [======>.......................] - ETA: 21s - loss: 0.4778 - categorical_accuracy: 0.8480
 3456/13806 [======>.......................] - ETA: 20s - loss: 0.4741 - categorical_accuracy: 0.8490
 3584/13806 [======>.......................] - ETA: 20s - loss: 0.4716 - categorical_accuracy: 0.8504
 3712/13806 [=======>......................] - ETA: 20s - loss: 0.4714 - categorical_accuracy: 0.8510
 3840/13806 [=======>......................] - ETA: 20s - loss: 0.4717 - categorical_accuracy: 0.8500
 3968/13806 [=======>......................] - ETA: 19s - loss: 0.4758 - categorical_accuracy: 0.8490
 4096/13806 [=======>......................] - ETA: 19s - loss: 0.4759 - categorical_accuracy: 0.8501
 4224/13806 [========>.....................] - ETA: 19s - loss: 0.4752 - categorical_accuracy: 0.8499
 4352/13806 [========>.....................] - ETA: 18s - loss: 0.4773 - categorical_accuracy: 0.8479
 4480/13806 [========>.....................] - ETA: 18s - loss: 0.4781 - categorical_accuracy: 0.8471
 4608/13806 [=========>....................] - ETA: 18s - loss: 0.4760 - categorical_accuracy: 0.8483
 4736/13806 [=========>....................] - ETA: 18s - loss: 0.4746 - categorical_accuracy: 0.8490
 4864/13806 [=========>....................] - ETA: 17s - loss: 0.4749 - categorical_accuracy: 0.8477
 4992/13806 [=========>....................] - ETA: 17s - loss: 0.4758 - categorical_accuracy: 0.8474
 5120/13806 [==========>...................] - ETA: 17s - loss: 0.4763 - categorical_accuracy: 0.8465
 5248/13806 [==========>...................] - ETA: 17s - loss: 0.4754 - categorical_accuracy: 0.8472
 5376/13806 [==========>...................] - ETA: 16s - loss: 0.4763 - categorical_accuracy: 0.8471
 5504/13806 [==========>...................] - ETA: 16s - loss: 0.4761 - categorical_accuracy: 0.8467
 5632/13806 [===========>..................] - ETA: 16s - loss: 0.4751 - categorical_accuracy: 0.8461
 5760/13806 [===========>..................] - ETA: 16s - loss: 0.4769 - categorical_accuracy: 0.8444
 5888/13806 [===========>..................] - ETA: 15s - loss: 0.4773 - categorical_accuracy: 0.8432
 6016/13806 [============>.................] - ETA: 15s - loss: 0.4755 - categorical_accuracy: 0.8441
 6144/13806 [============>.................] - ETA: 15s - loss: 0.4741 - categorical_accuracy: 0.8444
 6272/13806 [============>.................] - ETA: 15s - loss: 0.4735 - categorical_accuracy: 0.8447
 6400/13806 [============>.................] - ETA: 14s - loss: 0.4725 - categorical_accuracy: 0.8447
 6528/13806 [=============>................] - ETA: 14s - loss: 0.4738 - categorical_accuracy: 0.8439
 6656/13806 [=============>................] - ETA: 14s - loss: 0.4743 - categorical_accuracy: 0.8439
 6784/13806 [=============>................] - ETA: 14s - loss: 0.4755 - categorical_accuracy: 0.8433
 6912/13806 [==============>...............] - ETA: 13s - loss: 0.4758 - categorical_accuracy: 0.8432
 7040/13806 [==============>...............] - ETA: 13s - loss: 0.4739 - categorical_accuracy: 0.8440
 7168/13806 [==============>...............] - ETA: 13s - loss: 0.4730 - categorical_accuracy: 0.8443
 7296/13806 [==============>...............] - ETA: 13s - loss: 0.4709 - categorical_accuracy: 0.8450
 7424/13806 [===============>..............] - ETA: 12s - loss: 0.4715 - categorical_accuracy: 0.8442
 7552/13806 [===============>..............] - ETA: 12s - loss: 0.4708 - categorical_accuracy: 0.8445
 7680/13806 [===============>..............] - ETA: 12s - loss: 0.4704 - categorical_accuracy: 0.8445
 7808/13806 [===============>..............] - ETA: 12s - loss: 0.4687 - categorical_accuracy: 0.8457
 7936/13806 [================>.............] - ETA: 11s - loss: 0.4686 - categorical_accuracy: 0.8455
 8064/13806 [================>.............] - ETA: 11s - loss: 0.4677 - categorical_accuracy: 0.8452
 8192/13806 [================>.............] - ETA: 11s - loss: 0.4677 - categorical_accuracy: 0.8455
 8320/13806 [=================>............] - ETA: 11s - loss: 0.4658 - categorical_accuracy: 0.8465
 8448/13806 [=================>............] - ETA: 10s - loss: 0.4665 - categorical_accuracy: 0.8464
 8576/13806 [=================>............] - ETA: 10s - loss: 0.4677 - categorical_accuracy: 0.8455
 8704/13806 [=================>............] - ETA: 10s - loss: 0.4676 - categorical_accuracy: 0.8457
 8832/13806 [==================>...........] - ETA: 9s - loss: 0.4674 - categorical_accuracy: 0.8453 
 8960/13806 [==================>...........] - ETA: 9s - loss: 0.4669 - categorical_accuracy: 0.8454
 9088/13806 [==================>...........] - ETA: 9s - loss: 0.4671 - categorical_accuracy: 0.8454
 9216/13806 [===================>..........] - ETA: 9s - loss: 0.4665 - categorical_accuracy: 0.8453
 9344/13806 [===================>..........] - ETA: 8s - loss: 0.4680 - categorical_accuracy: 0.8440
 9472/13806 [===================>..........] - ETA: 8s - loss: 0.4689 - categorical_accuracy: 0.8439
 9600/13806 [===================>..........] - ETA: 8s - loss: 0.4700 - categorical_accuracy: 0.8428
 9728/13806 [====================>.........] - ETA: 8s - loss: 0.4693 - categorical_accuracy: 0.8428
 9856/13806 [====================>.........] - ETA: 7s - loss: 0.4690 - categorical_accuracy: 0.8431
 9984/13806 [====================>.........] - ETA: 7s - loss: 0.4686 - categorical_accuracy: 0.8430
10112/13806 [====================>.........] - ETA: 7s - loss: 0.4684 - categorical_accuracy: 0.8433
10240/13806 [=====================>........] - ETA: 7s - loss: 0.4686 - categorical_accuracy: 0.8432
10368/13806 [=====================>........] - ETA: 6s - loss: 0.4691 - categorical_accuracy: 0.8425
10496/13806 [=====================>........] - ETA: 6s - loss: 0.4699 - categorical_accuracy: 0.8422
10624/13806 [======================>.......] - ETA: 6s - loss: 0.4695 - categorical_accuracy: 0.8426
10752/13806 [======================>.......] - ETA: 6s - loss: 0.4691 - categorical_accuracy: 0.8424
10880/13806 [======================>.......] - ETA: 5s - loss: 0.4692 - categorical_accuracy: 0.8422
11008/13806 [======================>.......] - ETA: 5s - loss: 0.4694 - categorical_accuracy: 0.8419
11136/13806 [=======================>......] - ETA: 5s - loss: 0.4694 - categorical_accuracy: 0.8418
11264/13806 [=======================>......] - ETA: 5s - loss: 0.4689 - categorical_accuracy: 0.8422
11392/13806 [=======================>......] - ETA: 4s - loss: 0.4699 - categorical_accuracy: 0.8416
11520/13806 [========================>.....] - ETA: 4s - loss: 0.4692 - categorical_accuracy: 0.8424
11648/13806 [========================>.....] - ETA: 4s - loss: 0.4701 - categorical_accuracy: 0.8417
11776/13806 [========================>.....] - ETA: 4s - loss: 0.4711 - categorical_accuracy: 0.8414
11904/13806 [========================>.....] - ETA: 3s - loss: 0.4714 - categorical_accuracy: 0.8410
12032/13806 [=========================>....] - ETA: 3s - loss: 0.4711 - categorical_accuracy: 0.8412
12160/13806 [=========================>....] - ETA: 3s - loss: 0.4704 - categorical_accuracy: 0.8415
12288/13806 [=========================>....] - ETA: 3s - loss: 0.4697 - categorical_accuracy: 0.8416
12416/13806 [=========================>....] - ETA: 2s - loss: 0.4704 - categorical_accuracy: 0.8416
12544/13806 [==========================>...] - ETA: 2s - loss: 0.4692 - categorical_accuracy: 0.8422
12672/13806 [==========================>...] - ETA: 2s - loss: 0.4690 - categorical_accuracy: 0.8423
12800/13806 [==========================>...] - ETA: 2s - loss: 0.4694 - categorical_accuracy: 0.8418
12928/13806 [===========================>..] - ETA: 1s - loss: 0.4702 - categorical_accuracy: 0.8412
13056/13806 [===========================>..] - ETA: 1s - loss: 0.4715 - categorical_accuracy: 0.8410
13184/13806 [===========================>..] - ETA: 1s - loss: 0.4708 - categorical_accuracy: 0.8414
13312/13806 [===========================>..] - ETA: 0s - loss: 0.4705 - categorical_accuracy: 0.8419
13440/13806 [============================>.] - ETA: 0s - loss: 0.4709 - categorical_accuracy: 0.8420
13568/13806 [============================>.] - ETA: 0s - loss: 0.4715 - categorical_accuracy: 0.8418
13696/13806 [============================>.] - ETA: 0s - loss: 0.4711 - categorical_accuracy: 0.8415
13806/13806 [==============================] - 29s 2ms/step - loss: 0.4713 - categorical_accuracy: 0.8413 - val_loss: 1.3158 - val_categorical_accuracy: 0.5421

Epoch 00004: val_categorical_accuracy did not improve
Epoch 5/15

  128/13806 [..............................] - ETA: 26s - loss: 0.4580 - categorical_accuracy: 0.8438
  256/13806 [..............................] - ETA: 26s - loss: 0.4523 - categorical_accuracy: 0.8477
  384/13806 [..............................] - ETA: 26s - loss: 0.4478 - categorical_accuracy: 0.8464
  512/13806 [>.............................] - ETA: 26s - loss: 0.4571 - categorical_accuracy: 0.8301
  640/13806 [>.............................] - ETA: 25s - loss: 0.4916 - categorical_accuracy: 0.8203
  768/13806 [>.............................] - ETA: 25s - loss: 0.4735 - categorical_accuracy: 0.8320
  896/13806 [>.............................] - ETA: 25s - loss: 0.4737 - categorical_accuracy: 0.8292
 1024/13806 [=>............................] - ETA: 25s - loss: 0.4714 - categorical_accuracy: 0.8301
 1152/13806 [=>............................] - ETA: 25s - loss: 0.4690 - categorical_accuracy: 0.8299
 1280/13806 [=>............................] - ETA: 24s - loss: 0.4663 - categorical_accuracy: 0.8313
 1408/13806 [==>...........................] - ETA: 24s - loss: 0.4631 - categorical_accuracy: 0.8317
 1536/13806 [==>...........................] - ETA: 24s - loss: 0.4590 - categorical_accuracy: 0.8333
 1664/13806 [==>...........................] - ETA: 24s - loss: 0.4584 - categorical_accuracy: 0.8347
 1792/13806 [==>...........................] - ETA: 24s - loss: 0.4535 - categorical_accuracy: 0.8365
 1920/13806 [===>..........................] - ETA: 23s - loss: 0.4529 - categorical_accuracy: 0.8359
 2048/13806 [===>..........................] - ETA: 23s - loss: 0.4513 - categorical_accuracy: 0.8384
 2176/13806 [===>..........................] - ETA: 23s - loss: 0.4522 - categorical_accuracy: 0.8382
 2304/13806 [====>.........................] - ETA: 23s - loss: 0.4506 - categorical_accuracy: 0.8394
 2432/13806 [====>.........................] - ETA: 22s - loss: 0.4535 - categorical_accuracy: 0.8409
 2560/13806 [====>.........................] - ETA: 22s - loss: 0.4544 - categorical_accuracy: 0.8406
 2688/13806 [====>.........................] - ETA: 22s - loss: 0.4515 - categorical_accuracy: 0.8426
 2816/13806 [=====>........................] - ETA: 22s - loss: 0.4529 - categorical_accuracy: 0.8423
 2944/13806 [=====>........................] - ETA: 22s - loss: 0.4533 - categorical_accuracy: 0.8417
 3072/13806 [=====>........................] - ETA: 21s - loss: 0.4487 - categorical_accuracy: 0.8434
 3200/13806 [=====>........................] - ETA: 21s - loss: 0.4465 - categorical_accuracy: 0.8450
 3328/13806 [======>.......................] - ETA: 21s - loss: 0.4474 - categorical_accuracy: 0.8434
 3456/13806 [======>.......................] - ETA: 21s - loss: 0.4470 - categorical_accuracy: 0.8440
 3584/13806 [======>.......................] - ETA: 20s - loss: 0.4456 - categorical_accuracy: 0.8446
 3712/13806 [=======>......................] - ETA: 20s - loss: 0.4444 - categorical_accuracy: 0.8464
 3840/13806 [=======>......................] - ETA: 20s - loss: 0.4436 - categorical_accuracy: 0.8466
 3968/13806 [=======>......................] - ETA: 20s - loss: 0.4418 - categorical_accuracy: 0.8468
 4096/13806 [=======>......................] - ETA: 19s - loss: 0.4417 - categorical_accuracy: 0.8472
 4224/13806 [========>.....................] - ETA: 19s - loss: 0.4419 - categorical_accuracy: 0.8473
 4352/13806 [========>.....................] - ETA: 19s - loss: 0.4396 - categorical_accuracy: 0.8488
 4480/13806 [========>.....................] - ETA: 19s - loss: 0.4382 - categorical_accuracy: 0.8496
 4608/13806 [=========>....................] - ETA: 18s - loss: 0.4383 - categorical_accuracy: 0.8494
 4736/13806 [=========>....................] - ETA: 18s - loss: 0.4364 - categorical_accuracy: 0.8507
 4864/13806 [=========>....................] - ETA: 18s - loss: 0.4340 - categorical_accuracy: 0.8518
 4992/13806 [=========>....................] - ETA: 18s - loss: 0.4318 - categorical_accuracy: 0.8522
 5120/13806 [==========>...................] - ETA: 17s - loss: 0.4305 - categorical_accuracy: 0.8529
 5248/13806 [==========>...................] - ETA: 17s - loss: 0.4295 - categorical_accuracy: 0.8531
 5376/13806 [==========>...................] - ETA: 17s - loss: 0.4301 - categorical_accuracy: 0.8534
 5504/13806 [==========>...................] - ETA: 16s - loss: 0.4283 - categorical_accuracy: 0.8537
 5632/13806 [===========>..................] - ETA: 16s - loss: 0.4300 - categorical_accuracy: 0.8533
 5760/13806 [===========>..................] - ETA: 16s - loss: 0.4286 - categorical_accuracy: 0.8536
 5888/13806 [===========>..................] - ETA: 16s - loss: 0.4298 - categorical_accuracy: 0.8529
 6016/13806 [============>.................] - ETA: 15s - loss: 0.4288 - categorical_accuracy: 0.8534
 6144/13806 [============>.................] - ETA: 15s - loss: 0.4317 - categorical_accuracy: 0.8522
 6272/13806 [============>.................] - ETA: 15s - loss: 0.4315 - categorical_accuracy: 0.8522
 6400/13806 [============>.................] - ETA: 15s - loss: 0.4315 - categorical_accuracy: 0.8522
 6528/13806 [=============>................] - ETA: 14s - loss: 0.4295 - categorical_accuracy: 0.8532
 6656/13806 [=============>................] - ETA: 14s - loss: 0.4296 - categorical_accuracy: 0.8528
 6784/13806 [=============>................] - ETA: 14s - loss: 0.4307 - categorical_accuracy: 0.8524
 6912/13806 [==============>...............] - ETA: 14s - loss: 0.4297 - categorical_accuracy: 0.8534
 7040/13806 [==============>...............] - ETA: 13s - loss: 0.4316 - categorical_accuracy: 0.8531
 7168/13806 [==============>...............] - ETA: 13s - loss: 0.4317 - categorical_accuracy: 0.8532
 7296/13806 [==============>...............] - ETA: 13s - loss: 0.4310 - categorical_accuracy: 0.8533
 7424/13806 [===============>..............] - ETA: 13s - loss: 0.4312 - categorical_accuracy: 0.8530
 7552/13806 [===============>..............] - ETA: 12s - loss: 0.4302 - categorical_accuracy: 0.8535
 7680/13806 [===============>..............] - ETA: 12s - loss: 0.4313 - categorical_accuracy: 0.8535
 7808/13806 [===============>..............] - ETA: 12s - loss: 0.4299 - categorical_accuracy: 0.8540
 7936/13806 [================>.............] - ETA: 11s - loss: 0.4293 - categorical_accuracy: 0.8545
 8064/13806 [================>.............] - ETA: 11s - loss: 0.4292 - categorical_accuracy: 0.8548
 8192/13806 [================>.............] - ETA: 11s - loss: 0.4291 - categorical_accuracy: 0.8547
 8320/13806 [=================>............] - ETA: 11s - loss: 0.4278 - categorical_accuracy: 0.8552
 8448/13806 [=================>............] - ETA: 10s - loss: 0.4275 - categorical_accuracy: 0.8551
 8576/13806 [=================>............] - ETA: 10s - loss: 0.4275 - categorical_accuracy: 0.8554
 8704/13806 [=================>............] - ETA: 10s - loss: 0.4259 - categorical_accuracy: 0.8562
 8832/13806 [==================>...........] - ETA: 10s - loss: 0.4271 - categorical_accuracy: 0.8558
 8960/13806 [==================>...........] - ETA: 9s - loss: 0.4268 - categorical_accuracy: 0.8560 
 9088/13806 [==================>...........] - ETA: 9s - loss: 0.4250 - categorical_accuracy: 0.8570
 9216/13806 [===================>..........] - ETA: 9s - loss: 0.4239 - categorical_accuracy: 0.8580
 9344/13806 [===================>..........] - ETA: 9s - loss: 0.4237 - categorical_accuracy: 0.8576
 9472/13806 [===================>..........] - ETA: 8s - loss: 0.4242 - categorical_accuracy: 0.8577
 9600/13806 [===================>..........] - ETA: 8s - loss: 0.4238 - categorical_accuracy: 0.8581
 9728/13806 [====================>.........] - ETA: 8s - loss: 0.4230 - categorical_accuracy: 0.8584
 9856/13806 [====================>.........] - ETA: 8s - loss: 0.4221 - categorical_accuracy: 0.8589
 9984/13806 [====================>.........] - ETA: 7s - loss: 0.4212 - categorical_accuracy: 0.8597
10112/13806 [====================>.........] - ETA: 7s - loss: 0.4215 - categorical_accuracy: 0.8597
10240/13806 [=====================>........] - ETA: 7s - loss: 0.4215 - categorical_accuracy: 0.8597
10368/13806 [=====================>........] - ETA: 7s - loss: 0.4210 - categorical_accuracy: 0.8599
10496/13806 [=====================>........] - ETA: 6s - loss: 0.4200 - categorical_accuracy: 0.8600
10624/13806 [======================>.......] - ETA: 6s - loss: 0.4202 - categorical_accuracy: 0.8599
10752/13806 [======================>.......] - ETA: 6s - loss: 0.4198 - categorical_accuracy: 0.8600
10880/13806 [======================>.......] - ETA: 6s - loss: 0.4196 - categorical_accuracy: 0.8602
11008/13806 [======================>.......] - ETA: 5s - loss: 0.4197 - categorical_accuracy: 0.8600
11136/13806 [=======================>......] - ETA: 5s - loss: 0.4208 - categorical_accuracy: 0.8599
11264/13806 [=======================>......] - ETA: 5s - loss: 0.4211 - categorical_accuracy: 0.8598
11392/13806 [=======================>......] - ETA: 4s - loss: 0.4213 - categorical_accuracy: 0.8601
11520/13806 [========================>.....] - ETA: 4s - loss: 0.4213 - categorical_accuracy: 0.8601
11648/13806 [========================>.....] - ETA: 4s - loss: 0.4223 - categorical_accuracy: 0.8593
11776/13806 [========================>.....] - ETA: 4s - loss: 0.4219 - categorical_accuracy: 0.8596
11904/13806 [========================>.....] - ETA: 3s - loss: 0.4213 - categorical_accuracy: 0.8601
12032/13806 [=========================>....] - ETA: 3s - loss: 0.4212 - categorical_accuracy: 0.8602
12160/13806 [=========================>....] - ETA: 3s - loss: 0.4219 - categorical_accuracy: 0.8598
12288/13806 [=========================>....] - ETA: 3s - loss: 0.4219 - categorical_accuracy: 0.8599
12416/13806 [=========================>....] - ETA: 2s - loss: 0.4220 - categorical_accuracy: 0.8597
12544/13806 [==========================>...] - ETA: 2s - loss: 0.4212 - categorical_accuracy: 0.8599
12672/13806 [==========================>...] - ETA: 2s - loss: 0.4206 - categorical_accuracy: 0.8599
12800/13806 [==========================>...] - ETA: 2s - loss: 0.4213 - categorical_accuracy: 0.8596
12928/13806 [===========================>..] - ETA: 1s - loss: 0.4219 - categorical_accuracy: 0.8595
13056/13806 [===========================>..] - ETA: 1s - loss: 0.4223 - categorical_accuracy: 0.8597
13184/13806 [===========================>..] - ETA: 1s - loss: 0.4224 - categorical_accuracy: 0.8598
13312/13806 [===========================>..] - ETA: 1s - loss: 0.4215 - categorical_accuracy: 0.8601
13440/13806 [============================>.] - ETA: 0s - loss: 0.4219 - categorical_accuracy: 0.8600
13568/13806 [============================>.] - ETA: 0s - loss: 0.4223 - categorical_accuracy: 0.8596
13696/13806 [============================>.] - ETA: 0s - loss: 0.4234 - categorical_accuracy: 0.8594
13806/13806 [==============================] - 29s 2ms/step - loss: 0.4228 - categorical_accuracy: 0.8597 - val_loss: 1.3166 - val_categorical_accuracy: 0.5381

Epoch 00005: val_categorical_accuracy did not improve
Epoch 6/15

  128/13806 [..............................] - ETA: 27s - loss: 0.4293 - categorical_accuracy: 0.8672
  256/13806 [..............................] - ETA: 27s - loss: 0.4465 - categorical_accuracy: 0.8633
  384/13806 [..............................] - ETA: 27s - loss: 0.4406 - categorical_accuracy: 0.8490
  512/13806 [>.............................] - ETA: 27s - loss: 0.4375 - categorical_accuracy: 0.8516
  640/13806 [>.............................] - ETA: 26s - loss: 0.4469 - categorical_accuracy: 0.8516
  768/13806 [>.............................] - ETA: 26s - loss: 0.4548 - categorical_accuracy: 0.8516
  896/13806 [>.............................] - ETA: 26s - loss: 0.4560 - categorical_accuracy: 0.8516
 1024/13806 [=>............................] - ETA: 26s - loss: 0.4501 - categorical_accuracy: 0.8545
 1152/13806 [=>............................] - ETA: 25s - loss: 0.4573 - categorical_accuracy: 0.8524
 1280/13806 [=>............................] - ETA: 25s - loss: 0.4633 - categorical_accuracy: 0.8461
 1408/13806 [==>...........................] - ETA: 25s - loss: 0.4560 - categorical_accuracy: 0.8530
 1536/13806 [==>...........................] - ETA: 25s - loss: 0.4516 - categorical_accuracy: 0.8548
 1664/13806 [==>...........................] - ETA: 24s - loss: 0.4481 - categorical_accuracy: 0.8540
 1792/13806 [==>...........................] - ETA: 24s - loss: 0.4459 - categorical_accuracy: 0.8560
 1920/13806 [===>..........................] - ETA: 24s - loss: 0.4483 - categorical_accuracy: 0.8531
 2048/13806 [===>..........................] - ETA: 24s - loss: 0.4440 - categorical_accuracy: 0.8550
 2176/13806 [===>..........................] - ETA: 23s - loss: 0.4379 - categorical_accuracy: 0.8566
 2304/13806 [====>.........................] - ETA: 23s - loss: 0.4375 - categorical_accuracy: 0.8555
 2432/13806 [====>.........................] - ETA: 23s - loss: 0.4367 - categorical_accuracy: 0.8549
 2560/13806 [====>.........................] - ETA: 22s - loss: 0.4411 - categorical_accuracy: 0.8531
 2688/13806 [====>.........................] - ETA: 22s - loss: 0.4475 - categorical_accuracy: 0.8519
 2816/13806 [=====>........................] - ETA: 22s - loss: 0.4447 - categorical_accuracy: 0.8533
 2944/13806 [=====>........................] - ETA: 22s - loss: 0.4411 - categorical_accuracy: 0.8563
 3072/13806 [=====>........................] - ETA: 21s - loss: 0.4388 - categorical_accuracy: 0.8584
 3200/13806 [=====>........................] - ETA: 21s - loss: 0.4378 - categorical_accuracy: 0.8581
 3328/13806 [======>.......................] - ETA: 21s - loss: 0.4331 - categorical_accuracy: 0.8606
 3456/13806 [======>.......................] - ETA: 21s - loss: 0.4318 - categorical_accuracy: 0.8605
 3584/13806 [======>.......................] - ETA: 20s - loss: 0.4305 - categorical_accuracy: 0.8610
 3712/13806 [=======>......................] - ETA: 20s - loss: 0.4315 - categorical_accuracy: 0.8599
 3840/13806 [=======>......................] - ETA: 20s - loss: 0.4317 - categorical_accuracy: 0.8599
 3968/13806 [=======>......................] - ETA: 20s - loss: 0.4308 - categorical_accuracy: 0.8611
 4096/13806 [=======>......................] - ETA: 19s - loss: 0.4297 - categorical_accuracy: 0.8606
 4224/13806 [========>.....................] - ETA: 19s - loss: 0.4294 - categorical_accuracy: 0.8615
 4352/13806 [========>.....................] - ETA: 19s - loss: 0.4287 - categorical_accuracy: 0.8619
 4480/13806 [========>.....................] - ETA: 19s - loss: 0.4313 - categorical_accuracy: 0.8609
 4608/13806 [=========>....................] - ETA: 18s - loss: 0.4301 - categorical_accuracy: 0.8611
 4736/13806 [=========>....................] - ETA: 18s - loss: 0.4282 - categorical_accuracy: 0.8617
 4864/13806 [=========>....................] - ETA: 18s - loss: 0.4273 - categorical_accuracy: 0.8614
 4992/13806 [=========>....................] - ETA: 18s - loss: 0.4283 - categorical_accuracy: 0.8618
 5120/13806 [==========>...................] - ETA: 17s - loss: 0.4262 - categorical_accuracy: 0.8629
 5248/13806 [==========>...................] - ETA: 17s - loss: 0.4253 - categorical_accuracy: 0.8628
 5376/13806 [==========>...................] - ETA: 17s - loss: 0.4249 - categorical_accuracy: 0.8627
 5504/13806 [==========>...................] - ETA: 17s - loss: 0.4242 - categorical_accuracy: 0.8623
 5632/13806 [===========>..................] - ETA: 16s - loss: 0.4215 - categorical_accuracy: 0.8631
 5760/13806 [===========>..................] - ETA: 16s - loss: 0.4222 - categorical_accuracy: 0.8625
 5888/13806 [===========>..................] - ETA: 16s - loss: 0.4237 - categorical_accuracy: 0.8616
 6016/13806 [============>.................] - ETA: 16s - loss: 0.4243 - categorical_accuracy: 0.8614
 6144/13806 [============>.................] - ETA: 15s - loss: 0.4236 - categorical_accuracy: 0.8620
 6272/13806 [============>.................] - ETA: 15s - loss: 0.4234 - categorical_accuracy: 0.8619
 6400/13806 [============>.................] - ETA: 15s - loss: 0.4233 - categorical_accuracy: 0.8611
 6528/13806 [=============>................] - ETA: 15s - loss: 0.4219 - categorical_accuracy: 0.8623
 6656/13806 [=============>................] - ETA: 14s - loss: 0.4225 - categorical_accuracy: 0.8621
 6784/13806 [=============>................] - ETA: 14s - loss: 0.4219 - categorical_accuracy: 0.8622
 6912/13806 [==============>...............] - ETA: 14s - loss: 0.4227 - categorical_accuracy: 0.8611
 7040/13806 [==============>...............] - ETA: 13s - loss: 0.4222 - categorical_accuracy: 0.8609
 7168/13806 [==============>...............] - ETA: 13s - loss: 0.4232 - categorical_accuracy: 0.8612
 7296/13806 [==============>...............] - ETA: 13s - loss: 0.4219 - categorical_accuracy: 0.8616
 7424/13806 [===============>..............] - ETA: 13s - loss: 0.4213 - categorical_accuracy: 0.8615
 7552/13806 [===============>..............] - ETA: 12s - loss: 0.4208 - categorical_accuracy: 0.8619
 7680/13806 [===============>..............] - ETA: 12s - loss: 0.4226 - categorical_accuracy: 0.8612
 7808/13806 [===============>..............] - ETA: 12s - loss: 0.4228 - categorical_accuracy: 0.8614
 7936/13806 [================>.............] - ETA: 12s - loss: 0.4235 - categorical_accuracy: 0.8618
 8064/13806 [================>.............] - ETA: 11s - loss: 0.4225 - categorical_accuracy: 0.8624
 8192/13806 [================>.............] - ETA: 11s - loss: 0.4215 - categorical_accuracy: 0.8630
 8320/13806 [=================>............] - ETA: 11s - loss: 0.4206 - categorical_accuracy: 0.8629
 8448/13806 [=================>............] - ETA: 11s - loss: 0.4193 - categorical_accuracy: 0.8636
 8576/13806 [=================>............] - ETA: 10s - loss: 0.4181 - categorical_accuracy: 0.8638
 8704/13806 [=================>............] - ETA: 10s - loss: 0.4190 - categorical_accuracy: 0.8635
 8832/13806 [==================>...........] - ETA: 10s - loss: 0.4183 - categorical_accuracy: 0.8637
 8960/13806 [==================>...........] - ETA: 9s - loss: 0.4180 - categorical_accuracy: 0.8637 
 9088/13806 [==================>...........] - ETA: 9s - loss: 0.4184 - categorical_accuracy: 0.8638
 9216/13806 [===================>..........] - ETA: 9s - loss: 0.4189 - categorical_accuracy: 0.8636
 9344/13806 [===================>..........] - ETA: 9s - loss: 0.4189 - categorical_accuracy: 0.8639
 9472/13806 [===================>..........] - ETA: 8s - loss: 0.4184 - categorical_accuracy: 0.8637
 9600/13806 [===================>..........] - ETA: 8s - loss: 0.4187 - categorical_accuracy: 0.8634
 9728/13806 [====================>.........] - ETA: 8s - loss: 0.4191 - categorical_accuracy: 0.8632
 9856/13806 [====================>.........] - ETA: 8s - loss: 0.4183 - categorical_accuracy: 0.8636
 9984/13806 [====================>.........] - ETA: 7s - loss: 0.4178 - categorical_accuracy: 0.8637
10112/13806 [====================>.........] - ETA: 7s - loss: 0.4164 - categorical_accuracy: 0.8642
10240/13806 [=====================>........] - ETA: 7s - loss: 0.4151 - categorical_accuracy: 0.8646
10368/13806 [=====================>........] - ETA: 7s - loss: 0.4162 - categorical_accuracy: 0.8642
10496/13806 [=====================>........] - ETA: 6s - loss: 0.4155 - categorical_accuracy: 0.8646
10624/13806 [======================>.......] - ETA: 6s - loss: 0.4150 - categorical_accuracy: 0.8647
10752/13806 [======================>.......] - ETA: 6s - loss: 0.4147 - categorical_accuracy: 0.8650
10880/13806 [======================>.......] - ETA: 6s - loss: 0.4144 - categorical_accuracy: 0.8651
11008/13806 [======================>.......] - ETA: 5s - loss: 0.4145 - categorical_accuracy: 0.8646
11136/13806 [=======================>......] - ETA: 5s - loss: 0.4134 - categorical_accuracy: 0.8651
11264/13806 [=======================>......] - ETA: 5s - loss: 0.4136 - categorical_accuracy: 0.8651
11392/13806 [=======================>......] - ETA: 4s - loss: 0.4153 - categorical_accuracy: 0.8651
11520/13806 [========================>.....] - ETA: 4s - loss: 0.4146 - categorical_accuracy: 0.8657
11648/13806 [========================>.....] - ETA: 4s - loss: 0.4146 - categorical_accuracy: 0.8656
11776/13806 [========================>.....] - ETA: 4s - loss: 0.4146 - categorical_accuracy: 0.8656
11904/13806 [========================>.....] - ETA: 3s - loss: 0.4147 - categorical_accuracy: 0.8655
12032/13806 [=========================>....] - ETA: 3s - loss: 0.4143 - categorical_accuracy: 0.8659
12160/13806 [=========================>....] - ETA: 3s - loss: 0.4132 - categorical_accuracy: 0.8663
12288/13806 [=========================>....] - ETA: 3s - loss: 0.4126 - categorical_accuracy: 0.8667
12416/13806 [=========================>....] - ETA: 2s - loss: 0.4134 - categorical_accuracy: 0.8665
12544/13806 [==========================>...] - ETA: 2s - loss: 0.4134 - categorical_accuracy: 0.8665
12672/13806 [==========================>...] - ETA: 2s - loss: 0.4135 - categorical_accuracy: 0.8666
12800/13806 [==========================>...] - ETA: 2s - loss: 0.4132 - categorical_accuracy: 0.8667
12928/13806 [===========================>..] - ETA: 1s - loss: 0.4128 - categorical_accuracy: 0.8671
13056/13806 [===========================>..] - ETA: 1s - loss: 0.4124 - categorical_accuracy: 0.8671
13184/13806 [===========================>..] - ETA: 1s - loss: 0.4122 - categorical_accuracy: 0.8672
13312/13806 [===========================>..] - ETA: 1s - loss: 0.4125 - categorical_accuracy: 0.8670
13440/13806 [============================>.] - ETA: 0s - loss: 0.4123 - categorical_accuracy: 0.8670
13568/13806 [============================>.] - ETA: 0s - loss: 0.4135 - categorical_accuracy: 0.8668
13696/13806 [============================>.] - ETA: 0s - loss: 0.4138 - categorical_accuracy: 0.8668
13806/13806 [==============================] - 29s 2ms/step - loss: 0.4139 - categorical_accuracy: 0.8665 - val_loss: 1.3125 - val_categorical_accuracy: 0.5421

Epoch 00006: val_categorical_accuracy did not improve
Epoch 7/15

  128/13806 [..............................] - ETA: 27s - loss: 0.4004 - categorical_accuracy: 0.8594
  256/13806 [..............................] - ETA: 28s - loss: 0.4321 - categorical_accuracy: 0.8555
  384/13806 [..............................] - ETA: 28s - loss: 0.4610 - categorical_accuracy: 0.8594
  512/13806 [>.............................] - ETA: 28s - loss: 0.4500 - categorical_accuracy: 0.8613
  640/13806 [>.............................] - ETA: 28s - loss: 0.4369 - categorical_accuracy: 0.8672
  768/13806 [>.............................] - ETA: 27s - loss: 0.4269 - categorical_accuracy: 0.8672
  896/13806 [>.............................] - ETA: 27s - loss: 0.4232 - categorical_accuracy: 0.8694
 1024/13806 [=>............................] - ETA: 26s - loss: 0.4125 - categorical_accuracy: 0.8740
 1152/13806 [=>............................] - ETA: 26s - loss: 0.4186 - categorical_accuracy: 0.8707
 1280/13806 [=>............................] - ETA: 26s - loss: 0.4151 - categorical_accuracy: 0.8727
 1408/13806 [==>...........................] - ETA: 26s - loss: 0.4087 - categorical_accuracy: 0.8736
 1536/13806 [==>...........................] - ETA: 25s - loss: 0.4103 - categorical_accuracy: 0.8691
 1664/13806 [==>...........................] - ETA: 25s - loss: 0.4032 - categorical_accuracy: 0.8720
 1792/13806 [==>...........................] - ETA: 25s - loss: 0.3937 - categorical_accuracy: 0.8750
 1920/13806 [===>..........................] - ETA: 24s - loss: 0.3889 - categorical_accuracy: 0.8766
 2048/13806 [===>..........................] - ETA: 24s - loss: 0.3882 - categorical_accuracy: 0.8765
 2176/13806 [===>..........................] - ETA: 24s - loss: 0.3916 - categorical_accuracy: 0.8745
 2304/13806 [====>.........................] - ETA: 23s - loss: 0.3867 - categorical_accuracy: 0.8759
 2432/13806 [====>.........................] - ETA: 23s - loss: 0.3867 - categorical_accuracy: 0.8754
 2560/13806 [====>.........................] - ETA: 23s - loss: 0.3861 - categorical_accuracy: 0.8758
 2688/13806 [====>.........................] - ETA: 23s - loss: 0.3887 - categorical_accuracy: 0.8731
 2816/13806 [=====>........................] - ETA: 22s - loss: 0.3869 - categorical_accuracy: 0.8732
 2944/13806 [=====>........................] - ETA: 22s - loss: 0.3879 - categorical_accuracy: 0.8730
 3072/13806 [=====>........................] - ETA: 22s - loss: 0.3868 - categorical_accuracy: 0.8730
 3200/13806 [=====>........................] - ETA: 21s - loss: 0.3864 - categorical_accuracy: 0.8728
 3328/13806 [======>.......................] - ETA: 21s - loss: 0.3852 - categorical_accuracy: 0.8741
 3456/13806 [======>.......................] - ETA: 21s - loss: 0.3865 - categorical_accuracy: 0.8747
 3584/13806 [======>.......................] - ETA: 21s - loss: 0.3858 - categorical_accuracy: 0.8750
 3712/13806 [=======>......................] - ETA: 20s - loss: 0.3886 - categorical_accuracy: 0.8745
 3840/13806 [=======>......................] - ETA: 20s - loss: 0.3877 - categorical_accuracy: 0.8760
 3968/13806 [=======>......................] - ETA: 20s - loss: 0.3854 - categorical_accuracy: 0.8778
 4096/13806 [=======>......................] - ETA: 19s - loss: 0.3845 - categorical_accuracy: 0.8782
 4224/13806 [========>.....................] - ETA: 19s - loss: 0.3849 - categorical_accuracy: 0.8774
 4352/13806 [========>.....................] - ETA: 19s - loss: 0.3835 - categorical_accuracy: 0.8780
 4480/13806 [========>.....................] - ETA: 19s - loss: 0.3859 - categorical_accuracy: 0.8766
 4608/13806 [=========>....................] - ETA: 18s - loss: 0.3842 - categorical_accuracy: 0.8770
 4736/13806 [=========>....................] - ETA: 18s - loss: 0.3865 - categorical_accuracy: 0.8765
 4864/13806 [=========>....................] - ETA: 18s - loss: 0.3850 - categorical_accuracy: 0.8775
 4992/13806 [=========>....................] - ETA: 18s - loss: 0.3878 - categorical_accuracy: 0.8764
 5120/13806 [==========>...................] - ETA: 17s - loss: 0.3866 - categorical_accuracy: 0.8766
 5248/13806 [==========>...................] - ETA: 17s - loss: 0.3865 - categorical_accuracy: 0.8760
 5376/13806 [==========>...................] - ETA: 17s - loss: 0.3891 - categorical_accuracy: 0.8752
 5504/13806 [==========>...................] - ETA: 17s - loss: 0.3898 - categorical_accuracy: 0.8755
 5632/13806 [===========>..................] - ETA: 16s - loss: 0.3883 - categorical_accuracy: 0.8764
 5760/13806 [===========>..................] - ETA: 16s - loss: 0.3914 - categorical_accuracy: 0.8757
 5888/13806 [===========>..................] - ETA: 16s - loss: 0.3916 - categorical_accuracy: 0.8755
 6016/13806 [============>.................] - ETA: 16s - loss: 0.3953 - categorical_accuracy: 0.8748
 6144/13806 [============>.................] - ETA: 15s - loss: 0.3947 - categorical_accuracy: 0.8755
 6272/13806 [============>.................] - ETA: 15s - loss: 0.3941 - categorical_accuracy: 0.8758
 6400/13806 [============>.................] - ETA: 15s - loss: 0.3937 - categorical_accuracy: 0.8758
 6528/13806 [=============>................] - ETA: 14s - loss: 0.3938 - categorical_accuracy: 0.8756
 6656/13806 [=============>................] - ETA: 14s - loss: 0.3937 - categorical_accuracy: 0.8750
 6784/13806 [=============>................] - ETA: 14s - loss: 0.3932 - categorical_accuracy: 0.8754
 6912/13806 [==============>...............] - ETA: 14s - loss: 0.3916 - categorical_accuracy: 0.8754
 7040/13806 [==============>...............] - ETA: 13s - loss: 0.3930 - categorical_accuracy: 0.8754
 7168/13806 [==============>...............] - ETA: 13s - loss: 0.3914 - categorical_accuracy: 0.8761
 7296/13806 [==============>...............] - ETA: 13s - loss: 0.3923 - categorical_accuracy: 0.8761
 7424/13806 [===============>..............] - ETA: 13s - loss: 0.3936 - categorical_accuracy: 0.8763
 7552/13806 [===============>..............] - ETA: 12s - loss: 0.3933 - categorical_accuracy: 0.8762
 7680/13806 [===============>..............] - ETA: 12s - loss: 0.3941 - categorical_accuracy: 0.8762
 7808/13806 [===============>..............] - ETA: 12s - loss: 0.3946 - categorical_accuracy: 0.8758
 7936/13806 [================>.............] - ETA: 12s - loss: 0.3962 - categorical_accuracy: 0.8755
 8064/13806 [================>.............] - ETA: 11s - loss: 0.3968 - categorical_accuracy: 0.8746
 8192/13806 [================>.............] - ETA: 11s - loss: 0.3972 - categorical_accuracy: 0.8743
 8320/13806 [=================>............] - ETA: 11s - loss: 0.3970 - categorical_accuracy: 0.8745
 8448/13806 [=================>............] - ETA: 11s - loss: 0.3971 - categorical_accuracy: 0.8745
 8576/13806 [=================>............] - ETA: 10s - loss: 0.3980 - categorical_accuracy: 0.8742
 8704/13806 [=================>............] - ETA: 10s - loss: 0.3985 - categorical_accuracy: 0.8736
 8832/13806 [==================>...........] - ETA: 10s - loss: 0.3987 - categorical_accuracy: 0.8735
 8960/13806 [==================>...........] - ETA: 9s - loss: 0.3977 - categorical_accuracy: 0.8740 
 9088/13806 [==================>...........] - ETA: 9s - loss: 0.3981 - categorical_accuracy: 0.8741
 9216/13806 [===================>..........] - ETA: 9s - loss: 0.3984 - categorical_accuracy: 0.8743
 9344/13806 [===================>..........] - ETA: 9s - loss: 0.3986 - categorical_accuracy: 0.8744
 9472/13806 [===================>..........] - ETA: 8s - loss: 0.3995 - categorical_accuracy: 0.8740
 9600/13806 [===================>..........] - ETA: 8s - loss: 0.4004 - categorical_accuracy: 0.8738
 9728/13806 [====================>.........] - ETA: 8s - loss: 0.4009 - categorical_accuracy: 0.8736
 9856/13806 [====================>.........] - ETA: 8s - loss: 0.4002 - categorical_accuracy: 0.8735
 9984/13806 [====================>.........] - ETA: 7s - loss: 0.4002 - categorical_accuracy: 0.8731
10112/13806 [====================>.........] - ETA: 7s - loss: 0.4020 - categorical_accuracy: 0.8730
10240/13806 [=====================>........] - ETA: 7s - loss: 0.4023 - categorical_accuracy: 0.8729
10368/13806 [=====================>........] - ETA: 7s - loss: 0.4021 - categorical_accuracy: 0.8731
10496/13806 [=====================>........] - ETA: 6s - loss: 0.4021 - categorical_accuracy: 0.8730
10624/13806 [======================>.......] - ETA: 6s - loss: 0.4032 - categorical_accuracy: 0.8725
10752/13806 [======================>.......] - ETA: 6s - loss: 0.4027 - categorical_accuracy: 0.8728
10880/13806 [======================>.......] - ETA: 6s - loss: 0.4020 - categorical_accuracy: 0.8727
11008/13806 [======================>.......] - ETA: 5s - loss: 0.4010 - categorical_accuracy: 0.8733
11136/13806 [=======================>......] - ETA: 5s - loss: 0.4004 - categorical_accuracy: 0.8733
11264/13806 [=======================>......] - ETA: 5s - loss: 0.4003 - categorical_accuracy: 0.8731
11392/13806 [=======================>......] - ETA: 4s - loss: 0.3994 - categorical_accuracy: 0.8735
11520/13806 [========================>.....] - ETA: 4s - loss: 0.3988 - categorical_accuracy: 0.8740
11648/13806 [========================>.....] - ETA: 4s - loss: 0.3993 - categorical_accuracy: 0.8735
11776/13806 [========================>.....] - ETA: 4s - loss: 0.3988 - categorical_accuracy: 0.8735
11904/13806 [========================>.....] - ETA: 3s - loss: 0.3988 - categorical_accuracy: 0.8739
12032/13806 [=========================>....] - ETA: 3s - loss: 0.3989 - categorical_accuracy: 0.8738
12160/13806 [=========================>....] - ETA: 3s - loss: 0.3992 - categorical_accuracy: 0.8737
12288/13806 [=========================>....] - ETA: 3s - loss: 0.3997 - categorical_accuracy: 0.8735
12416/13806 [=========================>....] - ETA: 2s - loss: 0.3996 - categorical_accuracy: 0.8733
12544/13806 [==========================>...] - ETA: 2s - loss: 0.3991 - categorical_accuracy: 0.8734
12672/13806 [==========================>...] - ETA: 2s - loss: 0.3989 - categorical_accuracy: 0.8733
12800/13806 [==========================>...] - ETA: 2s - loss: 0.3980 - categorical_accuracy: 0.8737
12928/13806 [===========================>..] - ETA: 1s - loss: 0.3979 - categorical_accuracy: 0.8736
13056/13806 [===========================>..] - ETA: 1s - loss: 0.3979 - categorical_accuracy: 0.8736
13184/13806 [===========================>..] - ETA: 1s - loss: 0.3977 - categorical_accuracy: 0.8738
13312/13806 [===========================>..] - ETA: 1s - loss: 0.3978 - categorical_accuracy: 0.8737
13440/13806 [============================>.] - ETA: 0s - loss: 0.3982 - categorical_accuracy: 0.8735
13568/13806 [============================>.] - ETA: 0s - loss: 0.3976 - categorical_accuracy: 0.8743
13696/13806 [============================>.] - ETA: 0s - loss: 0.3975 - categorical_accuracy: 0.8743
13806/13806 [==============================] - 29s 2ms/step - loss: 0.3967 - categorical_accuracy: 0.8746 - val_loss: 1.3242 - val_categorical_accuracy: 0.5368

Epoch 00007: val_categorical_accuracy did not improve
Epoch 8/15

  128/13806 [..............................] - ETA: 28s - loss: 0.4970 - categorical_accuracy: 0.8281
  256/13806 [..............................] - ETA: 27s - loss: 0.4248 - categorical_accuracy: 0.8516
  384/13806 [..............................] - ETA: 27s - loss: 0.3801 - categorical_accuracy: 0.8646
  512/13806 [>.............................] - ETA: 27s - loss: 0.3920 - categorical_accuracy: 0.8496
  640/13806 [>.............................] - ETA: 26s - loss: 0.3948 - categorical_accuracy: 0.8516
  768/13806 [>.............................] - ETA: 26s - loss: 0.3794 - categorical_accuracy: 0.8620
  896/13806 [>.............................] - ETA: 26s - loss: 0.3811 - categorical_accuracy: 0.8616
 1024/13806 [=>............................] - ETA: 26s - loss: 0.3827 - categorical_accuracy: 0.8604
 1152/13806 [=>............................] - ETA: 25s - loss: 0.3895 - categorical_accuracy: 0.8594
 1280/13806 [=>............................] - ETA: 25s - loss: 0.3913 - categorical_accuracy: 0.8617
 1408/13806 [==>...........................] - ETA: 25s - loss: 0.3882 - categorical_accuracy: 0.8636
 1536/13806 [==>...........................] - ETA: 25s - loss: 0.3880 - categorical_accuracy: 0.8646
 1664/13806 [==>...........................] - ETA: 25s - loss: 0.3930 - categorical_accuracy: 0.8612
 1792/13806 [==>...........................] - ETA: 25s - loss: 0.3955 - categorical_accuracy: 0.8594
 1920/13806 [===>..........................] - ETA: 24s - loss: 0.4005 - categorical_accuracy: 0.8589
 2048/13806 [===>..........................] - ETA: 24s - loss: 0.3998 - categorical_accuracy: 0.8599
 2176/13806 [===>..........................] - ETA: 24s - loss: 0.3977 - categorical_accuracy: 0.8612
 2304/13806 [====>.........................] - ETA: 23s - loss: 0.4018 - categorical_accuracy: 0.8576
 2432/13806 [====>.........................] - ETA: 23s - loss: 0.4021 - categorical_accuracy: 0.8581
 2560/13806 [====>.........................] - ETA: 23s - loss: 0.4027 - categorical_accuracy: 0.8578
 2688/13806 [====>.........................] - ETA: 23s - loss: 0.4000 - categorical_accuracy: 0.8597
 2816/13806 [=====>........................] - ETA: 22s - loss: 0.4001 - categorical_accuracy: 0.8597
 2944/13806 [=====>........................] - ETA: 22s - loss: 0.3977 - categorical_accuracy: 0.8614
 3072/13806 [=====>........................] - ETA: 22s - loss: 0.3945 - categorical_accuracy: 0.8626
 3200/13806 [=====>........................] - ETA: 22s - loss: 0.3927 - categorical_accuracy: 0.8622
 3328/13806 [======>.......................] - ETA: 21s - loss: 0.3912 - categorical_accuracy: 0.8642
 3456/13806 [======>.......................] - ETA: 21s - loss: 0.3901 - categorical_accuracy: 0.8657
 3584/13806 [======>.......................] - ETA: 21s - loss: 0.3897 - categorical_accuracy: 0.8664
 3712/13806 [=======>......................] - ETA: 20s - loss: 0.3876 - categorical_accuracy: 0.8675
 3840/13806 [=======>......................] - ETA: 20s - loss: 0.3861 - categorical_accuracy: 0.8688
 3968/13806 [=======>......................] - ETA: 20s - loss: 0.3884 - categorical_accuracy: 0.8667
 4096/13806 [=======>......................] - ETA: 20s - loss: 0.3897 - categorical_accuracy: 0.8665
 4224/13806 [========>.....................] - ETA: 19s - loss: 0.3892 - categorical_accuracy: 0.8670
 4352/13806 [========>.....................] - ETA: 19s - loss: 0.3906 - categorical_accuracy: 0.8660
 4480/13806 [========>.....................] - ETA: 19s - loss: 0.3933 - categorical_accuracy: 0.8650
 4608/13806 [=========>....................] - ETA: 19s - loss: 0.3939 - categorical_accuracy: 0.8650
 4736/13806 [=========>....................] - ETA: 18s - loss: 0.3924 - categorical_accuracy: 0.8653
 4864/13806 [=========>....................] - ETA: 18s - loss: 0.3944 - categorical_accuracy: 0.8647
 4992/13806 [=========>....................] - ETA: 18s - loss: 0.3935 - categorical_accuracy: 0.8648
 5120/13806 [==========>...................] - ETA: 17s - loss: 0.3945 - categorical_accuracy: 0.8643
 5248/13806 [==========>...................] - ETA: 17s - loss: 0.3941 - categorical_accuracy: 0.8647
 5376/13806 [==========>...................] - ETA: 17s - loss: 0.3953 - categorical_accuracy: 0.8640
 5504/13806 [==========>...................] - ETA: 17s - loss: 0.3974 - categorical_accuracy: 0.8639
 5632/13806 [===========>..................] - ETA: 16s - loss: 0.3965 - categorical_accuracy: 0.8645
 5760/13806 [===========>..................] - ETA: 16s - loss: 0.3996 - categorical_accuracy: 0.8634
 5888/13806 [===========>..................] - ETA: 16s - loss: 0.4033 - categorical_accuracy: 0.8619
 6016/13806 [============>.................] - ETA: 16s - loss: 0.4025 - categorical_accuracy: 0.8622
 6144/13806 [============>.................] - ETA: 15s - loss: 0.4036 - categorical_accuracy: 0.8621
 6272/13806 [============>.................] - ETA: 15s - loss: 0.4030 - categorical_accuracy: 0.8626
 6400/13806 [============>.................] - ETA: 15s - loss: 0.4027 - categorical_accuracy: 0.8628
 6528/13806 [=============>................] - ETA: 15s - loss: 0.4035 - categorical_accuracy: 0.8624
 6656/13806 [=============>................] - ETA: 14s - loss: 0.4027 - categorical_accuracy: 0.8627
 6784/13806 [=============>................] - ETA: 14s - loss: 0.4023 - categorical_accuracy: 0.8629
 6912/13806 [==============>...............] - ETA: 14s - loss: 0.4021 - categorical_accuracy: 0.8631
 7040/13806 [==============>...............] - ETA: 14s - loss: 0.4017 - categorical_accuracy: 0.8636
 7168/13806 [==============>...............] - ETA: 13s - loss: 0.4012 - categorical_accuracy: 0.8637
 7296/13806 [==============>...............] - ETA: 13s - loss: 0.4016 - categorical_accuracy: 0.8635
 7424/13806 [===============>..............] - ETA: 13s - loss: 0.4013 - categorical_accuracy: 0.8634
 7552/13806 [===============>..............] - ETA: 13s - loss: 0.3994 - categorical_accuracy: 0.8645
 7680/13806 [===============>..............] - ETA: 12s - loss: 0.3993 - categorical_accuracy: 0.8643
 7808/13806 [===============>..............] - ETA: 12s - loss: 0.3986 - categorical_accuracy: 0.8649
 7936/13806 [================>.............] - ETA: 12s - loss: 0.3993 - categorical_accuracy: 0.8650
 8064/13806 [================>.............] - ETA: 12s - loss: 0.3987 - categorical_accuracy: 0.8661
 8192/13806 [================>.............] - ETA: 11s - loss: 0.3993 - categorical_accuracy: 0.8661
 8320/13806 [=================>............] - ETA: 11s - loss: 0.3979 - categorical_accuracy: 0.8667
 8448/13806 [=================>............] - ETA: 11s - loss: 0.3972 - categorical_accuracy: 0.8671
 8576/13806 [=================>............] - ETA: 10s - loss: 0.3977 - categorical_accuracy: 0.8672
 8704/13806 [=================>............] - ETA: 10s - loss: 0.3997 - categorical_accuracy: 0.8663
 8832/13806 [==================>...........] - ETA: 10s - loss: 0.3988 - categorical_accuracy: 0.8668
 8960/13806 [==================>...........] - ETA: 10s - loss: 0.3988 - categorical_accuracy: 0.8671
 9088/13806 [==================>...........] - ETA: 9s - loss: 0.3980 - categorical_accuracy: 0.8672 
 9216/13806 [===================>..........] - ETA: 9s - loss: 0.3982 - categorical_accuracy: 0.8670
 9344/13806 [===================>..........] - ETA: 9s - loss: 0.3980 - categorical_accuracy: 0.8669
 9472/13806 [===================>..........] - ETA: 9s - loss: 0.3968 - categorical_accuracy: 0.8675
 9600/13806 [===================>..........] - ETA: 8s - loss: 0.3977 - categorical_accuracy: 0.8672
 9728/13806 [====================>.........] - ETA: 8s - loss: 0.3965 - categorical_accuracy: 0.8676
 9856/13806 [====================>.........] - ETA: 8s - loss: 0.3964 - categorical_accuracy: 0.8674
 9984/13806 [====================>.........] - ETA: 7s - loss: 0.3953 - categorical_accuracy: 0.8681
10112/13806 [====================>.........] - ETA: 7s - loss: 0.3950 - categorical_accuracy: 0.8684
10240/13806 [=====================>........] - ETA: 7s - loss: 0.3950 - categorical_accuracy: 0.8684
10368/13806 [=====================>........] - ETA: 7s - loss: 0.3933 - categorical_accuracy: 0.8688
10496/13806 [=====================>........] - ETA: 6s - loss: 0.3921 - categorical_accuracy: 0.8694
10624/13806 [======================>.......] - ETA: 6s - loss: 0.3934 - categorical_accuracy: 0.8693
10752/13806 [======================>.......] - ETA: 6s - loss: 0.3940 - categorical_accuracy: 0.8692
10880/13806 [======================>.......] - ETA: 6s - loss: 0.3939 - categorical_accuracy: 0.8693
11008/13806 [======================>.......] - ETA: 5s - loss: 0.3938 - categorical_accuracy: 0.8690
11136/13806 [=======================>......] - ETA: 5s - loss: 0.3945 - categorical_accuracy: 0.8691
11264/13806 [=======================>......] - ETA: 5s - loss: 0.3945 - categorical_accuracy: 0.8694
11392/13806 [=======================>......] - ETA: 5s - loss: 0.3954 - categorical_accuracy: 0.8684
11520/13806 [========================>.....] - ETA: 4s - loss: 0.3963 - categorical_accuracy: 0.8678
11648/13806 [========================>.....] - ETA: 4s - loss: 0.3967 - categorical_accuracy: 0.8677
11776/13806 [========================>.....] - ETA: 4s - loss: 0.3958 - categorical_accuracy: 0.8682
11904/13806 [========================>.....] - ETA: 3s - loss: 0.3962 - categorical_accuracy: 0.8680
12032/13806 [=========================>....] - ETA: 3s - loss: 0.3969 - categorical_accuracy: 0.8681
12160/13806 [=========================>....] - ETA: 3s - loss: 0.3967 - categorical_accuracy: 0.8681
12288/13806 [=========================>....] - ETA: 3s - loss: 0.3966 - categorical_accuracy: 0.8680
12416/13806 [=========================>....] - ETA: 2s - loss: 0.3963 - categorical_accuracy: 0.8683
12544/13806 [==========================>...] - ETA: 2s - loss: 0.3964 - categorical_accuracy: 0.8685
12672/13806 [==========================>...] - ETA: 2s - loss: 0.3959 - categorical_accuracy: 0.8689
12800/13806 [==========================>...] - ETA: 2s - loss: 0.3951 - categorical_accuracy: 0.8692
12928/13806 [===========================>..] - ETA: 1s - loss: 0.3959 - categorical_accuracy: 0.8690
13056/13806 [===========================>..] - ETA: 1s - loss: 0.3954 - categorical_accuracy: 0.8693
13184/13806 [===========================>..] - ETA: 1s - loss: 0.3952 - categorical_accuracy: 0.8693
13312/13806 [===========================>..] - ETA: 1s - loss: 0.3954 - categorical_accuracy: 0.8693
13440/13806 [============================>.] - ETA: 0s - loss: 0.3960 - categorical_accuracy: 0.8694
13568/13806 [============================>.] - ETA: 0s - loss: 0.3953 - categorical_accuracy: 0.8698
13696/13806 [============================>.] - ETA: 0s - loss: 0.3952 - categorical_accuracy: 0.8695
13806/13806 [==============================] - 30s 2ms/step - loss: 0.3965 - categorical_accuracy: 0.8693 - val_loss: 1.3336 - val_categorical_accuracy: 0.5401
2018-03-23 16:55:39.779073: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:04:00.0, compute capability: 6.1)
/home/michon/anaconda2/envs/py35/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.

Epoch 00008: val_categorical_accuracy did not improve
Epoch 00008: early stopping

Final evaluation

f1_score
 0.530425763557435
accuracy_score
 0.5400927766732936

classification_report
              precision    recall  f1-score   support

        EGY       0.52      0.56      0.54       297
        GLF       0.47      0.53      0.50       259
        LAV       0.44      0.26      0.32       327
        MSA       0.64      0.76      0.69       280
        NOR       0.58      0.63      0.60       346

avg / total       0.53      0.54      0.53      1509


confusion_matrix
 [[165  31  42  28  31]
 [ 28 137  33  28  33]
 [ 60  65  84  43  75]
 [ 12  23  13 212  20]
 [ 52  35  20  22 217]]

Evaluation on best model

f1_score
 0.5310766969409465
accuracy_score
 0.5434062292909212

classification_report
              precision    recall  f1-score   support

        EGY       0.56      0.56      0.56       297
        GLF       0.48      0.53      0.50       259
        LAV       0.46      0.25      0.33       327
        MSA       0.57      0.79      0.66       280
        NOR       0.59      0.62      0.60       346

avg / total       0.53      0.54      0.53      1509


confusion_matrix
 [[166  26  40  36  29]
 [ 25 136  28  39  31]
 [ 54  62  83  58  70]
 [ 10  21   9 222  18]
 [ 44  36  20  33 213]]
Closing remaining open files:data/vardial2018/dataset.h5...done
############# train: DONE @ Fri Mar 23 16:55:43 CET 2018
