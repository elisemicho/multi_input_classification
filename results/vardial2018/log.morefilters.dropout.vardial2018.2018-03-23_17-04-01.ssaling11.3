############# train @ Fri Mar 23 17:04:01 CET 2018 GPUS=3  HOST=ssaling11 PWD=/home/michon/projects/VarDial2018/to_export/multi_input_modular
Loading data
Data Configurations loaded
Loading data
(13806, 8)
(1509, 8)
EGY    3085
LAV    2940
NOR    2866
GLF    2707
MSA    2208
Name: Class, dtype: int64
NOR    346
LAV    327
EGY    297
MSA    280
GLF    259
Name: Class, dtype: int64
Loading vocabularies
Words
48244 48244
Phones
45 45
39 39
61 61
51 51
Generating ids
Preprocessing data
Padding character sequences
(13806, 6830)
Padding phone sequences
(13806, 5885) (13806, 7329) (13806, 6436) (13806, 6837)
Turning labels in one-hot vectors
(13806, 5)
Taking ready-made acoustic embeddings
(13806, 600)
Padding character sequences
(1509, 6830)
Padding phone sequences
(1509, 5885) (1509, 7329) (1509, 6436) (1509, 6837)
Turning labels in one-hot vectors
(1509, 5)
Taking ready-made acoustic embeddings
(1509, 600)
MultiInputCharCNN Configurations loaded
Building the model
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
sent_input (InputLayer)         (None, 6830)         0                                            
__________________________________________________________________________________________________
phone_CZ_input (InputLayer)     (None, 5885)         0                                            
__________________________________________________________________________________________________
phone_EN_input (InputLayer)     (None, 7329)         0                                            
__________________________________________________________________________________________________
phone_HU_input (InputLayer)     (None, 6436)         0                                            
__________________________________________________________________________________________________
phone_RU_input (InputLayer)     (None, 6837)         0                                            
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 6830, 32)     3232        sent_input[0][0]                 
__________________________________________________________________________________________________
embedding_2 (Embedding)         (None, 5885, 32)     1472        phone_CZ_input[0][0]             
__________________________________________________________________________________________________
embedding_3 (Embedding)         (None, 7329, 32)     1280        phone_EN_input[0][0]             
__________________________________________________________________________________________________
embedding_4 (Embedding)         (None, 6436, 32)     1984        phone_HU_input[0][0]             
__________________________________________________________________________________________________
embedding_5 (Embedding)         (None, 6837, 32)     1664        phone_RU_input[0][0]             
__________________________________________________________________________________________________
zero_padding1d_1 (ZeroPadding1D (None, 6834, 32)     0           embedding_1[0][0]                
__________________________________________________________________________________________________
zero_padding1d_2 (ZeroPadding1D (None, 6838, 32)     0           embedding_1[0][0]                
__________________________________________________________________________________________________
zero_padding1d_3 (ZeroPadding1D (None, 6842, 32)     0           embedding_1[0][0]                
__________________________________________________________________________________________________
zero_padding1d_4 (ZeroPadding1D (None, 5889, 32)     0           embedding_2[0][0]                
__________________________________________________________________________________________________
zero_padding1d_5 (ZeroPadding1D (None, 5893, 32)     0           embedding_2[0][0]                
__________________________________________________________________________________________________
zero_padding1d_6 (ZeroPadding1D (None, 5897, 32)     0           embedding_2[0][0]                
__________________________________________________________________________________________________
zero_padding1d_7 (ZeroPadding1D (None, 7333, 32)     0           embedding_3[0][0]                
__________________________________________________________________________________________________
zero_padding1d_8 (ZeroPadding1D (None, 7337, 32)     0           embedding_3[0][0]                
__________________________________________________________________________________________________
zero_padding1d_9 (ZeroPadding1D (None, 7341, 32)     0           embedding_3[0][0]                
__________________________________________________________________________________________________
zero_padding1d_10 (ZeroPadding1 (None, 6440, 32)     0           embedding_4[0][0]                
__________________________________________________________________________________________________
zero_padding1d_11 (ZeroPadding1 (None, 6444, 32)     0           embedding_4[0][0]                
__________________________________________________________________________________________________
zero_padding1d_12 (ZeroPadding1 (None, 6448, 32)     0           embedding_4[0][0]                
__________________________________________________________________________________________________
zero_padding1d_13 (ZeroPadding1 (None, 6841, 32)     0           embedding_5[0][0]                
__________________________________________________________________________________________________
zero_padding1d_14 (ZeroPadding1 (None, 6845, 32)     0           embedding_5[0][0]                
__________________________________________________________________________________________________
zero_padding1d_15 (ZeroPadding1 (None, 6849, 32)     0           embedding_5[0][0]                
__________________________________________________________________________________________________
conv1d_1 (Conv1D)               (None, 6834, 8)      776         zero_padding1d_1[0][0]           
__________________________________________________________________________________________________
conv1d_2 (Conv1D)               (None, 6838, 8)      1288        zero_padding1d_2[0][0]           
__________________________________________________________________________________________________
conv1d_3 (Conv1D)               (None, 6842, 8)      1800        zero_padding1d_3[0][0]           
__________________________________________________________________________________________________
conv1d_4 (Conv1D)               (None, 5889, 8)      776         zero_padding1d_4[0][0]           
__________________________________________________________________________________________________
conv1d_5 (Conv1D)               (None, 5893, 8)      1288        zero_padding1d_5[0][0]           
__________________________________________________________________________________________________
conv1d_6 (Conv1D)               (None, 5897, 8)      1800        zero_padding1d_6[0][0]           
__________________________________________________________________________________________________
conv1d_7 (Conv1D)               (None, 7333, 8)      776         zero_padding1d_7[0][0]           
__________________________________________________________________________________________________
conv1d_8 (Conv1D)               (None, 7337, 8)      1288        zero_padding1d_8[0][0]           
__________________________________________________________________________________________________
conv1d_9 (Conv1D)               (None, 7341, 8)      1800        zero_padding1d_9[0][0]           
__________________________________________________________________________________________________
conv1d_10 (Conv1D)              (None, 6440, 8)      776         zero_padding1d_10[0][0]          
__________________________________________________________________________________________________
conv1d_11 (Conv1D)              (None, 6444, 8)      1288        zero_padding1d_11[0][0]          
__________________________________________________________________________________________________
conv1d_12 (Conv1D)              (None, 6448, 8)      1800        zero_padding1d_12[0][0]          
__________________________________________________________________________________________________
conv1d_13 (Conv1D)              (None, 6841, 8)      776         zero_padding1d_13[0][0]          
__________________________________________________________________________________________________
conv1d_14 (Conv1D)              (None, 6845, 8)      1288        zero_padding1d_14[0][0]          
__________________________________________________________________________________________________
conv1d_15 (Conv1D)              (None, 6849, 8)      1800        zero_padding1d_15[0][0]          
__________________________________________________________________________________________________
global_max_pooling1d_1 (GlobalM (None, 8)            0           conv1d_1[0][0]                   
__________________________________________________________________________________________________
global_max_pooling1d_2 (GlobalM (None, 8)            0           conv1d_2[0][0]                   
__________________________________________________________________________________________________
global_max_pooling1d_3 (GlobalM (None, 8)            0           conv1d_3[0][0]                   
__________________________________________________________________________________________________
global_max_pooling1d_4 (GlobalM (None, 8)            0           conv1d_4[0][0]                   
__________________________________________________________________________________________________
global_max_pooling1d_5 (GlobalM (None, 8)            0           conv1d_5[0][0]                   
__________________________________________________________________________________________________
global_max_pooling1d_6 (GlobalM (None, 8)            0           conv1d_6[0][0]                   
__________________________________________________________________________________________________
global_max_pooling1d_7 (GlobalM (None, 8)            0           conv1d_7[0][0]                   
__________________________________________________________________________________________________
global_max_pooling1d_8 (GlobalM (None, 8)            0           conv1d_8[0][0]                   
__________________________________________________________________________________________________
global_max_pooling1d_9 (GlobalM (None, 8)            0           conv1d_9[0][0]                   
__________________________________________________________________________________________________
global_max_pooling1d_10 (Global (None, 8)            0           conv1d_10[0][0]                  
__________________________________________________________________________________________________
global_max_pooling1d_11 (Global (None, 8)            0           conv1d_11[0][0]                  
__________________________________________________________________________________________________
global_max_pooling1d_12 (Global (None, 8)            0           conv1d_12[0][0]                  
__________________________________________________________________________________________________
global_max_pooling1d_13 (Global (None, 8)            0           conv1d_13[0][0]                  
__________________________________________________________________________________________________
global_max_pooling1d_14 (Global (None, 8)            0           conv1d_14[0][0]                  
__________________________________________________________________________________________________
global_max_pooling1d_15 (Global (None, 8)            0           conv1d_15[0][0]                  
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 8)            0           global_max_pooling1d_1[0][0]     
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 8)            0           global_max_pooling1d_2[0][0]     
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 8)            0           global_max_pooling1d_3[0][0]     
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 8)            0           global_max_pooling1d_4[0][0]     
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 8)            0           global_max_pooling1d_5[0][0]     
__________________________________________________________________________________________________
dropout_6 (Dropout)             (None, 8)            0           global_max_pooling1d_6[0][0]     
__________________________________________________________________________________________________
dropout_7 (Dropout)             (None, 8)            0           global_max_pooling1d_7[0][0]     
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 8)            0           global_max_pooling1d_8[0][0]     
__________________________________________________________________________________________________
dropout_9 (Dropout)             (None, 8)            0           global_max_pooling1d_9[0][0]     
__________________________________________________________________________________________________
dropout_10 (Dropout)            (None, 8)            0           global_max_pooling1d_10[0][0]    
__________________________________________________________________________________________________
dropout_11 (Dropout)            (None, 8)            0           global_max_pooling1d_11[0][0]    
__________________________________________________________________________________________________
dropout_12 (Dropout)            (None, 8)            0           global_max_pooling1d_12[0][0]    
__________________________________________________________________________________________________
dropout_13 (Dropout)            (None, 8)            0           global_max_pooling1d_13[0][0]    
__________________________________________________________________________________________________
dropout_14 (Dropout)            (None, 8)            0           global_max_pooling1d_14[0][0]    
__________________________________________________________________________________________________
dropout_15 (Dropout)            (None, 8)            0           global_max_pooling1d_15[0][0]    
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 120)          0           dropout_1[0][0]                  
                                                                 dropout_2[0][0]                  
                                                                 dropout_3[0][0]                  
                                                                 dropout_4[0][0]                  
                                                                 dropout_5[0][0]                  
                                                                 dropout_6[0][0]                  
                                                                 dropout_7[0][0]                  
                                                                 dropout_8[0][0]                  
                                                                 dropout_9[0][0]                  
                                                                 dropout_10[0][0]                 
                                                                 dropout_11[0][0]                 
                                                                 dropout_12[0][0]                 
                                                                 dropout_13[0][0]                 
                                                                 dropout_14[0][0]                 
                                                                 dropout_15[0][0]                 2018-03-23 17:04:11.113857: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-03-23 17:04:11.350870: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335
pciBusID: 0000:04:00.0
totalMemory: 7.92GiB freeMemory: 7.81GiB
2018-03-23 17:04:11.350900: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:04:00.0, compute capability: 6.1)

__________________________________________________________________________________________________
embed_input (InputLayer)        (None, 600)          0                                            
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 720)          0           concatenate_1[0][0]              
                                                                 embed_input[0][0]                
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 16)           11536       concatenate_2[0][0]              
__________________________________________________________________________________________________
dropout_16 (Dropout)            (None, 16)           0           dense_1[0][0]                    
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 16)           272         dropout_16[0][0]                 
__________________________________________________________________________________________________
dropout_17 (Dropout)            (None, 16)           0           dense_2[0][0]                    
__________________________________________________________________________________________________
l_out (Dense)                   (None, 5)            85          dropout_17[0][0]                 
==================================================================================================
Total params: 40,845
Trainable params: 40,845
Non-trainable params: 0
__________________________________________________________________________________________________
Training Configurations loaded
Training the model
no checkpoints available !
Train on 13806 samples, validate on 1509 samples
Epoch 1/15

  128/13806 [..............................] - ETA: 3:08 - loss: 2.2972 - categorical_accuracy: 0.1953
  256/13806 [..............................] - ETA: 1:52 - loss: 2.2463 - categorical_accuracy: 0.1953
  384/13806 [..............................] - ETA: 1:26 - loss: 2.2128 - categorical_accuracy: 0.1953
  512/13806 [>.............................] - ETA: 1:13 - loss: 2.1830 - categorical_accuracy: 0.2051
  640/13806 [>.............................] - ETA: 1:05 - loss: 2.1563 - categorical_accuracy: 0.2156
  768/13806 [>.............................] - ETA: 1:00 - loss: 2.1341 - categorical_accuracy: 0.2188
  896/13806 [>.............................] - ETA: 56s - loss: 2.1234 - categorical_accuracy: 0.2176 
 1024/13806 [=>............................] - ETA: 53s - loss: 2.1141 - categorical_accuracy: 0.2148
 1152/13806 [=>............................] - ETA: 50s - loss: 2.1041 - categorical_accuracy: 0.2161
 1280/13806 [=>............................] - ETA: 48s - loss: 2.0971 - categorical_accuracy: 0.2148
 1408/13806 [==>...........................] - ETA: 47s - loss: 2.0919 - categorical_accuracy: 0.2159
 1536/13806 [==>...........................] - ETA: 45s - loss: 2.0856 - categorical_accuracy: 0.2253
 1664/13806 [==>...........................] - ETA: 44s - loss: 2.0801 - categorical_accuracy: 0.2260
 1792/13806 [==>...........................] - ETA: 43s - loss: 2.0731 - categorical_accuracy: 0.2310
 1920/13806 [===>..........................] - ETA: 41s - loss: 2.0689 - categorical_accuracy: 0.2323
 2048/13806 [===>..........................] - ETA: 41s - loss: 2.0614 - categorical_accuracy: 0.2319
 2176/13806 [===>..........................] - ETA: 40s - loss: 2.0573 - categorical_accuracy: 0.2330
 2304/13806 [====>.........................] - ETA: 39s - loss: 2.0545 - categorical_accuracy: 0.2348
 2432/13806 [====>.........................] - ETA: 38s - loss: 2.0492 - categorical_accuracy: 0.2377
 2560/13806 [====>.........................] - ETA: 37s - loss: 2.0436 - categorical_accuracy: 0.2434
 2688/13806 [====>.........................] - ETA: 37s - loss: 2.0400 - categorical_accuracy: 0.2459
 2816/13806 [=====>........................] - ETA: 36s - loss: 2.0370 - categorical_accuracy: 0.2436
 2944/13806 [=====>........................] - ETA: 35s - loss: 2.0329 - categorical_accuracy: 0.2435
 3072/13806 [=====>........................] - ETA: 35s - loss: 2.0280 - categorical_accuracy: 0.2458
 3200/13806 [=====>........................] - ETA: 34s - loss: 2.0257 - categorical_accuracy: 0.2447
 3328/13806 [======>.......................] - ETA: 33s - loss: 2.0240 - categorical_accuracy: 0.2434
 3456/13806 [======>.......................] - ETA: 33s - loss: 2.0211 - categorical_accuracy: 0.2445
 3584/13806 [======>.......................] - ETA: 32s - loss: 2.0186 - categorical_accuracy: 0.2455
 3712/13806 [=======>......................] - ETA: 32s - loss: 2.0145 - categorical_accuracy: 0.2462
 3840/13806 [=======>......................] - ETA: 31s - loss: 2.0100 - categorical_accuracy: 0.2461
 3968/13806 [=======>......................] - ETA: 31s - loss: 2.0078 - categorical_accuracy: 0.2470
 4096/13806 [=======>......................] - ETA: 30s - loss: 2.0033 - categorical_accuracy: 0.2478
 4224/13806 [========>.....................] - ETA: 30s - loss: 1.9996 - categorical_accuracy: 0.2476
 4352/13806 [========>.....................] - ETA: 29s - loss: 1.9965 - categorical_accuracy: 0.2489
 4480/13806 [========>.....................] - ETA: 29s - loss: 1.9940 - categorical_accuracy: 0.2489
 4608/13806 [=========>....................] - ETA: 28s - loss: 1.9916 - categorical_accuracy: 0.2493
 4736/13806 [=========>....................] - ETA: 28s - loss: 1.9890 - categorical_accuracy: 0.2500
 4864/13806 [=========>....................] - ETA: 27s - loss: 1.9865 - categorical_accuracy: 0.2486
 4992/13806 [=========>....................] - ETA: 27s - loss: 1.9838 - categorical_accuracy: 0.2510
 5120/13806 [==========>...................] - ETA: 26s - loss: 1.9807 - categorical_accuracy: 0.2506
 5248/13806 [==========>...................] - ETA: 26s - loss: 1.9785 - categorical_accuracy: 0.2506
 5376/13806 [==========>...................] - ETA: 26s - loss: 1.9750 - categorical_accuracy: 0.2532
 5504/13806 [==========>...................] - ETA: 25s - loss: 1.9716 - categorical_accuracy: 0.2542
 5632/13806 [===========>..................] - ETA: 25s - loss: 1.9696 - categorical_accuracy: 0.2544
 5760/13806 [===========>..................] - ETA: 24s - loss: 1.9673 - categorical_accuracy: 0.2533
 5888/13806 [===========>..................] - ETA: 24s - loss: 1.9649 - categorical_accuracy: 0.2525
 6016/13806 [============>.................] - ETA: 23s - loss: 1.9630 - categorical_accuracy: 0.2518
 6144/13806 [============>.................] - ETA: 23s - loss: 1.9617 - categorical_accuracy: 0.2518
 6272/13806 [============>.................] - ETA: 23s - loss: 1.9600 - categorical_accuracy: 0.2513
 6400/13806 [============>.................] - ETA: 22s - loss: 1.9569 - categorical_accuracy: 0.2523
 6528/13806 [=============>................] - ETA: 22s - loss: 1.9546 - categorical_accuracy: 0.2518
 6656/13806 [=============>................] - ETA: 21s - loss: 1.9525 - categorical_accuracy: 0.2530
 6784/13806 [=============>................] - ETA: 21s - loss: 1.9492 - categorical_accuracy: 0.2547
 6912/13806 [==============>...............] - ETA: 20s - loss: 1.9471 - categorical_accuracy: 0.2551
 7040/13806 [==============>...............] - ETA: 20s - loss: 1.9437 - categorical_accuracy: 0.2560
 7168/13806 [==============>...............] - ETA: 20s - loss: 1.9410 - categorical_accuracy: 0.2571
 7296/13806 [==============>...............] - ETA: 19s - loss: 1.9388 - categorical_accuracy: 0.2567
 7424/13806 [===============>..............] - ETA: 19s - loss: 1.9367 - categorical_accuracy: 0.2578
 7552/13806 [===============>..............] - ETA: 18s - loss: 1.9348 - categorical_accuracy: 0.2577
 7680/13806 [===============>..............] - ETA: 18s - loss: 1.9332 - categorical_accuracy: 0.2578
 7808/13806 [===============>..............] - ETA: 18s - loss: 1.9306 - categorical_accuracy: 0.2583
 7936/13806 [================>.............] - ETA: 17s - loss: 1.9285 - categorical_accuracy: 0.2588
 8064/13806 [================>.............] - ETA: 17s - loss: 1.9247 - categorical_accuracy: 0.2595
 8192/13806 [================>.............] - ETA: 16s - loss: 1.9225 - categorical_accuracy: 0.2592
 8320/13806 [=================>............] - ETA: 16s - loss: 1.9198 - categorical_accuracy: 0.2595
 8448/13806 [=================>............] - ETA: 16s - loss: 1.9173 - categorical_accuracy: 0.2605
 8576/13806 [=================>............] - ETA: 15s - loss: 1.9151 - categorical_accuracy: 0.2610
 8704/13806 [=================>............] - ETA: 15s - loss: 1.9131 - categorical_accuracy: 0.2617
 8832/13806 [==================>...........] - ETA: 14s - loss: 1.9109 - categorical_accuracy: 0.2623
 8960/13806 [==================>...........] - ETA: 14s - loss: 1.9078 - categorical_accuracy: 0.2633
 9088/13806 [==================>...........] - ETA: 14s - loss: 1.9058 - categorical_accuracy: 0.2646
 9216/13806 [===================>..........] - ETA: 13s - loss: 1.9040 - categorical_accuracy: 0.2654
 9344/13806 [===================>..........] - ETA: 13s - loss: 1.9018 - categorical_accuracy: 0.2669
 9472/13806 [===================>..........] - ETA: 13s - loss: 1.9001 - categorical_accuracy: 0.2670
 9600/13806 [===================>..........] - ETA: 12s - loss: 1.8973 - categorical_accuracy: 0.2682
 9728/13806 [====================>.........] - ETA: 12s - loss: 1.8952 - categorical_accuracy: 0.2678
 9856/13806 [====================>.........] - ETA: 11s - loss: 1.8922 - categorical_accuracy: 0.2689
 9984/13806 [====================>.........] - ETA: 11s - loss: 1.8909 - categorical_accuracy: 0.2685
10112/13806 [====================>.........] - ETA: 11s - loss: 1.8890 - categorical_accuracy: 0.2686
10240/13806 [=====================>........] - ETA: 10s - loss: 1.8866 - categorical_accuracy: 0.2688
10368/13806 [=====================>........] - ETA: 10s - loss: 1.8844 - categorical_accuracy: 0.2699
10496/13806 [=====================>........] - ETA: 9s - loss: 1.8820 - categorical_accuracy: 0.2704 
10624/13806 [======================>.......] - ETA: 9s - loss: 1.8797 - categorical_accuracy: 0.2708
10752/13806 [======================>.......] - ETA: 9s - loss: 1.8768 - categorical_accuracy: 0.2714
10880/13806 [======================>.......] - ETA: 8s - loss: 1.8742 - categorical_accuracy: 0.2721
11008/13806 [======================>.......] - ETA: 8s - loss: 1.8722 - categorical_accuracy: 0.2733
11136/13806 [=======================>......] - ETA: 7s - loss: 1.8702 - categorical_accuracy: 0.2735
11264/13806 [=======================>......] - ETA: 7s - loss: 1.8679 - categorical_accuracy: 0.2739
11392/13806 [=======================>......] - ETA: 7s - loss: 1.8651 - categorical_accuracy: 0.2749
11520/13806 [========================>.....] - ETA: 6s - loss: 1.8623 - categorical_accuracy: 0.2757
11648/13806 [========================>.....] - ETA: 6s - loss: 1.8607 - categorical_accuracy: 0.2759
11776/13806 [========================>.....] - ETA: 6s - loss: 1.8594 - categorical_accuracy: 0.2761
11904/13806 [========================>.....] - ETA: 5s - loss: 1.8578 - categorical_accuracy: 0.2764
12032/13806 [=========================>....] - ETA: 5s - loss: 1.8556 - categorical_accuracy: 0.2768
12160/13806 [=========================>....] - ETA: 4s - loss: 1.8539 - categorical_accuracy: 0.2764
12288/13806 [=========================>....] - ETA: 4s - loss: 1.8524 - categorical_accuracy: 0.2766
12416/13806 [=========================>....] - ETA: 4s - loss: 1.8515 - categorical_accuracy: 0.2765
12544/13806 [==========================>...] - ETA: 3s - loss: 1.8500 - categorical_accuracy: 0.2765
12672/13806 [==========================>...] - ETA: 3s - loss: 1.8475 - categorical_accuracy: 0.2777
12800/13806 [==========================>...] - ETA: 2s - loss: 1.8457 - categorical_accuracy: 0.2780
12928/13806 [===========================>..] - ETA: 2s - loss: 1.8441 - categorical_accuracy: 0.2782
13056/13806 [===========================>..] - ETA: 2s - loss: 1.8425 - categorical_accuracy: 0.2787
13184/13806 [===========================>..] - ETA: 1s - loss: 1.8411 - categorical_accuracy: 0.2791
13312/13806 [===========================>..] - ETA: 1s - loss: 1.8396 - categorical_accuracy: 0.2795
13440/13806 [============================>.] - ETA: 1s - loss: 1.8381 - categorical_accuracy: 0.2795
13568/13806 [============================>.] - ETA: 0s - loss: 1.8363 - categorical_accuracy: 0.2798
13696/13806 [============================>.] - ETA: 0s - loss: 1.8346 - categorical_accuracy: 0.2800
13806/13806 [==============================] - 43s 3ms/step - loss: 1.8328 - categorical_accuracy: 0.2800 - val_loss: 1.7101 - val_categorical_accuracy: 0.4838

Epoch 00001: val_categorical_accuracy improved from -inf to 0.48376, saving model to results/vardial2018/multi_input_more_filters_with_more_dropout/model_weights.hdf5
Epoch 2/15

  128/13806 [..............................] - ETA: 39s - loss: 1.6595 - categorical_accuracy: 0.3750
  256/13806 [..............................] - ETA: 38s - loss: 1.6492 - categorical_accuracy: 0.3398
  384/13806 [..............................] - ETA: 37s - loss: 1.6416 - categorical_accuracy: 0.3516
  512/13806 [>.............................] - ETA: 37s - loss: 1.6433 - categorical_accuracy: 0.3457
  640/13806 [>.............................] - ETA: 36s - loss: 1.6292 - categorical_accuracy: 0.3563
  768/13806 [>.............................] - ETA: 36s - loss: 1.6276 - categorical_accuracy: 0.3490
  896/13806 [>.............................] - ETA: 36s - loss: 1.6262 - categorical_accuracy: 0.3482
 1024/13806 [=>............................] - ETA: 35s - loss: 1.6152 - categorical_accuracy: 0.3564
 1152/13806 [=>............................] - ETA: 35s - loss: 1.6186 - categorical_accuracy: 0.3507
 1280/13806 [=>............................] - ETA: 35s - loss: 1.6181 - categorical_accuracy: 0.3469
 1408/13806 [==>...........................] - ETA: 35s - loss: 1.6190 - categorical_accuracy: 0.3445
 1536/13806 [==>...........................] - ETA: 34s - loss: 1.6209 - categorical_accuracy: 0.3438
 1664/13806 [==>...........................] - ETA: 34s - loss: 1.6188 - categorical_accuracy: 0.3456
 1792/13806 [==>...........................] - ETA: 34s - loss: 1.6201 - categorical_accuracy: 0.3387
 1920/13806 [===>..........................] - ETA: 33s - loss: 1.6188 - categorical_accuracy: 0.3375
 2048/13806 [===>..........................] - ETA: 33s - loss: 1.6148 - categorical_accuracy: 0.3442
 2176/13806 [===>..........................] - ETA: 32s - loss: 1.6146 - categorical_accuracy: 0.3442
 2304/13806 [====>.........................] - ETA: 32s - loss: 1.6133 - categorical_accuracy: 0.3442
 2432/13806 [====>.........................] - ETA: 32s - loss: 1.6115 - categorical_accuracy: 0.3446
 2560/13806 [====>.........................] - ETA: 31s - loss: 1.6123 - categorical_accuracy: 0.3430
 2688/13806 [====>.........................] - ETA: 31s - loss: 1.6126 - categorical_accuracy: 0.3397
 2816/13806 [=====>........................] - ETA: 31s - loss: 1.6089 - categorical_accuracy: 0.3409
 2944/13806 [=====>........................] - ETA: 31s - loss: 1.6077 - categorical_accuracy: 0.3421
 3072/13806 [=====>........................] - ETA: 30s - loss: 1.6087 - categorical_accuracy: 0.3389
 3200/13806 [=====>........................] - ETA: 30s - loss: 1.6083 - categorical_accuracy: 0.3366
 3328/13806 [======>.......................] - ETA: 29s - loss: 1.6079 - categorical_accuracy: 0.3344
 3456/13806 [======>.......................] - ETA: 29s - loss: 1.6044 - categorical_accuracy: 0.3354
 3584/13806 [======>.......................] - ETA: 29s - loss: 1.6032 - categorical_accuracy: 0.3371
 3712/13806 [=======>......................] - ETA: 29s - loss: 1.6025 - categorical_accuracy: 0.3373
 3840/13806 [=======>......................] - ETA: 28s - loss: 1.6010 - categorical_accuracy: 0.3375
 3968/13806 [=======>......................] - ETA: 28s - loss: 1.5990 - categorical_accuracy: 0.3357
 4096/13806 [=======>......................] - ETA: 27s - loss: 1.5997 - categorical_accuracy: 0.3354
 4224/13806 [========>.....................] - ETA: 27s - loss: 1.5967 - categorical_accuracy: 0.3364
 4352/13806 [========>.....................] - ETA: 27s - loss: 1.5975 - categorical_accuracy: 0.3362
 4480/13806 [========>.....................] - ETA: 26s - loss: 1.5972 - categorical_accuracy: 0.3353
 4608/13806 [=========>....................] - ETA: 26s - loss: 1.5974 - categorical_accuracy: 0.3336
 4736/13806 [=========>....................] - ETA: 25s - loss: 1.5969 - categorical_accuracy: 0.3340
 4864/13806 [=========>....................] - ETA: 25s - loss: 1.5971 - categorical_accuracy: 0.3322
 4992/13806 [=========>....................] - ETA: 25s - loss: 1.5945 - categorical_accuracy: 0.3343
 5120/13806 [==========>...................] - ETA: 24s - loss: 1.5955 - categorical_accuracy: 0.3316
 5248/13806 [==========>...................] - ETA: 24s - loss: 1.5969 - categorical_accuracy: 0.3293
 5376/13806 [==========>...................] - ETA: 24s - loss: 1.5966 - categorical_accuracy: 0.3287
 5504/13806 [==========>...................] - ETA: 23s - loss: 1.5950 - categorical_accuracy: 0.3296
 5632/13806 [===========>..................] - ETA: 23s - loss: 1.5945 - categorical_accuracy: 0.3299
 5760/13806 [===========>..................] - ETA: 22s - loss: 1.5932 - categorical_accuracy: 0.3307
 5888/13806 [===========>..................] - ETA: 22s - loss: 1.5926 - categorical_accuracy: 0.3297
 6016/13806 [============>.................] - ETA: 22s - loss: 1.5921 - categorical_accuracy: 0.3303
 6144/13806 [============>.................] - ETA: 21s - loss: 1.5920 - categorical_accuracy: 0.3294
 6272/13806 [============>.................] - ETA: 21s - loss: 1.5896 - categorical_accuracy: 0.3307
 6400/13806 [============>.................] - ETA: 21s - loss: 1.5889 - categorical_accuracy: 0.3311
 6528/13806 [=============>................] - ETA: 20s - loss: 1.5869 - categorical_accuracy: 0.3321
 6656/13806 [=============>................] - ETA: 20s - loss: 1.5861 - categorical_accuracy: 0.3322
 6784/13806 [=============>................] - ETA: 20s - loss: 1.5855 - categorical_accuracy: 0.3325
 6912/13806 [==============>...............] - ETA: 19s - loss: 1.5857 - categorical_accuracy: 0.3320
 7040/13806 [==============>...............] - ETA: 19s - loss: 1.5838 - categorical_accuracy: 0.3328
 7168/13806 [==============>...............] - ETA: 18s - loss: 1.5828 - categorical_accuracy: 0.3330
 7296/13806 [==============>...............] - ETA: 18s - loss: 1.5799 - categorical_accuracy: 0.3340
 7424/13806 [===============>..............] - ETA: 18s - loss: 1.5790 - categorical_accuracy: 0.3349
 7552/13806 [===============>..............] - ETA: 17s - loss: 1.5776 - categorical_accuracy: 0.3346
 7680/13806 [===============>..............] - ETA: 17s - loss: 1.5769 - categorical_accuracy: 0.3341
 7808/13806 [===============>..............] - ETA: 17s - loss: 1.5770 - categorical_accuracy: 0.3331
 7936/13806 [================>.............] - ETA: 16s - loss: 1.5766 - categorical_accuracy: 0.3322
 8064/13806 [================>.............] - ETA: 16s - loss: 1.5758 - categorical_accuracy: 0.3330
 8192/13806 [================>.............] - ETA: 16s - loss: 1.5749 - categorical_accuracy: 0.3324
 8320/13806 [=================>............] - ETA: 15s - loss: 1.5752 - categorical_accuracy: 0.3322
 8448/13806 [=================>............] - ETA: 15s - loss: 1.5744 - categorical_accuracy: 0.3324
 8576/13806 [=================>............] - ETA: 14s - loss: 1.5724 - categorical_accuracy: 0.3335
 8704/13806 [=================>............] - ETA: 14s - loss: 1.5709 - categorical_accuracy: 0.3344
 8832/13806 [==================>...........] - ETA: 14s - loss: 1.5701 - categorical_accuracy: 0.3345
 8960/13806 [==================>...........] - ETA: 13s - loss: 1.5691 - categorical_accuracy: 0.3347
 9088/13806 [==================>...........] - ETA: 13s - loss: 1.5683 - categorical_accuracy: 0.3356
 9216/13806 [===================>..........] - ETA: 13s - loss: 1.5682 - categorical_accuracy: 0.3366
 9344/13806 [===================>..........] - ETA: 12s - loss: 1.5680 - categorical_accuracy: 0.3368
 9472/13806 [===================>..........] - ETA: 12s - loss: 1.5673 - categorical_accuracy: 0.3359
 9600/13806 [===================>..........] - ETA: 12s - loss: 1.5675 - categorical_accuracy: 0.3361
 9728/13806 [====================>.........] - ETA: 11s - loss: 1.5673 - categorical_accuracy: 0.3360
 9856/13806 [====================>.........] - ETA: 11s - loss: 1.5658 - categorical_accuracy: 0.3366
 9984/13806 [====================>.........] - ETA: 10s - loss: 1.5655 - categorical_accuracy: 0.3366
10112/13806 [====================>.........] - ETA: 10s - loss: 1.5638 - categorical_accuracy: 0.3373
10240/13806 [=====================>........] - ETA: 10s - loss: 1.5626 - categorical_accuracy: 0.3382
10368/13806 [=====================>........] - ETA: 9s - loss: 1.5618 - categorical_accuracy: 0.3394 
10496/13806 [=====================>........] - ETA: 9s - loss: 1.5613 - categorical_accuracy: 0.3387
10624/13806 [======================>.......] - ETA: 9s - loss: 1.5600 - categorical_accuracy: 0.3391
10752/13806 [======================>.......] - ETA: 8s - loss: 1.5597 - categorical_accuracy: 0.3397
10880/13806 [======================>.......] - ETA: 8s - loss: 1.5591 - categorical_accuracy: 0.3398
11008/13806 [======================>.......] - ETA: 8s - loss: 1.5582 - categorical_accuracy: 0.3403
11136/13806 [=======================>......] - ETA: 7s - loss: 1.5574 - categorical_accuracy: 0.3404
11264/13806 [=======================>......] - ETA: 7s - loss: 1.5569 - categorical_accuracy: 0.3402
11392/13806 [=======================>......] - ETA: 6s - loss: 1.5553 - categorical_accuracy: 0.3409
11520/13806 [========================>.....] - ETA: 6s - loss: 1.5535 - categorical_accuracy: 0.3411
11648/13806 [========================>.....] - ETA: 6s - loss: 1.5527 - categorical_accuracy: 0.3419
11776/13806 [========================>.....] - ETA: 5s - loss: 1.5521 - categorical_accuracy: 0.3421
11904/13806 [========================>.....] - ETA: 5s - loss: 1.5517 - categorical_accuracy: 0.3422
12032/13806 [=========================>....] - ETA: 5s - loss: 1.5508 - categorical_accuracy: 0.3423
12160/13806 [=========================>....] - ETA: 4s - loss: 1.5501 - categorical_accuracy: 0.3427
12288/13806 [=========================>....] - ETA: 4s - loss: 1.5491 - categorical_accuracy: 0.3431
12416/13806 [=========================>....] - ETA: 4s - loss: 1.5486 - categorical_accuracy: 0.3438
12544/13806 [==========================>...] - ETA: 3s - loss: 1.5477 - categorical_accuracy: 0.3438
12672/13806 [==========================>...] - ETA: 3s - loss: 1.5469 - categorical_accuracy: 0.3440
12800/13806 [==========================>...] - ETA: 2s - loss: 1.5456 - categorical_accuracy: 0.3444
12928/13806 [===========================>..] - ETA: 2s - loss: 1.5445 - categorical_accuracy: 0.3448
13056/13806 [===========================>..] - ETA: 2s - loss: 1.5443 - categorical_accuracy: 0.3451
13184/13806 [===========================>..] - ETA: 1s - loss: 1.5438 - categorical_accuracy: 0.3455
13312/13806 [===========================>..] - ETA: 1s - loss: 1.5434 - categorical_accuracy: 0.3459
13440/13806 [============================>.] - ETA: 1s - loss: 1.5434 - categorical_accuracy: 0.3462
13568/13806 [============================>.] - ETA: 0s - loss: 1.5425 - categorical_accuracy: 0.3462
13696/13806 [============================>.] - ETA: 0s - loss: 1.5417 - categorical_accuracy: 0.3467
13806/13806 [==============================] - 41s 3ms/step - loss: 1.5410 - categorical_accuracy: 0.3469 - val_loss: 1.5529 - val_categorical_accuracy: 0.4997

Epoch 00002: val_categorical_accuracy improved from 0.48376 to 0.49967, saving model to results/vardial2018/multi_input_more_filters_with_more_dropout/model_weights.hdf5
Epoch 3/15

  128/13806 [..............................] - ETA: 39s - loss: 1.5277 - categorical_accuracy: 0.3672
  256/13806 [..............................] - ETA: 40s - loss: 1.5255 - categorical_accuracy: 0.3477
  384/13806 [..............................] - ETA: 39s - loss: 1.4929 - categorical_accuracy: 0.3646
  512/13806 [>.............................] - ETA: 39s - loss: 1.4875 - categorical_accuracy: 0.3574
  640/13806 [>.............................] - ETA: 39s - loss: 1.4807 - categorical_accuracy: 0.3656
  768/13806 [>.............................] - ETA: 39s - loss: 1.4777 - categorical_accuracy: 0.3672
  896/13806 [>.............................] - ETA: 39s - loss: 1.4651 - categorical_accuracy: 0.3694
 1024/13806 [=>............................] - ETA: 38s - loss: 1.4694 - categorical_accuracy: 0.3604
 1152/13806 [=>............................] - ETA: 38s - loss: 1.4600 - categorical_accuracy: 0.3655
 1280/13806 [=>............................] - ETA: 37s - loss: 1.4632 - categorical_accuracy: 0.3648
 1408/13806 [==>...........................] - ETA: 37s - loss: 1.4592 - categorical_accuracy: 0.3608
 1536/13806 [==>...........................] - ETA: 36s - loss: 1.4578 - categorical_accuracy: 0.3600
 1664/13806 [==>...........................] - ETA: 36s - loss: 1.4545 - categorical_accuracy: 0.3660
 1792/13806 [==>...........................] - ETA: 35s - loss: 1.4491 - categorical_accuracy: 0.3672
 1920/13806 [===>..........................] - ETA: 35s - loss: 1.4512 - categorical_accuracy: 0.3635
 2048/13806 [===>..........................] - ETA: 35s - loss: 1.4553 - categorical_accuracy: 0.3657
 2176/13806 [===>..........................] - ETA: 34s - loss: 1.4558 - categorical_accuracy: 0.3672
 2304/13806 [====>.........................] - ETA: 34s - loss: 1.4522 - categorical_accuracy: 0.3676
 2432/13806 [====>.........................] - ETA: 34s - loss: 1.4557 - categorical_accuracy: 0.3668
 2560/13806 [====>.........................] - ETA: 33s - loss: 1.4565 - categorical_accuracy: 0.3680
 2688/13806 [====>.........................] - ETA: 33s - loss: 1.4537 - categorical_accuracy: 0.3683
 2816/13806 [=====>........................] - ETA: 32s - loss: 1.4500 - categorical_accuracy: 0.3683
 2944/13806 [=====>........................] - ETA: 32s - loss: 1.4488 - categorical_accuracy: 0.3685
 3072/13806 [=====>........................] - ETA: 32s - loss: 1.4486 - categorical_accuracy: 0.3701
 3200/13806 [=====>........................] - ETA: 31s - loss: 1.4466 - categorical_accuracy: 0.3722
 3328/13806 [======>.......................] - ETA: 31s - loss: 1.4490 - categorical_accuracy: 0.3708
 3456/13806 [======>.......................] - ETA: 31s - loss: 1.4479 - categorical_accuracy: 0.3733
 3584/13806 [======>.......................] - ETA: 30s - loss: 1.4490 - categorical_accuracy: 0.3761
 3712/13806 [=======>......................] - ETA: 30s - loss: 1.4504 - categorical_accuracy: 0.3742
 3840/13806 [=======>......................] - ETA: 30s - loss: 1.4496 - categorical_accuracy: 0.3768
 3968/13806 [=======>......................] - ETA: 29s - loss: 1.4508 - categorical_accuracy: 0.3770
 4096/13806 [=======>......................] - ETA: 29s - loss: 1.4511 - categorical_accuracy: 0.3755
 4224/13806 [========>.....................] - ETA: 28s - loss: 1.4494 - categorical_accuracy: 0.3769
 4352/13806 [========>.....................] - ETA: 28s - loss: 1.4502 - categorical_accuracy: 0.3764
 4480/13806 [========>.....................] - ETA: 28s - loss: 1.4473 - categorical_accuracy: 0.3777
 4608/13806 [=========>....................] - ETA: 27s - loss: 1.4463 - categorical_accuracy: 0.3767
 4736/13806 [=========>....................] - ETA: 27s - loss: 1.4443 - categorical_accuracy: 0.3769
 4864/13806 [=========>....................] - ETA: 27s - loss: 1.4429 - categorical_accuracy: 0.3777
 4992/13806 [=========>....................] - ETA: 26s - loss: 1.4415 - categorical_accuracy: 0.3784
 5120/13806 [==========>...................] - ETA: 26s - loss: 1.4383 - categorical_accuracy: 0.3801
 5248/13806 [==========>...................] - ETA: 25s - loss: 1.4378 - categorical_accuracy: 0.3805
 5376/13806 [==========>...................] - ETA: 25s - loss: 1.4358 - categorical_accuracy: 0.3813
 5504/13806 [==========>...................] - ETA: 25s - loss: 1.4349 - categorical_accuracy: 0.3826
 5632/13806 [===========>..................] - ETA: 24s - loss: 1.4322 - categorical_accuracy: 0.3844
 5760/13806 [===========>..................] - ETA: 24s - loss: 1.4310 - categorical_accuracy: 0.3849
 5888/13806 [===========>..................] - ETA: 23s - loss: 1.4309 - categorical_accuracy: 0.3849
 6016/13806 [============>.................] - ETA: 23s - loss: 1.4299 - categorical_accuracy: 0.3863
 6144/13806 [============>.................] - ETA: 23s - loss: 1.4287 - categorical_accuracy: 0.3864
 6272/13806 [============>.................] - ETA: 22s - loss: 1.4260 - categorical_accuracy: 0.3879
 6400/13806 [============>.................] - ETA: 22s - loss: 1.4259 - categorical_accuracy: 0.3878
 6528/13806 [=============>................] - ETA: 21s - loss: 1.4265 - categorical_accuracy: 0.3877
 6656/13806 [=============>................] - ETA: 21s - loss: 1.4253 - categorical_accuracy: 0.3882
 6784/13806 [=============>................] - ETA: 21s - loss: 1.4239 - categorical_accuracy: 0.3902
 6912/13806 [==============>...............] - ETA: 20s - loss: 1.4226 - categorical_accuracy: 0.3900
 7040/13806 [==============>...............] - ETA: 20s - loss: 1.4213 - categorical_accuracy: 0.3913
 7168/13806 [==============>...............] - ETA: 19s - loss: 1.4201 - categorical_accuracy: 0.3917
 7296/13806 [==============>...............] - ETA: 19s - loss: 1.4192 - categorical_accuracy: 0.3919
 7424/13806 [===============>..............] - ETA: 19s - loss: 1.4188 - categorical_accuracy: 0.3924
 7552/13806 [===============>..............] - ETA: 18s - loss: 1.4193 - categorical_accuracy: 0.3926
 7680/13806 [===============>..............] - ETA: 18s - loss: 1.4182 - categorical_accuracy: 0.3934
 7808/13806 [===============>..............] - ETA: 18s - loss: 1.4165 - categorical_accuracy: 0.3934
 7936/13806 [================>.............] - ETA: 17s - loss: 1.4156 - categorical_accuracy: 0.3943
 8064/13806 [================>.............] - ETA: 17s - loss: 1.4142 - categorical_accuracy: 0.3948
 8192/13806 [================>.............] - ETA: 16s - loss: 1.4124 - categorical_accuracy: 0.3951
 8320/13806 [=================>............] - ETA: 16s - loss: 1.4126 - categorical_accuracy: 0.3948
 8448/13806 [=================>............] - ETA: 16s - loss: 1.4124 - categorical_accuracy: 0.3946
 8576/13806 [=================>............] - ETA: 15s - loss: 1.4134 - categorical_accuracy: 0.3940
 8704/13806 [=================>............] - ETA: 15s - loss: 1.4135 - categorical_accuracy: 0.3941
 8832/13806 [==================>...........] - ETA: 14s - loss: 1.4141 - categorical_accuracy: 0.3939
 8960/13806 [==================>...........] - ETA: 14s - loss: 1.4134 - categorical_accuracy: 0.3937
 9088/13806 [==================>...........] - ETA: 14s - loss: 1.4119 - categorical_accuracy: 0.3944
 9216/13806 [===================>..........] - ETA: 13s - loss: 1.4110 - categorical_accuracy: 0.3953
 9344/13806 [===================>..........] - ETA: 13s - loss: 1.4113 - categorical_accuracy: 0.3954
 9472/13806 [===================>..........] - ETA: 13s - loss: 1.4105 - categorical_accuracy: 0.3957
 9600/13806 [===================>..........] - ETA: 12s - loss: 1.4106 - categorical_accuracy: 0.3956
 9728/13806 [====================>.........] - ETA: 12s - loss: 1.4112 - categorical_accuracy: 0.3953
 9856/13806 [====================>.........] - ETA: 11s - loss: 1.4107 - categorical_accuracy: 0.3950
 9984/13806 [====================>.........] - ETA: 11s - loss: 1.4100 - categorical_accuracy: 0.3951
10112/13806 [====================>.........] - ETA: 11s - loss: 1.4103 - categorical_accuracy: 0.3947
10240/13806 [=====================>........] - ETA: 10s - loss: 1.4097 - categorical_accuracy: 0.3949
10368/13806 [=====================>........] - ETA: 10s - loss: 1.4095 - categorical_accuracy: 0.3953
10496/13806 [=====================>........] - ETA: 9s - loss: 1.4091 - categorical_accuracy: 0.3954 
10624/13806 [======================>.......] - ETA: 9s - loss: 1.4089 - categorical_accuracy: 0.3953
10752/13806 [======================>.......] - ETA: 9s - loss: 1.4090 - categorical_accuracy: 0.3954
10880/13806 [======================>.......] - ETA: 8s - loss: 1.4082 - categorical_accuracy: 0.3962
11008/13806 [======================>.......] - ETA: 8s - loss: 1.4067 - categorical_accuracy: 0.3973
11136/13806 [=======================>......] - ETA: 8s - loss: 1.4056 - categorical_accuracy: 0.3981
11264/13806 [=======================>......] - ETA: 7s - loss: 1.4046 - categorical_accuracy: 0.3985
11392/13806 [=======================>......] - ETA: 7s - loss: 1.4042 - categorical_accuracy: 0.3985
11520/13806 [========================>.....] - ETA: 6s - loss: 1.4041 - categorical_accuracy: 0.3989
11648/13806 [========================>.....] - ETA: 6s - loss: 1.4029 - categorical_accuracy: 0.3996
11776/13806 [========================>.....] - ETA: 6s - loss: 1.4019 - categorical_accuracy: 0.4001
11904/13806 [========================>.....] - ETA: 5s - loss: 1.4015 - categorical_accuracy: 0.3999
12032/13806 [=========================>....] - ETA: 5s - loss: 1.4012 - categorical_accuracy: 0.4005
12160/13806 [=========================>....] - ETA: 4s - loss: 1.4011 - categorical_accuracy: 0.4002
12288/13806 [=========================>....] - ETA: 4s - loss: 1.4002 - categorical_accuracy: 0.4010
12416/13806 [=========================>....] - ETA: 4s - loss: 1.3997 - categorical_accuracy: 0.4005
12544/13806 [==========================>...] - ETA: 3s - loss: 1.3993 - categorical_accuracy: 0.4002
12672/13806 [==========================>...] - ETA: 3s - loss: 1.3994 - categorical_accuracy: 0.3999
12800/13806 [==========================>...] - ETA: 3s - loss: 1.3990 - categorical_accuracy: 0.4004
12928/13806 [===========================>..] - ETA: 2s - loss: 1.3992 - categorical_accuracy: 0.4005
13056/13806 [===========================>..] - ETA: 2s - loss: 1.3989 - categorical_accuracy: 0.4003
13184/13806 [===========================>..] - ETA: 1s - loss: 1.3980 - categorical_accuracy: 0.4006
13312/13806 [===========================>..] - ETA: 1s - loss: 1.3972 - categorical_accuracy: 0.4010
13440/13806 [============================>.] - ETA: 1s - loss: 1.3964 - categorical_accuracy: 0.4012
13568/13806 [============================>.] - ETA: 0s - loss: 1.3945 - categorical_accuracy: 0.4019
13696/13806 [============================>.] - ETA: 0s - loss: 1.3940 - categorical_accuracy: 0.4016
13806/13806 [==============================] - 43s 3ms/step - loss: 1.3932 - categorical_accuracy: 0.4019 - val_loss: 1.4754 - val_categorical_accuracy: 0.5242

Epoch 00003: val_categorical_accuracy improved from 0.49967 to 0.52419, saving model to results/vardial2018/multi_input_more_filters_with_more_dropout/model_weights.hdf5
Epoch 4/15

  128/13806 [..............................] - ETA: 41s - loss: 1.3751 - categorical_accuracy: 0.4375
  256/13806 [..............................] - ETA: 41s - loss: 1.3464 - categorical_accuracy: 0.4180
  384/13806 [..............................] - ETA: 40s - loss: 1.3180 - categorical_accuracy: 0.4271
  512/13806 [>.............................] - ETA: 39s - loss: 1.3480 - categorical_accuracy: 0.4199
  640/13806 [>.............................] - ETA: 39s - loss: 1.3588 - categorical_accuracy: 0.4031
  768/13806 [>.............................] - ETA: 39s - loss: 1.3636 - categorical_accuracy: 0.4049
  896/13806 [>.............................] - ETA: 38s - loss: 1.3625 - categorical_accuracy: 0.3996
 1024/13806 [=>............................] - ETA: 38s - loss: 1.3592 - categorical_accuracy: 0.4072
 1152/13806 [=>............................] - ETA: 37s - loss: 1.3520 - categorical_accuracy: 0.4115
 1280/13806 [=>............................] - ETA: 37s - loss: 1.3555 - categorical_accuracy: 0.4078
 1408/13806 [==>...........................] - ETA: 37s - loss: 1.3514 - categorical_accuracy: 0.4091
 1536/13806 [==>...........................] - ETA: 36s - loss: 1.3527 - categorical_accuracy: 0.4108
 1664/13806 [==>...........................] - ETA: 36s - loss: 1.3471 - categorical_accuracy: 0.4141
 1792/13806 [==>...........................] - ETA: 36s - loss: 1.3451 - categorical_accuracy: 0.4146
 1920/13806 [===>..........................] - ETA: 35s - loss: 1.3482 - categorical_accuracy: 0.4141
 2048/13806 [===>..........................] - ETA: 35s - loss: 1.3486 - categorical_accuracy: 0.4141
 2176/13806 [===>..........................] - ETA: 34s - loss: 1.3486 - categorical_accuracy: 0.4141
 2304/13806 [====>.........................] - ETA: 34s - loss: 1.3463 - categorical_accuracy: 0.4132
 2432/13806 [====>.........................] - ETA: 34s - loss: 1.3465 - categorical_accuracy: 0.4169
 2560/13806 [====>.........................] - ETA: 33s - loss: 1.3447 - categorical_accuracy: 0.4168
 2688/13806 [====>.........................] - ETA: 33s - loss: 1.3461 - categorical_accuracy: 0.4159
 2816/13806 [=====>........................] - ETA: 33s - loss: 1.3489 - categorical_accuracy: 0.4155
 2944/13806 [=====>........................] - ETA: 32s - loss: 1.3448 - categorical_accuracy: 0.4168
 3072/13806 [=====>........................] - ETA: 32s - loss: 1.3478 - categorical_accuracy: 0.4144
 3200/13806 [=====>........................] - ETA: 31s - loss: 1.3472 - categorical_accuracy: 0.4169
 3328/13806 [======>.......................] - ETA: 31s - loss: 1.3461 - categorical_accuracy: 0.4180
 3456/13806 [======>.......................] - ETA: 31s - loss: 1.3480 - categorical_accuracy: 0.4178
 3584/13806 [======>.......................] - ETA: 30s - loss: 1.3517 - categorical_accuracy: 0.4155
 3712/13806 [=======>......................] - ETA: 30s - loss: 1.3510 - categorical_accuracy: 0.4151
 3840/13806 [=======>......................] - ETA: 30s - loss: 1.3508 - categorical_accuracy: 0.4161
 3968/13806 [=======>......................] - ETA: 29s - loss: 1.3475 - categorical_accuracy: 0.4178
 4096/13806 [=======>......................] - ETA: 29s - loss: 1.3464 - categorical_accuracy: 0.4180
 4224/13806 [========>.....................] - ETA: 28s - loss: 1.3444 - categorical_accuracy: 0.4193
 4352/13806 [========>.....................] - ETA: 28s - loss: 1.3427 - categorical_accuracy: 0.4191
 4480/13806 [========>.....................] - ETA: 28s - loss: 1.3410 - categorical_accuracy: 0.4183
 4608/13806 [=========>....................] - ETA: 27s - loss: 1.3406 - categorical_accuracy: 0.4167
 4736/13806 [=========>....................] - ETA: 27s - loss: 1.3392 - categorical_accuracy: 0.4174
 4864/13806 [=========>....................] - ETA: 26s - loss: 1.3368 - categorical_accuracy: 0.4190
 4992/13806 [=========>....................] - ETA: 26s - loss: 1.3363 - categorical_accuracy: 0.4187
 5120/13806 [==========>...................] - ETA: 26s - loss: 1.3372 - categorical_accuracy: 0.4180
 5248/13806 [==========>...................] - ETA: 25s - loss: 1.3389 - categorical_accuracy: 0.4171
 5376/13806 [==========>...................] - ETA: 25s - loss: 1.3373 - categorical_accuracy: 0.4182
 5504/13806 [==========>...................] - ETA: 25s - loss: 1.3362 - categorical_accuracy: 0.4188
 5632/13806 [===========>..................] - ETA: 24s - loss: 1.3344 - categorical_accuracy: 0.4185
 5760/13806 [===========>..................] - ETA: 24s - loss: 1.3350 - categorical_accuracy: 0.4179
 5888/13806 [===========>..................] - ETA: 23s - loss: 1.3356 - categorical_accuracy: 0.4166
 6016/13806 [============>.................] - ETA: 23s - loss: 1.3341 - categorical_accuracy: 0.4181
 6144/13806 [============>.................] - ETA: 23s - loss: 1.3346 - categorical_accuracy: 0.4173
 6272/13806 [============>.................] - ETA: 22s - loss: 1.3354 - categorical_accuracy: 0.4169
 6400/13806 [============>.................] - ETA: 22s - loss: 1.3339 - categorical_accuracy: 0.4177
 6528/13806 [=============>................] - ETA: 21s - loss: 1.3346 - categorical_accuracy: 0.4177
 6656/13806 [=============>................] - ETA: 21s - loss: 1.3347 - categorical_accuracy: 0.4163
 6784/13806 [=============>................] - ETA: 21s - loss: 1.3336 - categorical_accuracy: 0.4172
 6912/13806 [==============>...............] - ETA: 20s - loss: 1.3333 - categorical_accuracy: 0.4175
 7040/13806 [==============>...............] - ETA: 20s - loss: 1.3343 - categorical_accuracy: 0.4159
 7168/13806 [==============>...............] - ETA: 19s - loss: 1.3326 - categorical_accuracy: 0.4166
 7296/13806 [==============>...............] - ETA: 19s - loss: 1.3327 - categorical_accuracy: 0.4161
 7424/13806 [===============>..............] - ETA: 19s - loss: 1.3324 - categorical_accuracy: 0.4169
 7552/13806 [===============>..............] - ETA: 18s - loss: 1.3324 - categorical_accuracy: 0.4160
 7680/13806 [===============>..............] - ETA: 18s - loss: 1.3301 - categorical_accuracy: 0.4169
 7808/13806 [===============>..............] - ETA: 17s - loss: 1.3285 - categorical_accuracy: 0.4169
 7936/13806 [================>.............] - ETA: 17s - loss: 1.3283 - categorical_accuracy: 0.4186
 8064/13806 [================>.............] - ETA: 17s - loss: 1.3295 - categorical_accuracy: 0.4185
 8192/13806 [================>.............] - ETA: 16s - loss: 1.3287 - categorical_accuracy: 0.4198
 8320/13806 [=================>............] - ETA: 16s - loss: 1.3283 - categorical_accuracy: 0.4190
 8448/13806 [=================>............] - ETA: 16s - loss: 1.3286 - categorical_accuracy: 0.4190
 8576/13806 [=================>............] - ETA: 15s - loss: 1.3286 - categorical_accuracy: 0.4191
 8704/13806 [=================>............] - ETA: 15s - loss: 1.3285 - categorical_accuracy: 0.4183
 8832/13806 [==================>...........] - ETA: 14s - loss: 1.3286 - categorical_accuracy: 0.4176
 8960/13806 [==================>...........] - ETA: 14s - loss: 1.3275 - categorical_accuracy: 0.4181
 9088/13806 [==================>...........] - ETA: 14s - loss: 1.3268 - categorical_accuracy: 0.4189
 9216/13806 [===================>..........] - ETA: 13s - loss: 1.3279 - categorical_accuracy: 0.4191
 9344/13806 [===================>..........] - ETA: 13s - loss: 1.3275 - categorical_accuracy: 0.4203
 9472/13806 [===================>..........] - ETA: 12s - loss: 1.3267 - categorical_accuracy: 0.4216
 9600/13806 [===================>..........] - ETA: 12s - loss: 1.3255 - categorical_accuracy: 0.4221
 9728/13806 [====================>.........] - ETA: 12s - loss: 1.3264 - categorical_accuracy: 0.4219
 9856/13806 [====================>.........] - ETA: 11s - loss: 1.3244 - categorical_accuracy: 0.4230
 9984/13806 [====================>.........] - ETA: 11s - loss: 1.3248 - categorical_accuracy: 0.4232
10112/13806 [====================>.........] - ETA: 11s - loss: 1.3245 - categorical_accuracy: 0.4237
10240/13806 [=====================>........] - ETA: 10s - loss: 1.3248 - categorical_accuracy: 0.4233
10368/13806 [=====================>........] - ETA: 10s - loss: 1.3251 - categorical_accuracy: 0.4231
10496/13806 [=====================>........] - ETA: 9s - loss: 1.3261 - categorical_accuracy: 0.4237 
10624/13806 [======================>.......] - ETA: 9s - loss: 1.3258 - categorical_accuracy: 0.4235
10752/13806 [======================>.......] - ETA: 9s - loss: 1.3265 - categorical_accuracy: 0.4230
10880/13806 [======================>.......] - ETA: 8s - loss: 1.3275 - categorical_accuracy: 0.4223
11008/13806 [======================>.......] - ETA: 8s - loss: 1.3279 - categorical_accuracy: 0.4221
11136/13806 [=======================>......] - ETA: 8s - loss: 1.3267 - categorical_accuracy: 0.4229
11264/13806 [=======================>......] - ETA: 7s - loss: 1.3269 - categorical_accuracy: 0.4233
11392/13806 [=======================>......] - ETA: 7s - loss: 1.3277 - categorical_accuracy: 0.4226
11520/13806 [========================>.....] - ETA: 6s - loss: 1.3270 - categorical_accuracy: 0.4228
11648/13806 [========================>.....] - ETA: 6s - loss: 1.3274 - categorical_accuracy: 0.4230
11776/13806 [========================>.....] - ETA: 6s - loss: 1.3273 - categorical_accuracy: 0.4226
11904/13806 [========================>.....] - ETA: 5s - loss: 1.3262 - categorical_accuracy: 0.4231
12032/13806 [=========================>....] - ETA: 5s - loss: 1.3257 - categorical_accuracy: 0.4226
12160/13806 [=========================>....] - ETA: 4s - loss: 1.3264 - categorical_accuracy: 0.4225
12288/13806 [=========================>....] - ETA: 4s - loss: 1.3268 - categorical_accuracy: 0.4224
12416/13806 [=========================>....] - ETA: 4s - loss: 1.3263 - categorical_accuracy: 0.4226
12544/13806 [==========================>...] - ETA: 3s - loss: 1.3255 - categorical_accuracy: 0.4232
12672/13806 [==========================>...] - ETA: 3s - loss: 1.3250 - categorical_accuracy: 0.4229
12800/13806 [==========================>...] - ETA: 3s - loss: 1.3237 - categorical_accuracy: 0.4234
12928/13806 [===========================>..] - ETA: 2s - loss: 1.3226 - categorical_accuracy: 0.4237
13056/13806 [===========================>..] - ETA: 2s - loss: 1.3222 - categorical_accuracy: 0.4249
13184/13806 [===========================>..] - ETA: 1s - loss: 1.3218 - categorical_accuracy: 0.4253
13312/13806 [===========================>..] - ETA: 1s - loss: 1.3215 - categorical_accuracy: 0.4251
13440/13806 [============================>.] - ETA: 1s - loss: 1.3220 - categorical_accuracy: 0.4249
13568/13806 [============================>.] - ETA: 0s - loss: 1.3218 - categorical_accuracy: 0.4247
13696/13806 [============================>.] - ETA: 0s - loss: 1.3219 - categorical_accuracy: 0.4246
13806/13806 [==============================] - 43s 3ms/step - loss: 1.3216 - categorical_accuracy: 0.4246 - val_loss: 1.4218 - val_categorical_accuracy: 0.5149

Epoch 00004: val_categorical_accuracy did not improve
Epoch 5/15

  128/13806 [..............................] - ETA: 41s - loss: 1.2364 - categorical_accuracy: 0.4844
  256/13806 [..............................] - ETA: 40s - loss: 1.2491 - categorical_accuracy: 0.4648
  384/13806 [..............................] - ETA: 39s - loss: 1.2571 - categorical_accuracy: 0.4505
  512/13806 [>.............................] - ETA: 40s - loss: 1.2691 - categorical_accuracy: 0.4590
  640/13806 [>.............................] - ETA: 39s - loss: 1.2795 - categorical_accuracy: 0.4531
  768/13806 [>.............................] - ETA: 39s - loss: 1.2612 - categorical_accuracy: 0.4622
  896/13806 [>.............................] - ETA: 39s - loss: 1.2530 - categorical_accuracy: 0.4643
 1024/13806 [=>............................] - ETA: 38s - loss: 1.2491 - categorical_accuracy: 0.4639
 1152/13806 [=>............................] - ETA: 38s - loss: 1.2438 - categorical_accuracy: 0.4705
 1280/13806 [=>............................] - ETA: 37s - loss: 1.2523 - categorical_accuracy: 0.4617
 1408/13806 [==>...........................] - ETA: 37s - loss: 1.2584 - categorical_accuracy: 0.4602
 1536/13806 [==>...........................] - ETA: 36s - loss: 1.2658 - categorical_accuracy: 0.4557
 1664/13806 [==>...........................] - ETA: 36s - loss: 1.2627 - categorical_accuracy: 0.4549
 1792/13806 [==>...........................] - ETA: 36s - loss: 1.2638 - categorical_accuracy: 0.4537
 1920/13806 [===>..........................] - ETA: 35s - loss: 1.2687 - categorical_accuracy: 0.4484
 2048/13806 [===>..........................] - ETA: 35s - loss: 1.2679 - categorical_accuracy: 0.4487
 2176/13806 [===>..........................] - ETA: 34s - loss: 1.2727 - categorical_accuracy: 0.4444
 2304/13806 [====>.........................] - ETA: 34s - loss: 1.2755 - categorical_accuracy: 0.4466
 2432/13806 [====>.........................] - ETA: 34s - loss: 1.2743 - categorical_accuracy: 0.4437
 2560/13806 [====>.........................] - ETA: 33s - loss: 1.2731 - categorical_accuracy: 0.4453
 2688/13806 [====>.........................] - ETA: 33s - loss: 1.2712 - categorical_accuracy: 0.4472
 2816/13806 [=====>........................] - ETA: 33s - loss: 1.2706 - categorical_accuracy: 0.4464
 2944/13806 [=====>........................] - ETA: 32s - loss: 1.2758 - categorical_accuracy: 0.4416
 3072/13806 [=====>........................] - ETA: 32s - loss: 1.2726 - categorical_accuracy: 0.4421
 3200/13806 [=====>........................] - ETA: 32s - loss: 1.2729 - categorical_accuracy: 0.4437
 3328/13806 [======>.......................] - ETA: 31s - loss: 1.2756 - categorical_accuracy: 0.4429
 3456/13806 [======>.......................] - ETA: 31s - loss: 1.2774 - categorical_accuracy: 0.4430
 3584/13806 [======>.......................] - ETA: 30s - loss: 1.2802 - categorical_accuracy: 0.4414
 3712/13806 [=======>......................] - ETA: 30s - loss: 1.2810 - categorical_accuracy: 0.4399
 3840/13806 [=======>......................] - ETA: 30s - loss: 1.2822 - categorical_accuracy: 0.4393
 3968/13806 [=======>......................] - ETA: 29s - loss: 1.2790 - categorical_accuracy: 0.4400
 4096/13806 [=======>......................] - ETA: 29s - loss: 1.2795 - categorical_accuracy: 0.4409
 4224/13806 [========>.....................] - ETA: 28s - loss: 1.2783 - categorical_accuracy: 0.4406
 4352/13806 [========>.....................] - ETA: 28s - loss: 1.2805 - categorical_accuracy: 0.4382
 4480/13806 [========>.....................] - ETA: 28s - loss: 1.2804 - categorical_accuracy: 0.4375
 4608/13806 [=========>....................] - ETA: 27s - loss: 1.2829 - categorical_accuracy: 0.4362
 4736/13806 [=========>....................] - ETA: 27s - loss: 1.2831 - categorical_accuracy: 0.4371
 4864/13806 [=========>....................] - ETA: 27s - loss: 1.2830 - categorical_accuracy: 0.4373
 4992/13806 [=========>....................] - ETA: 26s - loss: 1.2816 - categorical_accuracy: 0.4379
 5120/13806 [==========>...................] - ETA: 26s - loss: 1.2789 - categorical_accuracy: 0.4391
 5248/13806 [==========>...................] - ETA: 25s - loss: 1.2783 - categorical_accuracy: 0.4405
 5376/13806 [==========>...................] - ETA: 25s - loss: 1.2789 - categorical_accuracy: 0.4399
 5504/13806 [==========>...................] - ETA: 25s - loss: 1.2768 - categorical_accuracy: 0.4404
 5632/13806 [===========>..................] - ETA: 24s - loss: 1.2772 - categorical_accuracy: 0.4411
 5760/13806 [===========>..................] - ETA: 24s - loss: 1.2788 - categorical_accuracy: 0.4418
 5888/13806 [===========>..................] - ETA: 24s - loss: 1.2789 - categorical_accuracy: 0.4423
 6016/13806 [============>.................] - ETA: 23s - loss: 1.2771 - categorical_accuracy: 0.4440
 6144/13806 [============>.................] - ETA: 23s - loss: 1.2765 - categorical_accuracy: 0.4434
 6272/13806 [============>.................] - ETA: 22s - loss: 1.2730 - categorical_accuracy: 0.4447
 6400/13806 [============>.................] - ETA: 22s - loss: 1.2714 - categorical_accuracy: 0.4444
 6528/13806 [=============>................] - ETA: 22s - loss: 1.2711 - categorical_accuracy: 0.4445
 6656/13806 [=============>................] - ETA: 21s - loss: 1.2711 - categorical_accuracy: 0.4447
 6784/13806 [=============>................] - ETA: 21s - loss: 1.2703 - categorical_accuracy: 0.4458
 6912/13806 [==============>...............] - ETA: 20s - loss: 1.2701 - categorical_accuracy: 0.4460
 7040/13806 [==============>...............] - ETA: 20s - loss: 1.2716 - categorical_accuracy: 0.4460
 7168/13806 [==============>...............] - ETA: 20s - loss: 1.2693 - categorical_accuracy: 0.4468
 7296/13806 [==============>...............] - ETA: 19s - loss: 1.2703 - categorical_accuracy: 0.4457
 7424/13806 [===============>..............] - ETA: 19s - loss: 1.2717 - categorical_accuracy: 0.4454
 7552/13806 [===============>..............] - ETA: 18s - loss: 1.2720 - categorical_accuracy: 0.4449
 7680/13806 [===============>..............] - ETA: 18s - loss: 1.2717 - categorical_accuracy: 0.4453
 7808/13806 [===============>..............] - ETA: 18s - loss: 1.2718 - categorical_accuracy: 0.4449
 7936/13806 [================>.............] - ETA: 17s - loss: 1.2701 - categorical_accuracy: 0.4462
 8064/13806 [================>.............] - ETA: 17s - loss: 1.2696 - categorical_accuracy: 0.4474
 8192/13806 [================>.............] - ETA: 17s - loss: 1.2698 - categorical_accuracy: 0.4469
 8320/13806 [=================>............] - ETA: 16s - loss: 1.2707 - categorical_accuracy: 0.4464
 8448/13806 [=================>............] - ETA: 16s - loss: 1.2734 - categorical_accuracy: 0.4460
 8576/13806 [=================>............] - ETA: 15s - loss: 1.2727 - categorical_accuracy: 0.4466
 8704/13806 [=================>............] - ETA: 15s - loss: 1.2722 - categorical_accuracy: 0.4469
 8832/13806 [==================>...........] - ETA: 15s - loss: 1.2708 - categorical_accuracy: 0.4476
 8960/13806 [==================>...........] - ETA: 14s - loss: 1.2708 - categorical_accuracy: 0.4475
 9088/13806 [==================>...........] - ETA: 14s - loss: 1.2699 - categorical_accuracy: 0.4478
 9216/13806 [===================>..........] - ETA: 13s - loss: 1.2705 - categorical_accuracy: 0.4476
 9344/13806 [===================>..........] - ETA: 13s - loss: 1.2701 - categorical_accuracy: 0.4480
 9472/13806 [===================>..........] - ETA: 13s - loss: 1.2696 - categorical_accuracy: 0.4480
 9600/13806 [===================>..........] - ETA: 12s - loss: 1.2693 - categorical_accuracy: 0.4480
 9728/13806 [====================>.........] - ETA: 12s - loss: 1.2683 - categorical_accuracy: 0.4484
 9856/13806 [====================>.........] - ETA: 11s - loss: 1.2678 - categorical_accuracy: 0.4487
 9984/13806 [====================>.........] - ETA: 11s - loss: 1.2676 - categorical_accuracy: 0.4483
10112/13806 [====================>.........] - ETA: 11s - loss: 1.2671 - categorical_accuracy: 0.4488
10240/13806 [=====================>........] - ETA: 10s - loss: 1.2691 - categorical_accuracy: 0.4479
10368/13806 [=====================>........] - ETA: 10s - loss: 1.2685 - categorical_accuracy: 0.4479
10496/13806 [=====================>........] - ETA: 10s - loss: 1.2679 - categorical_accuracy: 0.4484
10624/13806 [======================>.......] - ETA: 9s - loss: 1.2671 - categorical_accuracy: 0.4497 
10752/13806 [======================>.......] - ETA: 9s - loss: 1.2679 - categorical_accuracy: 0.4490
10880/13806 [======================>.......] - ETA: 8s - loss: 1.2682 - categorical_accuracy: 0.4484
11008/13806 [======================>.......] - ETA: 8s - loss: 1.2685 - categorical_accuracy: 0.4477
11136/13806 [=======================>......] - ETA: 8s - loss: 1.2692 - categorical_accuracy: 0.4474
11264/13806 [=======================>......] - ETA: 7s - loss: 1.2688 - categorical_accuracy: 0.4479
11392/13806 [=======================>......] - ETA: 7s - loss: 1.2685 - categorical_accuracy: 0.4476
11520/13806 [========================>.....] - ETA: 6s - loss: 1.2675 - categorical_accuracy: 0.4481
11648/13806 [========================>.....] - ETA: 6s - loss: 1.2678 - categorical_accuracy: 0.4479
11776/13806 [========================>.....] - ETA: 6s - loss: 1.2680 - categorical_accuracy: 0.4487
11904/13806 [========================>.....] - ETA: 5s - loss: 1.2684 - categorical_accuracy: 0.4492
12032/13806 [=========================>....] - ETA: 5s - loss: 1.2687 - categorical_accuracy: 0.4491
12160/13806 [=========================>....] - ETA: 4s - loss: 1.2679 - categorical_accuracy: 0.4506
12288/13806 [=========================>....] - ETA: 4s - loss: 1.2681 - categorical_accuracy: 0.4508
12416/13806 [=========================>....] - ETA: 4s - loss: 1.2687 - categorical_accuracy: 0.4498
12544/13806 [==========================>...] - ETA: 3s - loss: 1.2688 - categorical_accuracy: 0.4498
12672/13806 [==========================>...] - ETA: 3s - loss: 1.2680 - categorical_accuracy: 0.4508
12800/13806 [==========================>...] - ETA: 3s - loss: 1.2675 - categorical_accuracy: 0.4509
12928/13806 [===========================>..] - ETA: 2s - loss: 1.2673 - categorical_accuracy: 0.4510
13056/13806 [===========================>..] - ETA: 2s - loss: 1.2667 - categorical_accuracy: 0.4506
13184/13806 [===========================>..] - ETA: 1s - loss: 1.2664 - categorical_accuracy: 0.4504
13312/13806 [===========================>..] - ETA: 1s - loss: 1.2652 - categorical_accuracy: 0.4506
13440/13806 [============================>.] - ETA: 1s - loss: 1.2642 - categorical_accuracy: 0.4510
13568/13806 [============================>.] - ETA: 0s - loss: 1.2644 - categorical_accuracy: 0.4508
13696/13806 [============================>.] - ETA: 0s - loss: 1.2647 - categorical_accuracy: 0.4503
13806/13806 [==============================] - 43s 3ms/step - loss: 1.2654 - categorical_accuracy: 0.4502 - val_loss: 1.4319 - val_categorical_accuracy: 0.4811

Epoch 00005: val_categorical_accuracy did not improve
Epoch 6/15

  128/13806 [..............................] - ETA: 40s - loss: 1.2658 - categorical_accuracy: 0.4531
  256/13806 [..............................] - ETA: 40s - loss: 1.2736 - categorical_accuracy: 0.4609
  384/13806 [..............................] - ETA: 40s - loss: 1.2347 - categorical_accuracy: 0.4870
  512/13806 [>.............................] - ETA: 39s - loss: 1.2226 - categorical_accuracy: 0.4805
  640/13806 [>.............................] - ETA: 39s - loss: 1.2128 - categorical_accuracy: 0.4875
  768/13806 [>.............................] - ETA: 39s - loss: 1.2272 - categorical_accuracy: 0.4805
  896/13806 [>.............................] - ETA: 38s - loss: 1.2320 - categorical_accuracy: 0.4766
 1024/13806 [=>............................] - ETA: 38s - loss: 1.2395 - categorical_accuracy: 0.4736
 1152/13806 [=>............................] - ETA: 38s - loss: 1.2462 - categorical_accuracy: 0.4653
 1280/13806 [=>............................] - ETA: 37s - loss: 1.2456 - categorical_accuracy: 0.4648
 1408/13806 [==>...........................] - ETA: 37s - loss: 1.2454 - categorical_accuracy: 0.4659
 1536/13806 [==>...........................] - ETA: 37s - loss: 1.2451 - categorical_accuracy: 0.4674
 1664/13806 [==>...........................] - ETA: 36s - loss: 1.2465 - categorical_accuracy: 0.4645
 1792/13806 [==>...........................] - ETA: 36s - loss: 1.2497 - categorical_accuracy: 0.4665
 1920/13806 [===>..........................] - ETA: 35s - loss: 1.2469 - categorical_accuracy: 0.4646
 2048/13806 [===>..........................] - ETA: 35s - loss: 1.2451 - categorical_accuracy: 0.4648
 2176/13806 [===>..........................] - ETA: 35s - loss: 1.2471 - categorical_accuracy: 0.4609
 2304/13806 [====>.........................] - ETA: 34s - loss: 1.2469 - categorical_accuracy: 0.4622
 2432/13806 [====>.........................] - ETA: 34s - loss: 1.2454 - categorical_accuracy: 0.4642
 2560/13806 [====>.........................] - ETA: 33s - loss: 1.2475 - categorical_accuracy: 0.4637
 2688/13806 [====>.........................] - ETA: 33s - loss: 1.2459 - categorical_accuracy: 0.4635
 2816/13806 [=====>........................] - ETA: 33s - loss: 1.2454 - categorical_accuracy: 0.4645
 2944/13806 [=====>........................] - ETA: 32s - loss: 1.2441 - categorical_accuracy: 0.4667
 3072/13806 [=====>........................] - ETA: 32s - loss: 1.2453 - categorical_accuracy: 0.4671
 3200/13806 [=====>........................] - ETA: 31s - loss: 1.2416 - categorical_accuracy: 0.4697
 3328/13806 [======>.......................] - ETA: 31s - loss: 1.2403 - categorical_accuracy: 0.4721
 3456/13806 [======>.......................] - ETA: 31s - loss: 1.2402 - categorical_accuracy: 0.4708
 3584/13806 [======>.......................] - ETA: 30s - loss: 1.2408 - categorical_accuracy: 0.4699
 3712/13806 [=======>......................] - ETA: 30s - loss: 1.2389 - categorical_accuracy: 0.4701
 3840/13806 [=======>......................] - ETA: 29s - loss: 1.2428 - categorical_accuracy: 0.4672
 3968/13806 [=======>......................] - ETA: 29s - loss: 1.2436 - categorical_accuracy: 0.4665
 4096/13806 [=======>......................] - ETA: 29s - loss: 1.2420 - categorical_accuracy: 0.4670
 4224/13806 [========>.....................] - ETA: 28s - loss: 1.2411 - categorical_accuracy: 0.4664
 4352/13806 [========>.....................] - ETA: 28s - loss: 1.2420 - categorical_accuracy: 0.4658
 4480/13806 [========>.....................] - ETA: 28s - loss: 1.2438 - categorical_accuracy: 0.4661
 4608/13806 [=========>....................] - ETA: 27s - loss: 1.2437 - categorical_accuracy: 0.4646
 4736/13806 [=========>....................] - ETA: 27s - loss: 1.2407 - categorical_accuracy: 0.4662
 4864/13806 [=========>....................] - ETA: 26s - loss: 1.2446 - categorical_accuracy: 0.4644
 4992/13806 [=========>....................] - ETA: 26s - loss: 1.2434 - categorical_accuracy: 0.4639
 5120/13806 [==========>...................] - ETA: 26s - loss: 1.2431 - categorical_accuracy: 0.4635
 5248/13806 [==========>...................] - ETA: 25s - loss: 1.2449 - categorical_accuracy: 0.4627
 5376/13806 [==========>...................] - ETA: 25s - loss: 1.2452 - categorical_accuracy: 0.4639
 5504/13806 [==========>...................] - ETA: 25s - loss: 1.2464 - categorical_accuracy: 0.4618
 5632/13806 [===========>..................] - ETA: 24s - loss: 1.2482 - categorical_accuracy: 0.4606
 5760/13806 [===========>..................] - ETA: 24s - loss: 1.2480 - categorical_accuracy: 0.4606
 5888/13806 [===========>..................] - ETA: 23s - loss: 1.2505 - categorical_accuracy: 0.4584
 6016/13806 [============>.................] - ETA: 23s - loss: 1.2521 - categorical_accuracy: 0.4571
 6144/13806 [============>.................] - ETA: 23s - loss: 1.2519 - categorical_accuracy: 0.4585
 6272/13806 [============>.................] - ETA: 22s - loss: 1.2500 - categorical_accuracy: 0.4600
 6400/13806 [============>.................] - ETA: 22s - loss: 1.2524 - categorical_accuracy: 0.4600
 6528/13806 [=============>................] - ETA: 21s - loss: 1.2535 - categorical_accuracy: 0.4593
 6656/13806 [=============>................] - ETA: 21s - loss: 1.2545 - categorical_accuracy: 0.4596
 6784/13806 [=============>................] - ETA: 21s - loss: 1.2551 - categorical_accuracy: 0.4590
 6912/13806 [==============>...............] - ETA: 20s - loss: 1.2568 - categorical_accuracy: 0.4580
 7040/13806 [==============>...............] - ETA: 20s - loss: 1.2542 - categorical_accuracy: 0.4592
 7168/13806 [==============>...............] - ETA: 19s - loss: 1.2535 - categorical_accuracy: 0.4602
 7296/13806 [==============>...............] - ETA: 19s - loss: 1.2540 - categorical_accuracy: 0.4601
 7424/13806 [===============>..............] - ETA: 19s - loss: 1.2523 - categorical_accuracy: 0.4608
 7552/13806 [===============>..............] - ETA: 18s - loss: 1.2528 - categorical_accuracy: 0.4605
 7680/13806 [===============>..............] - ETA: 18s - loss: 1.2542 - categorical_accuracy: 0.4595
 7808/13806 [===============>..............] - ETA: 18s - loss: 1.2549 - categorical_accuracy: 0.4600
 7936/13806 [================>.............] - ETA: 17s - loss: 1.2541 - categorical_accuracy: 0.4602
 8064/13806 [================>.............] - ETA: 17s - loss: 1.2534 - categorical_accuracy: 0.4609
 8192/13806 [================>.............] - ETA: 16s - loss: 1.2531 - categorical_accuracy: 0.4613
 8320/13806 [=================>............] - ETA: 16s - loss: 1.2524 - categorical_accuracy: 0.4612
 8448/13806 [=================>............] - ETA: 16s - loss: 1.2507 - categorical_accuracy: 0.4619
 8576/13806 [=================>............] - ETA: 15s - loss: 1.2520 - categorical_accuracy: 0.4609
 8704/13806 [=================>............] - ETA: 15s - loss: 1.2517 - categorical_accuracy: 0.4612
 8832/13806 [==================>...........] - ETA: 14s - loss: 1.2506 - categorical_accuracy: 0.4611
 8960/13806 [==================>...........] - ETA: 14s - loss: 1.2508 - categorical_accuracy: 0.4606
 9088/13806 [==================>...........] - ETA: 14s - loss: 1.2507 - categorical_accuracy: 0.4605
 9216/13806 [===================>..........] - ETA: 13s - loss: 1.2492 - categorical_accuracy: 0.4617
 9344/13806 [===================>..........] - ETA: 13s - loss: 1.2510 - categorical_accuracy: 0.4609
 9472/13806 [===================>..........] - ETA: 13s - loss: 1.2519 - categorical_accuracy: 0.4602
 9600/13806 [===================>..........] - ETA: 12s - loss: 1.2519 - categorical_accuracy: 0.4600
 9728/13806 [====================>.........] - ETA: 12s - loss: 1.2501 - categorical_accuracy: 0.4606
 9856/13806 [====================>.........] - ETA: 11s - loss: 1.2516 - categorical_accuracy: 0.4604
 9984/13806 [====================>.........] - ETA: 11s - loss: 1.2509 - categorical_accuracy: 0.4607
10112/13806 [====================>.........] - ETA: 11s - loss: 1.2511 - categorical_accuracy: 0.4607
10240/13806 [=====================>........] - ETA: 10s - loss: 1.2499 - categorical_accuracy: 0.4618
10368/13806 [=====================>........] - ETA: 10s - loss: 1.2497 - categorical_accuracy: 0.4625
10496/13806 [=====================>........] - ETA: 9s - loss: 1.2496 - categorical_accuracy: 0.4626 
10624/13806 [======================>.......] - ETA: 9s - loss: 1.2502 - categorical_accuracy: 0.4622
10752/13806 [======================>.......] - ETA: 9s - loss: 1.2499 - categorical_accuracy: 0.4623
10880/13806 [======================>.......] - ETA: 8s - loss: 1.2499 - categorical_accuracy: 0.4631
11008/13806 [======================>.......] - ETA: 8s - loss: 1.2482 - categorical_accuracy: 0.4638
11136/13806 [=======================>......] - ETA: 8s - loss: 1.2494 - categorical_accuracy: 0.4628
11264/13806 [=======================>......] - ETA: 7s - loss: 1.2497 - categorical_accuracy: 0.4636
11392/13806 [=======================>......] - ETA: 7s - loss: 1.2492 - categorical_accuracy: 0.4637
11520/13806 [========================>.....] - ETA: 6s - loss: 1.2493 - categorical_accuracy: 0.4632
11648/13806 [========================>.....] - ETA: 6s - loss: 1.2499 - categorical_accuracy: 0.4627
11776/13806 [========================>.....] - ETA: 6s - loss: 1.2494 - categorical_accuracy: 0.4627
11904/13806 [========================>.....] - ETA: 5s - loss: 1.2495 - categorical_accuracy: 0.4625
12032/13806 [=========================>....] - ETA: 5s - loss: 1.2494 - categorical_accuracy: 0.4625
12160/13806 [=========================>....] - ETA: 4s - loss: 1.2490 - categorical_accuracy: 0.4622
12288/13806 [=========================>....] - ETA: 4s - loss: 1.2483 - categorical_accuracy: 0.4628
12416/13806 [=========================>....] - ETA: 4s - loss: 1.2479 - categorical_accuracy: 0.4628
12544/13806 [==========================>...] - ETA: 3s - loss: 1.2481 - categorical_accuracy: 0.4624
12672/13806 [==========================>...] - ETA: 3s - loss: 1.2484 - categorical_accuracy: 0.4628
12800/13806 [==========================>...] - ETA: 3s - loss: 1.2482 - categorical_accuracy: 0.4627
12928/13806 [===========================>..] - ETA: 2s - loss: 1.2480 - categorical_accuracy: 0.4629
13056/13806 [===========================>..] - ETA: 2s - loss: 1.2482 - categorical_accuracy: 0.4623
13184/13806 [===========================>..] - ETA: 1s - loss: 1.2487 - categorical_accuracy: 0.4618
13312/13806 [===========================>..] - ETA: 1s - loss: 1.2495 - categorical_accuracy: 0.4609
13440/13806 [============================>.] - ETA: 1s - loss: 1.2495 - categorical_accuracy: 0.4605
13568/13806 [============================>.] - ETA: 0s - loss: 1.2497 - categorical_accuracy: 0.4608
13696/13806 [============================>.] - ETA: 0s - loss: 1.2500 - categorical_accuracy: 0.4603
13806/13806 [==============================] - 43s 3ms/step - loss: 1.2495 - categorical_accuracy: 0.4604 - val_loss: 1.4369 - val_categorical_accuracy: 0.4394

Epoch 00006: val_categorical_accuracy did not improve
Epoch 7/15

  128/13806 [..............................] - ETA: 43s - loss: 1.1602 - categorical_accuracy: 0.4688
  256/13806 [..............................] - ETA: 41s - loss: 1.1771 - categorical_accuracy: 0.4922
  384/13806 [..............................] - ETA: 41s - loss: 1.2042 - categorical_accuracy: 0.4948
  512/13806 [>.............................] - ETA: 41s - loss: 1.2244 - categorical_accuracy: 0.4746
  640/13806 [>.............................] - ETA: 40s - loss: 1.2072 - categorical_accuracy: 0.4766
  768/13806 [>.............................] - ETA: 39s - loss: 1.2074 - categorical_accuracy: 0.4740
  896/13806 [>.............................] - ETA: 39s - loss: 1.2224 - categorical_accuracy: 0.4676
 1024/13806 [=>............................] - ETA: 38s - loss: 1.2240 - categorical_accuracy: 0.4688
 1152/13806 [=>............................] - ETA: 38s - loss: 1.2208 - categorical_accuracy: 0.4679
 1280/13806 [=>............................] - ETA: 37s - loss: 1.2299 - categorical_accuracy: 0.4602
 1408/13806 [==>...........................] - ETA: 37s - loss: 1.2299 - categorical_accuracy: 0.4616
 1536/13806 [==>...........................] - ETA: 37s - loss: 1.2285 - categorical_accuracy: 0.4661
 1664/13806 [==>...........................] - ETA: 36s - loss: 1.2325 - categorical_accuracy: 0.4657
 1792/13806 [==>...........................] - ETA: 36s - loss: 1.2342 - categorical_accuracy: 0.4609
 1920/13806 [===>..........................] - ETA: 36s - loss: 1.2306 - categorical_accuracy: 0.4630
 2048/13806 [===>..........................] - ETA: 35s - loss: 1.2263 - categorical_accuracy: 0.4644
 2176/13806 [===>..........................] - ETA: 35s - loss: 1.2215 - categorical_accuracy: 0.4678
 2304/13806 [====>.........................] - ETA: 34s - loss: 1.2184 - categorical_accuracy: 0.4722
 2432/13806 [====>.........................] - ETA: 34s - loss: 1.2266 - categorical_accuracy: 0.4712
 2560/13806 [====>.........................] - ETA: 34s - loss: 1.2299 - categorical_accuracy: 0.4695
 2688/13806 [====>.........................] - ETA: 33s - loss: 1.2316 - categorical_accuracy: 0.4688
 2816/13806 [=====>........................] - ETA: 33s - loss: 1.2321 - categorical_accuracy: 0.4670
 2944/13806 [=====>........................] - ETA: 32s - loss: 1.2352 - categorical_accuracy: 0.4647
 3072/13806 [=====>........................] - ETA: 32s - loss: 1.2413 - categorical_accuracy: 0.4613
 3200/13806 [=====>........................] - ETA: 32s - loss: 1.2416 - categorical_accuracy: 0.4609
 3328/13806 [======>.......................] - ETA: 31s - loss: 1.2410 - categorical_accuracy: 0.4603
 3456/13806 [======>.......................] - ETA: 31s - loss: 1.2400 - categorical_accuracy: 0.4615
 3584/13806 [======>.......................] - ETA: 30s - loss: 1.2381 - categorical_accuracy: 0.4637
 3712/13806 [=======>......................] - ETA: 30s - loss: 1.2372 - categorical_accuracy: 0.4650
 3840/13806 [=======>......................] - ETA: 30s - loss: 1.2362 - categorical_accuracy: 0.4643
 3968/13806 [=======>......................] - ETA: 29s - loss: 1.2350 - categorical_accuracy: 0.4635
 4096/13806 [=======>......................] - ETA: 29s - loss: 1.2379 - categorical_accuracy: 0.4607
 4224/13806 [========>.....................] - ETA: 28s - loss: 1.2396 - categorical_accuracy: 0.4598
 4352/13806 [========>.....................] - ETA: 28s - loss: 1.2423 - categorical_accuracy: 0.4596
 4480/13806 [========>.....................] - ETA: 28s - loss: 1.2376 - categorical_accuracy: 0.4609
 4608/13806 [=========>....................] - ETA: 27s - loss: 1.2381 - categorical_accuracy: 0.4609
 4736/13806 [=========>....................] - ETA: 27s - loss: 1.2390 - categorical_accuracy: 0.4607
 4864/13806 [=========>....................] - ETA: 26s - loss: 1.2399 - categorical_accuracy: 0.4601
 4992/13806 [=========>....................] - ETA: 26s - loss: 1.2369 - categorical_accuracy: 0.4609
 5120/13806 [==========>...................] - ETA: 26s - loss: 1.2381 - categorical_accuracy: 0.4607
 5248/13806 [==========>...................] - ETA: 25s - loss: 1.2369 - categorical_accuracy: 0.4611
 5376/13806 [==========>...................] - ETA: 25s - loss: 1.2389 - categorical_accuracy: 0.4606
 5504/13806 [==========>...................] - ETA: 25s - loss: 1.2399 - categorical_accuracy: 0.4588
 5632/13806 [===========>..................] - ETA: 24s - loss: 1.2390 - categorical_accuracy: 0.4586
 5760/13806 [===========>..................] - ETA: 24s - loss: 1.2405 - categorical_accuracy: 0.4564
 5888/13806 [===========>..................] - ETA: 23s - loss: 1.2397 - categorical_accuracy: 0.4570
 6016/13806 [============>.................] - ETA: 23s - loss: 1.2399 - categorical_accuracy: 0.4579
 6144/13806 [============>.................] - ETA: 23s - loss: 1.2404 - categorical_accuracy: 0.4574
 6272/13806 [============>.................] - ETA: 22s - loss: 1.2403 - categorical_accuracy: 0.4568
 6400/13806 [============>.................] - ETA: 22s - loss: 1.2388 - categorical_accuracy: 0.4578
 6528/13806 [=============>................] - ETA: 21s - loss: 1.2393 - categorical_accuracy: 0.4583
 6656/13806 [=============>................] - ETA: 21s - loss: 1.2410 - categorical_accuracy: 0.4579
 6784/13806 [=============>................] - ETA: 21s - loss: 1.2410 - categorical_accuracy: 0.4574
 6912/13806 [==============>...............] - ETA: 20s - loss: 1.2415 - categorical_accuracy: 0.4570
 7040/13806 [==============>...............] - ETA: 20s - loss: 1.2415 - categorical_accuracy: 0.4565
 7168/13806 [==============>...............] - ETA: 20s - loss: 1.2411 - categorical_accuracy: 0.4566
 7296/13806 [==============>...............] - ETA: 19s - loss: 1.2423 - categorical_accuracy: 0.4560
 7424/13806 [===============>..............] - ETA: 19s - loss: 1.2424 - categorical_accuracy: 0.4564
 7552/13806 [===============>..............] - ETA: 18s - loss: 1.2417 - categorical_accuracy: 0.4572
 7680/13806 [===============>..............] - ETA: 18s - loss: 1.2412 - categorical_accuracy: 0.4573
 7808/13806 [===============>..............] - ETA: 18s - loss: 1.2425 - categorical_accuracy: 0.4562
 7936/13806 [================>.............] - ETA: 17s - loss: 1.2438 - categorical_accuracy: 0.4556
 8064/13806 [================>.............] - ETA: 17s - loss: 1.2431 - categorical_accuracy: 0.4556
 8192/13806 [================>.............] - ETA: 16s - loss: 1.2440 - categorical_accuracy: 0.4546
 8320/13806 [=================>............] - ETA: 16s - loss: 1.2443 - categorical_accuracy: 0.4546
 8448/13806 [=================>............] - ETA: 16s - loss: 1.2432 - categorical_accuracy: 0.4550
 8576/13806 [=================>............] - ETA: 15s - loss: 1.2423 - categorical_accuracy: 0.4552
 8704/13806 [=================>............] - ETA: 15s - loss: 1.2423 - categorical_accuracy: 0.4557
 8832/13806 [==================>...........] - ETA: 15s - loss: 1.2410 - categorical_accuracy: 0.4561
 8960/13806 [==================>...........] - ETA: 14s - loss: 1.2413 - categorical_accuracy: 0.4564
 9088/13806 [==================>...........] - ETA: 14s - loss: 1.2427 - categorical_accuracy: 0.4571
 9216/13806 [===================>..........] - ETA: 13s - loss: 1.2428 - categorical_accuracy: 0.4566
 9344/13806 [===================>..........] - ETA: 13s - loss: 1.2419 - categorical_accuracy: 0.4572
 9472/13806 [===================>..........] - ETA: 13s - loss: 1.2422 - categorical_accuracy: 0.4568
 9600/13806 [===================>..........] - ETA: 12s - loss: 1.2416 - categorical_accuracy: 0.4580
 9728/13806 [====================>.........] - ETA: 12s - loss: 1.2402 - categorical_accuracy: 0.4588
 9856/13806 [====================>.........] - ETA: 11s - loss: 1.2417 - categorical_accuracy: 0.4581
 9984/13806 [====================>.........] - ETA: 11s - loss: 1.2418 - categorical_accuracy: 0.4571
10112/13806 [====================>.........] - ETA: 11s - loss: 1.2410 - categorical_accuracy: 0.4582
10240/13806 [=====================>........] - ETA: 10s - loss: 1.2404 - categorical_accuracy: 0.4587
10368/13806 [=====================>........] - ETA: 10s - loss: 1.2404 - categorical_accuracy: 0.4593
10496/13806 [=====================>........] - ETA: 9s - loss: 1.2397 - categorical_accuracy: 0.4596 
10624/13806 [======================>.......] - ETA: 9s - loss: 1.2397 - categorical_accuracy: 0.4601
10752/13806 [======================>.......] - ETA: 9s - loss: 1.2390 - categorical_accuracy: 0.4612
10880/13806 [======================>.......] - ETA: 8s - loss: 1.2393 - categorical_accuracy: 0.4604
11008/13806 [======================>.......] - ETA: 8s - loss: 1.2394 - categorical_accuracy: 0.4609
11136/13806 [=======================>......] - ETA: 8s - loss: 1.2387 - categorical_accuracy: 0.4618
11264/13806 [=======================>......] - ETA: 7s - loss: 1.2387 - categorical_accuracy: 0.4617
11392/13806 [=======================>......] - ETA: 7s - loss: 1.2389 - categorical_accuracy: 0.4616
11520/13806 [========================>.....] - ETA: 6s - loss: 1.2394 - categorical_accuracy: 0.4611
11648/13806 [========================>.....] - ETA: 6s - loss: 1.2387 - categorical_accuracy: 0.4613
11776/13806 [========================>.....] - ETA: 6s - loss: 1.2390 - categorical_accuracy: 0.4611
11904/13806 [========================>.....] - ETA: 5s - loss: 1.2392 - categorical_accuracy: 0.4614
12032/13806 [=========================>....] - ETA: 5s - loss: 1.2380 - categorical_accuracy: 0.4623
12160/13806 [=========================>....] - ETA: 4s - loss: 1.2392 - categorical_accuracy: 0.4623
12288/13806 [=========================>....] - ETA: 4s - loss: 1.2392 - categorical_accuracy: 0.4625
12416/13806 [=========================>....] - ETA: 4s - loss: 1.2383 - categorical_accuracy: 0.4634
12544/13806 [==========================>...] - ETA: 3s - loss: 1.2378 - categorical_accuracy: 0.4636
12672/13806 [==========================>...] - ETA: 3s - loss: 1.2378 - categorical_accuracy: 0.4631
12800/13806 [==========================>...] - ETA: 3s - loss: 1.2373 - categorical_accuracy: 0.4632
12928/13806 [===========================>..] - ETA: 2s - loss: 1.2371 - categorical_accuracy: 0.4632
13056/13806 [===========================>..] - ETA: 2s - loss: 1.2357 - categorical_accuracy: 0.4638
13184/13806 [===========================>..] - ETA: 1s - loss: 1.2366 - categorical_accuracy: 0.4639
13312/13806 [===========================>..] - ETA: 1s - loss: 1.2362 - categorical_accuracy: 0.4639
13440/13806 [============================>.] - ETA: 1s - loss: 1.2355 - categorical_accuracy: 0.4641
13568/13806 [============================>.] - ETA: 0s - loss: 1.2350 - categorical_accuracy: 0.4643
13696/13806 [============================>.] - ETA: 0s - loss: 1.2349 - categorical_accuracy: 0.4642
13806/13806 [==============================] - 43s 3ms/step - loss: 1.2349 - categorical_accuracy: 0.4647 - val_loss: 1.4408 - val_categorical_accuracy: 0.4155

Epoch 00007: val_categorical_accuracy did not improve
Epoch 8/15

  128/13806 [..............................] - ETA: 40s - loss: 1.3175 - categorical_accuracy: 0.4453
  256/13806 [..............................] - ETA: 40s - loss: 1.2419 - categorical_accuracy: 0.4688
  384/13806 [..............................] - ETA: 41s - loss: 1.1945 - categorical_accuracy: 0.4922
  512/13806 [>.............................] - ETA: 41s - loss: 1.2065 - categorical_accuracy: 0.4824
  640/13806 [>.............................] - ETA: 40s - loss: 1.2290 - categorical_accuracy: 0.4625
  768/13806 [>.............................] - ETA: 39s - loss: 1.2477 - categorical_accuracy: 0.4518
  896/13806 [>.............................] - ETA: 39s - loss: 1.2391 - categorical_accuracy: 0.4542
 1024/13806 [=>............................] - ETA: 39s - loss: 1.2388 - categorical_accuracy: 0.4570
 1152/13806 [=>............................] - ETA: 38s - loss: 1.2399 - categorical_accuracy: 0.4618
 1280/13806 [=>............................] - ETA: 38s - loss: 1.2301 - categorical_accuracy: 0.4633
 1408/13806 [==>...........................] - ETA: 37s - loss: 1.2236 - categorical_accuracy: 0.4680
 1536/13806 [==>...........................] - ETA: 37s - loss: 1.2271 - categorical_accuracy: 0.4701
 1664/13806 [==>...........................] - ETA: 36s - loss: 1.2203 - categorical_accuracy: 0.4736
 1792/13806 [==>...........................] - ETA: 36s - loss: 1.2220 - categorical_accuracy: 0.4738
 1920/13806 [===>..........................] - ETA: 36s - loss: 1.2219 - categorical_accuracy: 0.4708
 2048/13806 [===>..........................] - ETA: 35s - loss: 1.2269 - categorical_accuracy: 0.4697
 2176/13806 [===>..........................] - ETA: 35s - loss: 1.2375 - categorical_accuracy: 0.4692
 2304/13806 [====>.........................] - ETA: 34s - loss: 1.2384 - categorical_accuracy: 0.4622
 2432/13806 [====>.........................] - ETA: 34s - loss: 1.2373 - categorical_accuracy: 0.4634
 2560/13806 [====>.........................] - ETA: 33s - loss: 1.2375 - categorical_accuracy: 0.4617
 2688/13806 [====>.........................] - ETA: 33s - loss: 1.2348 - categorical_accuracy: 0.4628
 2816/13806 [=====>........................] - ETA: 33s - loss: 1.2327 - categorical_accuracy: 0.4648
 2944/13806 [=====>........................] - ETA: 32s - loss: 1.2336 - categorical_accuracy: 0.4654
 3072/13806 [=====>........................] - ETA: 32s - loss: 1.2324 - categorical_accuracy: 0.4661
 3200/13806 [=====>........................] - ETA: 32s - loss: 1.2281 - categorical_accuracy: 0.4688
 3328/13806 [======>.......................] - ETA: 31s - loss: 1.2288 - categorical_accuracy: 0.4684
 3456/13806 [======>.......................] - ETA: 31s - loss: 1.2274 - categorical_accuracy: 0.4679
 3584/13806 [======>.......................] - ETA: 30s - loss: 1.2246 - categorical_accuracy: 0.4701
 3712/13806 [=======>......................] - ETA: 30s - loss: 1.2226 - categorical_accuracy: 0.4717
 3840/13806 [=======>......................] - ETA: 30s - loss: 1.2227 - categorical_accuracy: 0.4708
 3968/13806 [=======>......................] - ETA: 29s - loss: 1.2233 - categorical_accuracy: 0.4713
 4096/13806 [=======>......................] - ETA: 29s - loss: 1.2238 - categorical_accuracy: 0.4702
 4224/13806 [========>.....................] - ETA: 28s - loss: 1.2227 - categorical_accuracy: 0.4718
 4352/13806 [========>.....................] - ETA: 28s - loss: 1.2249 - categorical_accuracy: 0.4697
 4480/13806 [========>.....................] - ETA: 28s - loss: 1.2246 - categorical_accuracy: 0.4714
 4608/13806 [=========>....................] - ETA: 27s - loss: 1.2251 - categorical_accuracy: 0.4711
 4736/13806 [=========>....................] - ETA: 27s - loss: 1.2272 - categorical_accuracy: 0.4715
 4864/13806 [=========>....................] - ETA: 27s - loss: 1.2261 - categorical_accuracy: 0.4725
 4992/13806 [=========>....................] - ETA: 26s - loss: 1.2262 - categorical_accuracy: 0.4724
 5120/13806 [==========>...................] - ETA: 26s - loss: 1.2270 - categorical_accuracy: 0.4699
 5248/13806 [==========>...................] - ETA: 25s - loss: 1.2252 - categorical_accuracy: 0.4708
 5376/13806 [==========>...................] - ETA: 25s - loss: 1.2246 - categorical_accuracy: 0.4710
 5504/13806 [==========>...................] - ETA: 25s - loss: 1.2248 - categorical_accuracy: 0.4697
 5632/13806 [===========>..................] - ETA: 24s - loss: 1.2247 - categorical_accuracy: 0.4695
 5760/13806 [===========>..................] - ETA: 24s - loss: 1.2249 - categorical_accuracy: 0.4688
 5888/13806 [===========>..................] - ETA: 23s - loss: 1.2262 - categorical_accuracy: 0.4679
 6016/13806 [============>.................] - ETA: 23s - loss: 1.2250 - categorical_accuracy: 0.4689
 6144/13806 [============>.................] - ETA: 23s - loss: 1.2255 - categorical_accuracy: 0.4694
 6272/13806 [============>.................] - ETA: 22s - loss: 1.2249 - categorical_accuracy: 0.4697
 6400/13806 [============>.................] - ETA: 22s - loss: 1.2241 - categorical_accuracy: 0.4691
 6528/13806 [=============>................] - ETA: 21s - loss: 1.2239 - categorical_accuracy: 0.4688
 6656/13806 [=============>................] - ETA: 21s - loss: 1.2228 - categorical_accuracy: 0.4684
 6784/13806 [=============>................] - ETA: 21s - loss: 1.2247 - categorical_accuracy: 0.4683
 6912/13806 [==============>...............] - ETA: 20s - loss: 1.2250 - categorical_accuracy: 0.4685
 7040/13806 [==============>...............] - ETA: 20s - loss: 1.2253 - categorical_accuracy: 0.4670
 7168/13806 [==============>...............] - ETA: 20s - loss: 1.2264 - categorical_accuracy: 0.4667
 7296/13806 [==============>...............] - ETA: 19s - loss: 1.2301 - categorical_accuracy: 0.4646
 7424/13806 [===============>..............] - ETA: 19s - loss: 1.2297 - categorical_accuracy: 0.4639
 7552/13806 [===============>..............] - ETA: 18s - loss: 1.2287 - categorical_accuracy: 0.4641
 7680/13806 [===============>..............] - ETA: 18s - loss: 1.2277 - categorical_accuracy: 0.4635
 7808/13806 [===============>..............] - ETA: 18s - loss: 1.2278 - categorical_accuracy: 0.4632
 7936/13806 [================>.............] - ETA: 17s - loss: 1.2253 - categorical_accuracy: 0.4647
 8064/13806 [================>.............] - ETA: 17s - loss: 1.2258 - categorical_accuracy: 0.4642
 8192/13806 [================>.............] - ETA: 16s - loss: 1.2247 - categorical_accuracy: 0.4646
 8320/13806 [=================>............] - ETA: 16s - loss: 1.2233 - categorical_accuracy: 0.4656
 8448/13806 [=================>............] - ETA: 16s - loss: 1.2231 - categorical_accuracy: 0.4667
 8576/13806 [=================>............] - ETA: 15s - loss: 1.2235 - categorical_accuracy: 0.4669
 8704/13806 [=================>............] - ETA: 15s - loss: 1.2233 - categorical_accuracy: 0.4668
 8832/13806 [==================>...........] - ETA: 14s - loss: 1.2225 - categorical_accuracy: 0.4673
 8960/13806 [==================>...........] - ETA: 14s - loss: 1.2223 - categorical_accuracy: 0.4673
 9088/13806 [==================>...........] - ETA: 14s - loss: 1.2238 - categorical_accuracy: 0.4668
 9216/13806 [===================>..........] - ETA: 13s - loss: 1.2232 - categorical_accuracy: 0.4669
 9344/13806 [===================>..........] - ETA: 13s - loss: 1.2230 - categorical_accuracy: 0.4665
 9472/13806 [===================>..........] - ETA: 13s - loss: 1.2234 - categorical_accuracy: 0.4666
 9600/13806 [===================>..........] - ETA: 12s - loss: 1.2221 - categorical_accuracy: 0.4673
 9728/13806 [====================>.........] - ETA: 12s - loss: 1.2226 - categorical_accuracy: 0.4668
 9856/13806 [====================>.........] - ETA: 11s - loss: 1.2211 - categorical_accuracy: 0.4676
 9984/13806 [====================>.........] - ETA: 11s - loss: 1.2205 - categorical_accuracy: 0.4680
10112/13806 [====================>.........] - ETA: 11s - loss: 1.2205 - categorical_accuracy: 0.4679
10240/13806 [=====================>........] - ETA: 10s - loss: 1.2214 - categorical_accuracy: 0.4675
10368/13806 [=====================>........] - ETA: 10s - loss: 1.2218 - categorical_accuracy: 0.4674
10496/13806 [=====================>........] - ETA: 9s - loss: 1.2209 - categorical_accuracy: 0.4687 
10624/13806 [======================>.......] - ETA: 9s - loss: 1.2212 - categorical_accuracy: 0.4675
10752/13806 [======================>.......] - ETA: 9s - loss: 1.2217 - categorical_accuracy: 0.4673
10880/13806 [======================>.......] - ETA: 8s - loss: 1.2224 - categorical_accuracy: 0.4669
11008/13806 [======================>.......] - ETA: 8s - loss: 1.2222 - categorical_accuracy: 0.4666
11136/13806 [=======================>......] - ETA: 8s - loss: 1.2219 - categorical_accuracy: 0.4670
11264/13806 [=======================>......] - ETA: 7s - loss: 1.2211 - categorical_accuracy: 0.4671
11392/13806 [=======================>......] - ETA: 7s - loss: 1.2207 - categorical_accuracy: 0.4669
11520/13806 [========================>.....] - ETA: 6s - loss: 1.2205 - categorical_accuracy: 0.4675
11648/13806 [========================>.....] - ETA: 6s - loss: 1.2197 - categorical_accuracy: 0.4678
11776/13806 [========================>.....] - ETA: 6s - loss: 1.2202 - categorical_accuracy: 0.4677
11904/13806 [========================>.....] - ETA: 5s - loss: 1.2192 - categorical_accuracy: 0.4684
12032/13806 [=========================>....] - ETA: 5s - loss: 1.2201 - categorical_accuracy: 0.4676
12160/13806 [=========================>....] - ETA: 4s - loss: 1.2197 - categorical_accuracy: 0.4680
12288/13806 [=========================>....] - ETA: 4s - loss: 1.2200 - categorical_accuracy: 0.4681
12416/13806 [=========================>....] - ETA: 4s - loss: 1.2195 - categorical_accuracy: 0.4686
12544/13806 [==========================>...] - ETA: 3s - loss: 1.2191 - categorical_accuracy: 0.4688
12672/13806 [==========================>...] - ETA: 3s - loss: 1.2194 - categorical_accuracy: 0.4690
12800/13806 [==========================>...] - ETA: 3s - loss: 1.2199 - categorical_accuracy: 0.4688
12928/13806 [===========================>..] - ETA: 2s - loss: 1.2202 - categorical_accuracy: 0.4683
13056/13806 [===========================>..] - ETA: 2s - loss: 1.2209 - categorical_accuracy: 0.4685
13184/13806 [===========================>..] - ETA: 1s - loss: 1.2209 - categorical_accuracy: 0.4681
13312/13806 [===========================>..] - ETA: 1s - loss: 1.2201 - categorical_accuracy: 0.4682
13440/13806 [============================>.] - ETA: 1s - loss: 1.2219 - categorical_accuracy: 0.4680
13568/13806 [============================>.] - ETA: 0s - loss: 1.2220 - categorical_accuracy: 0.4679
13696/13806 [============================>.] - ETA: 0s - loss: 1.2213 - categorical_accuracy: 0.4679
13806/13806 [==============================] - 43s 3ms/step - loss: 1.2213 - categorical_accuracy: 0.4680 - val_loss: 1.4541 - val_categorical_accuracy: 0.4049
2018-03-23 17:10:25.991434: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:04:00.0, compute capability: 6.1)
/home/michon/anaconda2/envs/py35/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.

Epoch 00008: val_categorical_accuracy did not improve
Epoch 00008: early stopping

Final evaluation

f1_score
 0.36679514561602944
accuracy_score
 0.4049039098740888

classification_report
              precision    recall  f1-score   support

        EGY       0.34      0.59      0.43       297
        GLF       0.49      0.39      0.44       259
        LAV       0.34      0.54      0.42       327
        MSA       1.00      0.03      0.06       280
        NOR       0.57      0.43      0.49       346

avg / total       0.54      0.40      0.37      1509


confusion_matrix
 [[176  11  85   0  25]
 [ 34 102  97   0  26]
 [ 61  45 176   0  45]
 [175  25  57   8  15]
 [ 76  25  96   0 149]]

Evaluation on best model

f1_score
 0.533530467990664
accuracy_score
 0.5241882041086813

classification_report
              precision    recall  f1-score   support

        EGY       0.46      0.55      0.50       297
        GLF       0.48      0.51      0.50       259
        LAV       0.40      0.46      0.43       327
        MSA       0.84      0.57      0.68       280
        NOR       0.59      0.53      0.56       346

avg / total       0.55      0.52      0.53      1509


confusion_matrix
 [[162  21  67  14  33]
 [ 27 133  70   5  24]
 [ 49  62 151  10  55]
 [ 49  24  29 161  17]
 [ 62  37  62   1 184]]
Closing remaining open files:data/vardial2018/dataset.h5...done
############# train: DONE @ Fri Mar 23 17:10:30 CET 2018
