############# train @ Fri Mar 23 10:58:43 CET 2018 GPUS=2  HOST=ssaling11 PWD=/home/michon/projects/VarDial2018/to_export/multi_input_modular
Loading data
Data Configurations loaded
Loading data
(13806, 8)
(1509, 8)
EGY    3085
LAV    2940
NOR    2866
GLF    2707
MSA    2208
Name: Class, dtype: int64
NOR    346
LAV    327
EGY    297
MSA    280
GLF    259
Name: Class, dtype: int64
Loading vocabularies
Words
48244 48244
Phones
45 45
39 39
61 61
51 51
Generating ids
Preprocessing data
Padding character sequences
(13806, 6830)
Padding phone sequences
(13806, 5885) (13806, 7329) (13806, 6436) (13806, 6837)
Turning labels in one-hot vectors
(13806, 5)
Taking ready-made acoustic embeddings
(13806, 600)
Padding character sequences
(1509, 6830)
Padding phone sequences
(1509, 5885) (1509, 7329) (1509, 6436) (1509, 6837)
Turning labels in one-hot vectors
(1509, 5)
Taking ready-made acoustic embeddings
(1509, 600)
MultiInputCharCNN Configurations loaded
Building the model
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
sent_input (InputLayer)         (None, 6830)         0                                            
__________________________________________________________________________________________________
phone_CZ_input (InputLayer)     (None, 5885)         0                                            
__________________________________________________________________________________________________
phone_EN_input (InputLayer)     (None, 7329)         0                                            
__________________________________________________________________________________________________
phone_HU_input (InputLayer)     (None, 6436)         0                                            
__________________________________________________________________________________________________
phone_RU_input (InputLayer)     (None, 6837)         0                                            
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 6830, 64)     6464        sent_input[0][0]                 
__________________________________________________________________________________________________
embedding_2 (Embedding)         (None, 5885, 64)     2944        phone_CZ_input[0][0]             
__________________________________________________________________________________________________
embedding_3 (Embedding)         (None, 7329, 64)     2560        phone_EN_input[0][0]             
__________________________________________________________________________________________________
embedding_4 (Embedding)         (None, 6436, 64)     3968        phone_HU_input[0][0]             
__________________________________________________________________________________________________
embedding_5 (Embedding)         (None, 6837, 64)     3328        phone_RU_input[0][0]             
__________________________________________________________________________________________________
zero_padding1d_1 (ZeroPadding1D (None, 6834, 64)     0           embedding_1[0][0]                
__________________________________________________________________________________________________
zero_padding1d_2 (ZeroPadding1D (None, 6838, 64)     0           embedding_1[0][0]                
__________________________________________________________________________________________________
zero_padding1d_3 (ZeroPadding1D (None, 5889, 64)     0           embedding_2[0][0]                
__________________________________________________________________________________________________
zero_padding1d_4 (ZeroPadding1D (None, 5893, 64)     0           embedding_2[0][0]                
__________________________________________________________________________________________________
zero_padding1d_5 (ZeroPadding1D (None, 7333, 64)     0           embedding_3[0][0]                
__________________________________________________________________________________________________
zero_padding1d_6 (ZeroPadding1D (None, 7337, 64)     0           embedding_3[0][0]                
__________________________________________________________________________________________________
zero_padding1d_7 (ZeroPadding1D (None, 6440, 64)     0           embedding_4[0][0]                
__________________________________________________________________________________________________
zero_padding1d_8 (ZeroPadding1D (None, 6444, 64)     0           embedding_4[0][0]                
__________________________________________________________________________________________________
zero_padding1d_9 (ZeroPadding1D (None, 6841, 64)     0           embedding_5[0][0]                
__________________________________________________________________________________________________
zero_padding1d_10 (ZeroPadding1 (None, 6845, 64)     0           embedding_5[0][0]                
__________________________________________________________________________________________________
conv1d_1 (Conv1D)               (None, 6834, 16)     3088        zero_padding1d_1[0][0]           
__________________________________________________________________________________________________
conv1d_2 (Conv1D)               (None, 6838, 16)     5136        zero_padding1d_2[0][0]           
__________________________________________________________________________________________________
conv1d_3 (Conv1D)               (None, 5889, 16)     3088        zero_padding1d_3[0][0]           
__________________________________________________________________________________________________
conv1d_4 (Conv1D)               (None, 5893, 16)     5136        zero_padding1d_4[0][0]           
__________________________________________________________________________________________________
conv1d_5 (Conv1D)               (None, 7333, 16)     3088        zero_padding1d_5[0][0]           
__________________________________________________________________________________________________
conv1d_6 (Conv1D)               (None, 7337, 16)     5136        zero_padding1d_6[0][0]           
__________________________________________________________________________________________________
conv1d_7 (Conv1D)               (None, 6440, 16)     3088        zero_padding1d_7[0][0]           
__________________________________________________________________________________________________
conv1d_8 (Conv1D)               (None, 6444, 16)     5136        zero_padding1d_8[0][0]           
__________________________________________________________________________________________________
conv1d_9 (Conv1D)               (None, 6841, 16)     3088        zero_padding1d_9[0][0]           
__________________________________________________________________________________________________
conv1d_10 (Conv1D)              (None, 6845, 16)     5136        zero_padding1d_10[0][0]          
__________________________________________________________________________________________________
global_max_pooling1d_1 (GlobalM (None, 16)           0           conv1d_1[0][0]                   
__________________________________________________________________________________________________
global_max_pooling1d_2 (GlobalM (None, 16)           0           conv1d_2[0][0]                   
__________________________________________________________________________________________________
global_max_pooling1d_3 (GlobalM (None, 16)           0           conv1d_3[0][0]                   
__________________________________________________________________________________________________
global_max_pooling1d_4 (GlobalM (None, 16)           0           conv1d_4[0][0]                   
__________________________________________________________________________________________________
global_max_pooling1d_5 (GlobalM (None, 16)           0           conv1d_5[0][0]                   
__________________________________________________________________________________________________
global_max_pooling1d_6 (GlobalM (None, 16)           0           conv1d_6[0][0]                   
__________________________________________________________________________________________________2018-03-23 10:58:53.343726: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-03-23 10:58:53.548765: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335
pciBusID: 0000:03:00.0
totalMemory: 7.92GiB freeMemory: 7.81GiB
2018-03-23 10:58:53.548793: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:03:00.0, compute capability: 6.1)

global_max_pooling1d_7 (GlobalM (None, 16)           0           conv1d_7[0][0]                   
__________________________________________________________________________________________________
global_max_pooling1d_8 (GlobalM (None, 16)           0           conv1d_8[0][0]                   
__________________________________________________________________________________________________
global_max_pooling1d_9 (GlobalM (None, 16)           0           conv1d_9[0][0]                   
__________________________________________________________________________________________________
global_max_pooling1d_10 (Global (None, 16)           0           conv1d_10[0][0]                  
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 160)          0           global_max_pooling1d_1[0][0]     
                                                                 global_max_pooling1d_2[0][0]     
                                                                 global_max_pooling1d_3[0][0]     
                                                                 global_max_pooling1d_4[0][0]     
                                                                 global_max_pooling1d_5[0][0]     
                                                                 global_max_pooling1d_6[0][0]     
                                                                 global_max_pooling1d_7[0][0]     
                                                                 global_max_pooling1d_8[0][0]     
                                                                 global_max_pooling1d_9[0][0]     
                                                                 global_max_pooling1d_10[0][0]    
__________________________________________________________________________________________________
embed_input (InputLayer)        (None, 600)          0                                            
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 760)          0           concatenate_1[0][0]              
                                                                 embed_input[0][0]                
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 128)          97408       concatenate_2[0][0]              
__________________________________________________________________________________________________
l_out (Dense)                   (None, 5)            645         dense_1[0][0]                    
==================================================================================================
Total params: 158,437
Trainable params: 158,437
Non-trainable params: 0
__________________________________________________________________________________________________
Training Configurations loaded
Training the model
no checkpoints available !
Train on 13806 samples, validate on 1509 samples
Epoch 1/15

  128/13806 [..............................] - ETA: 3:18 - loss: 4.0595 - categorical_accuracy: 0.1641
  256/13806 [..............................] - ETA: 2:05 - loss: 3.8868 - categorical_accuracy: 0.1992
  384/13806 [..............................] - ETA: 1:39 - loss: 3.7355 - categorical_accuracy: 0.3698
  512/13806 [>.............................] - ETA: 1:27 - loss: 3.5952 - categorical_accuracy: 0.4902
  640/13806 [>.............................] - ETA: 1:19 - loss: 3.4844 - categorical_accuracy: 0.5687
  768/13806 [>.............................] - ETA: 1:13 - loss: 3.3687 - categorical_accuracy: 0.6263
  896/13806 [>.............................] - ETA: 1:09 - loss: 3.2731 - categorical_accuracy: 0.6618
 1024/13806 [=>............................] - ETA: 1:06 - loss: 3.1978 - categorical_accuracy: 0.6875
 1152/13806 [=>............................] - ETA: 1:04 - loss: 3.1201 - categorical_accuracy: 0.7188
 1280/13806 [=>............................] - ETA: 1:02 - loss: 3.0509 - categorical_accuracy: 0.7430
 1408/13806 [==>...........................] - ETA: 1:00 - loss: 2.9800 - categorical_accuracy: 0.7656
 1536/13806 [==>...........................] - ETA: 59s - loss: 2.9182 - categorical_accuracy: 0.7826 
 1664/13806 [==>...........................] - ETA: 57s - loss: 2.8575 - categorical_accuracy: 0.7975
 1792/13806 [==>...........................] - ETA: 56s - loss: 2.8016 - categorical_accuracy: 0.8119
 1920/13806 [===>..........................] - ETA: 55s - loss: 2.7478 - categorical_accuracy: 0.8245
 2048/13806 [===>..........................] - ETA: 54s - loss: 2.6980 - categorical_accuracy: 0.8350
 2176/13806 [===>..........................] - ETA: 53s - loss: 2.6526 - categorical_accuracy: 0.8428
 2304/13806 [====>.........................] - ETA: 52s - loss: 2.6121 - categorical_accuracy: 0.8498
 2432/13806 [====>.........................] - ETA: 51s - loss: 2.5710 - categorical_accuracy: 0.8569
 2560/13806 [====>.........................] - ETA: 50s - loss: 2.5320 - categorical_accuracy: 0.8633
 2688/13806 [====>.........................] - ETA: 49s - loss: 2.4926 - categorical_accuracy: 0.8698
 2816/13806 [=====>........................] - ETA: 48s - loss: 2.4574 - categorical_accuracy: 0.8754
 2944/13806 [=====>........................] - ETA: 47s - loss: 2.4237 - categorical_accuracy: 0.8801
 3072/13806 [=====>........................] - ETA: 46s - loss: 2.3915 - categorical_accuracy: 0.8841
 3200/13806 [=====>........................] - ETA: 46s - loss: 2.3579 - categorical_accuracy: 0.8888
 3328/13806 [======>.......................] - ETA: 45s - loss: 2.3265 - categorical_accuracy: 0.8930
 3456/13806 [======>.......................] - ETA: 44s - loss: 2.2962 - categorical_accuracy: 0.8967
 3584/13806 [======>.......................] - ETA: 44s - loss: 2.2669 - categorical_accuracy: 0.8998
 3712/13806 [=======>......................] - ETA: 43s - loss: 2.2382 - categorical_accuracy: 0.9033
 3840/13806 [=======>......................] - ETA: 42s - loss: 2.2100 - categorical_accuracy: 0.9062
 3968/13806 [=======>......................] - ETA: 42s - loss: 2.1828 - categorical_accuracy: 0.9093
 4096/13806 [=======>......................] - ETA: 41s - loss: 2.1565 - categorical_accuracy: 0.9119
 4224/13806 [========>.....................] - ETA: 40s - loss: 2.1320 - categorical_accuracy: 0.9138
 4352/13806 [========>.....................] - ETA: 40s - loss: 2.1084 - categorical_accuracy: 0.9157
 4480/13806 [========>.....................] - ETA: 39s - loss: 2.0854 - categorical_accuracy: 0.9176
 4608/13806 [=========>....................] - ETA: 38s - loss: 2.0625 - categorical_accuracy: 0.9193
 4736/13806 [=========>....................] - ETA: 38s - loss: 2.0390 - categorical_accuracy: 0.9212
 4864/13806 [=========>....................] - ETA: 37s - loss: 2.0169 - categorical_accuracy: 0.9227
 4992/13806 [=========>....................] - ETA: 37s - loss: 1.9954 - categorical_accuracy: 0.9241
 5120/13806 [==========>...................] - ETA: 36s - loss: 1.9740 - categorical_accuracy: 0.9256
 5248/13806 [==========>...................] - ETA: 35s - loss: 1.9531 - categorical_accuracy: 0.9270
 5376/13806 [==========>...................] - ETA: 35s - loss: 1.9328 - categorical_accuracy: 0.9282
 5504/13806 [==========>...................] - ETA: 34s - loss: 1.9118 - categorical_accuracy: 0.9299
 5632/13806 [===========>..................] - ETA: 34s - loss: 1.8915 - categorical_accuracy: 0.9313
 5760/13806 [===========>..................] - ETA: 33s - loss: 1.8719 - categorical_accuracy: 0.9325
 5888/13806 [===========>..................] - ETA: 32s - loss: 1.8542 - categorical_accuracy: 0.9334
 6016/13806 [============>.................] - ETA: 32s - loss: 1.8352 - categorical_accuracy: 0.9347
 6144/13806 [============>.................] - ETA: 31s - loss: 1.8170 - categorical_accuracy: 0.9357
 6272/13806 [============>.................] - ETA: 31s - loss: 1.7986 - categorical_accuracy: 0.9370
 6400/13806 [============>.................] - ETA: 30s - loss: 1.7831 - categorical_accuracy: 0.9375
 6528/13806 [=============>................] - ETA: 30s - loss: 1.7659 - categorical_accuracy: 0.9383
 6656/13806 [=============>................] - ETA: 29s - loss: 1.7481 - categorical_accuracy: 0.9395
 6784/13806 [=============>................] - ETA: 29s - loss: 1.7327 - categorical_accuracy: 0.9399
 6912/13806 [==============>...............] - ETA: 28s - loss: 1.7156 - categorical_accuracy: 0.9410
 7040/13806 [==============>...............] - ETA: 27s - loss: 1.7006 - categorical_accuracy: 0.9415
 7168/13806 [==============>...............] - ETA: 27s - loss: 1.6845 - categorical_accuracy: 0.9424
 7296/13806 [==============>...............] - ETA: 26s - loss: 1.6684 - categorical_accuracy: 0.9433
 7424/13806 [===============>..............] - ETA: 26s - loss: 1.6533 - categorical_accuracy: 0.9441
 7552/13806 [===============>..............] - ETA: 25s - loss: 1.6376 - categorical_accuracy: 0.9450
 7680/13806 [===============>..............] - ETA: 25s - loss: 1.6226 - categorical_accuracy: 0.9458
 7808/13806 [===============>..............] - ETA: 24s - loss: 1.6086 - categorical_accuracy: 0.9463
 7936/13806 [================>.............] - ETA: 24s - loss: 1.5939 - categorical_accuracy: 0.9471
 8064/13806 [================>.............] - ETA: 23s - loss: 1.5794 - categorical_accuracy: 0.9478
 8192/13806 [================>.............] - ETA: 23s - loss: 1.5664 - categorical_accuracy: 0.9481
 8320/13806 [=================>............] - ETA: 22s - loss: 1.5534 - categorical_accuracy: 0.9483
 8448/13806 [=================>............] - ETA: 21s - loss: 1.5397 - categorical_accuracy: 0.9489
 8576/13806 [=================>............] - ETA: 21s - loss: 1.5264 - categorical_accuracy: 0.9494
 8704/13806 [=================>............] - ETA: 20s - loss: 1.5130 - categorical_accuracy: 0.9501
 8832/13806 [==================>...........] - ETA: 20s - loss: 1.4996 - categorical_accuracy: 0.9509
 8960/13806 [==================>...........] - ETA: 19s - loss: 1.4871 - categorical_accuracy: 0.9512
 9088/13806 [==================>...........] - ETA: 19s - loss: 1.4742 - categorical_accuracy: 0.9519
 9216/13806 [===================>..........] - ETA: 18s - loss: 1.4616 - categorical_accuracy: 0.9525
 9344/13806 [===================>..........] - ETA: 18s - loss: 1.4495 - categorical_accuracy: 0.9530
 9472/13806 [===================>..........] - ETA: 17s - loss: 1.4374 - categorical_accuracy: 0.9535
 9600/13806 [===================>..........] - ETA: 17s - loss: 1.4256 - categorical_accuracy: 0.9541
 9728/13806 [====================>.........] - ETA: 16s - loss: 1.4137 - categorical_accuracy: 0.9546
 9856/13806 [====================>.........] - ETA: 16s - loss: 1.4031 - categorical_accuracy: 0.9550
 9984/13806 [====================>.........] - ETA: 15s - loss: 1.3916 - categorical_accuracy: 0.9554
10112/13806 [====================>.........] - ETA: 15s - loss: 1.3805 - categorical_accuracy: 0.9558
10240/13806 [=====================>........] - ETA: 14s - loss: 1.3698 - categorical_accuracy: 0.9561
10368/13806 [=====================>........] - ETA: 14s - loss: 1.3594 - categorical_accuracy: 0.9564
10496/13806 [=====================>........] - ETA: 13s - loss: 1.3487 - categorical_accuracy: 0.9567
10624/13806 [======================>.......] - ETA: 12s - loss: 1.3388 - categorical_accuracy: 0.9571
10752/13806 [======================>.......] - ETA: 12s - loss: 1.3285 - categorical_accuracy: 0.9575
10880/13806 [======================>.......] - ETA: 11s - loss: 1.3190 - categorical_accuracy: 0.9576
11008/13806 [======================>.......] - ETA: 11s - loss: 1.3100 - categorical_accuracy: 0.9578
11136/13806 [=======================>......] - ETA: 10s - loss: 1.3000 - categorical_accuracy: 0.9581
11264/13806 [=======================>......] - ETA: 10s - loss: 1.2901 - categorical_accuracy: 0.9585
11392/13806 [=======================>......] - ETA: 9s - loss: 1.2809 - categorical_accuracy: 0.9587 
11520/13806 [========================>.....] - ETA: 9s - loss: 1.2714 - categorical_accuracy: 0.9591
11648/13806 [========================>.....] - ETA: 8s - loss: 1.2622 - categorical_accuracy: 0.9594
11776/13806 [========================>.....] - ETA: 8s - loss: 1.2530 - categorical_accuracy: 0.9597
11904/13806 [========================>.....] - ETA: 7s - loss: 1.2439 - categorical_accuracy: 0.9600
12032/13806 [=========================>....] - ETA: 7s - loss: 1.2356 - categorical_accuracy: 0.9602
12160/13806 [=========================>....] - ETA: 6s - loss: 1.2266 - categorical_accuracy: 0.9606
12288/13806 [=========================>....] - ETA: 6s - loss: 1.2177 - categorical_accuracy: 0.9610
12416/13806 [=========================>....] - ETA: 5s - loss: 1.2093 - categorical_accuracy: 0.9613
12544/13806 [==========================>...] - ETA: 5s - loss: 1.2015 - categorical_accuracy: 0.9614
12672/13806 [==========================>...] - ETA: 4s - loss: 1.1934 - categorical_accuracy: 0.9616
12800/13806 [==========================>...] - ETA: 4s - loss: 1.1850 - categorical_accuracy: 0.9619
12928/13806 [===========================>..] - ETA: 3s - loss: 1.1769 - categorical_accuracy: 0.9622
13056/13806 [===========================>..] - ETA: 3s - loss: 1.1691 - categorical_accuracy: 0.9625
13184/13806 [===========================>..] - ETA: 2s - loss: 1.1612 - categorical_accuracy: 0.9628
13312/13806 [===========================>..] - ETA: 2s - loss: 1.1537 - categorical_accuracy: 0.9629
13440/13806 [============================>.] - ETA: 1s - loss: 1.1460 - categorical_accuracy: 0.9632
13568/13806 [============================>.] - ETA: 0s - loss: 1.1386 - categorical_accuracy: 0.9634
13696/13806 [============================>.] - ETA: 0s - loss: 1.1317 - categorical_accuracy: 0.9635
13806/13806 [==============================] - 58s 4ms/step - loss: 1.1253 - categorical_accuracy: 0.9636 - val_loss: 1.5794 - val_categorical_accuracy: 0.5374

Epoch 00001: val_categorical_accuracy improved from -inf to 0.53744, saving model to results/vardial2018/multi_input/model_weights.hdf5
Epoch 2/15

  128/13806 [..............................] - ETA: 53s - loss: 0.3004 - categorical_accuracy: 1.0000
  256/13806 [..............................] - ETA: 54s - loss: 0.3265 - categorical_accuracy: 0.9883
  384/13806 [..............................] - ETA: 53s - loss: 0.3170 - categorical_accuracy: 0.9896
  512/13806 [>.............................] - ETA: 52s - loss: 0.3092 - categorical_accuracy: 0.9922
  640/13806 [>.............................] - ETA: 52s - loss: 0.3053 - categorical_accuracy: 0.9922
  768/13806 [>.............................] - ETA: 51s - loss: 0.3123 - categorical_accuracy: 0.9896
  896/13806 [>.............................] - ETA: 51s - loss: 0.3143 - categorical_accuracy: 0.9888
 1024/13806 [=>............................] - ETA: 50s - loss: 0.3094 - categorical_accuracy: 0.9893
 1152/13806 [=>............................] - ETA: 50s - loss: 0.3069 - categorical_accuracy: 0.9896
 1280/13806 [=>............................] - ETA: 49s - loss: 0.3022 - categorical_accuracy: 0.9906
 1408/13806 [==>...........................] - ETA: 49s - loss: 0.3041 - categorical_accuracy: 0.9908
 1536/13806 [==>...........................] - ETA: 48s - loss: 0.3023 - categorical_accuracy: 0.9909
 1664/13806 [==>...........................] - ETA: 48s - loss: 0.2993 - categorical_accuracy: 0.9916
 1792/13806 [==>...........................] - ETA: 47s - loss: 0.3050 - categorical_accuracy: 0.9900
 1920/13806 [===>..........................] - ETA: 47s - loss: 0.3069 - categorical_accuracy: 0.9891
 2048/13806 [===>..........................] - ETA: 46s - loss: 0.3055 - categorical_accuracy: 0.9888
 2176/13806 [===>..........................] - ETA: 46s - loss: 0.3018 - categorical_accuracy: 0.9894
 2304/13806 [====>.........................] - ETA: 45s - loss: 0.3025 - categorical_accuracy: 0.9891
 2432/13806 [====>.........................] - ETA: 45s - loss: 0.3002 - categorical_accuracy: 0.9893
 2560/13806 [====>.........................] - ETA: 44s - loss: 0.2971 - categorical_accuracy: 0.9898
 2688/13806 [====>.........................] - ETA: 44s - loss: 0.2948 - categorical_accuracy: 0.9896
 2816/13806 [=====>........................] - ETA: 43s - loss: 0.2927 - categorical_accuracy: 0.9897
 2944/13806 [=====>........................] - ETA: 43s - loss: 0.2932 - categorical_accuracy: 0.9891
 3072/13806 [=====>........................] - ETA: 42s - loss: 0.2903 - categorical_accuracy: 0.9896
 3200/13806 [=====>........................] - ETA: 42s - loss: 0.2884 - categorical_accuracy: 0.9891
 3328/13806 [======>.......................] - ETA: 41s - loss: 0.2874 - categorical_accuracy: 0.9889
 3456/13806 [======>.......................] - ETA: 41s - loss: 0.2858 - categorical_accuracy: 0.9887
 3584/13806 [======>.......................] - ETA: 40s - loss: 0.2843 - categorical_accuracy: 0.9888
 3712/13806 [=======>......................] - ETA: 40s - loss: 0.2822 - categorical_accuracy: 0.9887
 3840/13806 [=======>......................] - ETA: 39s - loss: 0.2803 - categorical_accuracy: 0.9888
 3968/13806 [=======>......................] - ETA: 39s - loss: 0.2789 - categorical_accuracy: 0.9887
 4096/13806 [=======>......................] - ETA: 38s - loss: 0.2790 - categorical_accuracy: 0.9883
 4224/13806 [========>.....................] - ETA: 38s - loss: 0.2774 - categorical_accuracy: 0.9882
 4352/13806 [========>.....................] - ETA: 37s - loss: 0.2755 - categorical_accuracy: 0.9883
 4480/13806 [========>.....................] - ETA: 37s - loss: 0.2750 - categorical_accuracy: 0.9884
 4608/13806 [=========>....................] - ETA: 36s - loss: 0.2768 - categorical_accuracy: 0.9878
 4736/13806 [=========>....................] - ETA: 36s - loss: 0.2750 - categorical_accuracy: 0.9880
 4864/13806 [=========>....................] - ETA: 35s - loss: 0.2730 - categorical_accuracy: 0.9881
 4992/13806 [=========>....................] - ETA: 34s - loss: 0.2735 - categorical_accuracy: 0.9876
 5120/13806 [==========>...................] - ETA: 34s - loss: 0.2738 - categorical_accuracy: 0.9871
 5248/13806 [==========>...................] - ETA: 33s - loss: 0.2717 - categorical_accuracy: 0.9872
 5376/13806 [==========>...................] - ETA: 33s - loss: 0.2709 - categorical_accuracy: 0.9868
 5504/13806 [==========>...................] - ETA: 32s - loss: 0.2708 - categorical_accuracy: 0.9866
 5632/13806 [===========>..................] - ETA: 32s - loss: 0.2706 - categorical_accuracy: 0.9863
 5760/13806 [===========>..................] - ETA: 32s - loss: 0.2692 - categorical_accuracy: 0.9865
 5888/13806 [===========>..................] - ETA: 31s - loss: 0.2692 - categorical_accuracy: 0.9861
 6016/13806 [============>.................] - ETA: 30s - loss: 0.2680 - categorical_accuracy: 0.9859
 6144/13806 [============>.................] - ETA: 30s - loss: 0.2677 - categorical_accuracy: 0.9857
 6272/13806 [============>.................] - ETA: 29s - loss: 0.2691 - categorical_accuracy: 0.9852
 6400/13806 [============>.................] - ETA: 29s - loss: 0.2682 - categorical_accuracy: 0.9852
 6528/13806 [=============>................] - ETA: 28s - loss: 0.2667 - categorical_accuracy: 0.9853
 6656/13806 [=============>................] - ETA: 28s - loss: 0.2651 - categorical_accuracy: 0.9853
 6784/13806 [=============>................] - ETA: 27s - loss: 0.2661 - categorical_accuracy: 0.9848
 6912/13806 [==============>...............] - ETA: 27s - loss: 0.2653 - categorical_accuracy: 0.9848
 7040/13806 [==============>...............] - ETA: 26s - loss: 0.2638 - categorical_accuracy: 0.9848
 7168/13806 [==============>...............] - ETA: 26s - loss: 0.2625 - categorical_accuracy: 0.9848
 7296/13806 [==============>...............] - ETA: 25s - loss: 0.2612 - categorical_accuracy: 0.9848
 7424/13806 [===============>..............] - ETA: 25s - loss: 0.2604 - categorical_accuracy: 0.9846
 7552/13806 [===============>..............] - ETA: 24s - loss: 0.2593 - categorical_accuracy: 0.9846
 7680/13806 [===============>..............] - ETA: 24s - loss: 0.2578 - categorical_accuracy: 0.9848
 7808/13806 [===============>..............] - ETA: 23s - loss: 0.2577 - categorical_accuracy: 0.9845
 7936/13806 [================>.............] - ETA: 23s - loss: 0.2571 - categorical_accuracy: 0.9844
 8064/13806 [================>.............] - ETA: 22s - loss: 0.2558 - categorical_accuracy: 0.9844
 8192/13806 [================>.............] - ETA: 22s - loss: 0.2546 - categorical_accuracy: 0.9844
 8320/13806 [=================>............] - ETA: 21s - loss: 0.2533 - categorical_accuracy: 0.9845
 8448/13806 [=================>............] - ETA: 21s - loss: 0.2518 - categorical_accuracy: 0.9846
 8576/13806 [=================>............] - ETA: 20s - loss: 0.2513 - categorical_accuracy: 0.9844
 8704/13806 [=================>............] - ETA: 20s - loss: 0.2504 - categorical_accuracy: 0.9843
 8832/13806 [==================>...........] - ETA: 19s - loss: 0.2491 - categorical_accuracy: 0.9843
 8960/13806 [==================>...........] - ETA: 19s - loss: 0.2483 - categorical_accuracy: 0.9842
 9088/13806 [==================>...........] - ETA: 18s - loss: 0.2475 - categorical_accuracy: 0.9842
 9216/13806 [===================>..........] - ETA: 18s - loss: 0.2465 - categorical_accuracy: 0.9842
 9344/13806 [===================>..........] - ETA: 17s - loss: 0.2456 - categorical_accuracy: 0.9842
 9472/13806 [===================>..........] - ETA: 17s - loss: 0.2456 - categorical_accuracy: 0.9840
 9600/13806 [===================>..........] - ETA: 16s - loss: 0.2443 - categorical_accuracy: 0.9842
 9728/13806 [====================>.........] - ETA: 16s - loss: 0.2435 - categorical_accuracy: 0.9843
 9856/13806 [====================>.........] - ETA: 15s - loss: 0.2422 - categorical_accuracy: 0.9844
 9984/13806 [====================>.........] - ETA: 15s - loss: 0.2418 - categorical_accuracy: 0.9844
10112/13806 [====================>.........] - ETA: 14s - loss: 0.2409 - categorical_accuracy: 0.9844
10240/13806 [=====================>........] - ETA: 14s - loss: 0.2410 - categorical_accuracy: 0.9841
10368/13806 [=====================>........] - ETA: 13s - loss: 0.2405 - categorical_accuracy: 0.9841
10496/13806 [=====================>........] - ETA: 13s - loss: 0.2393 - categorical_accuracy: 0.9842
10624/13806 [======================>.......] - ETA: 12s - loss: 0.2386 - categorical_accuracy: 0.9841
10752/13806 [======================>.......] - ETA: 12s - loss: 0.2376 - categorical_accuracy: 0.9842
10880/13806 [======================>.......] - ETA: 11s - loss: 0.2369 - categorical_accuracy: 0.9842
11008/13806 [======================>.......] - ETA: 11s - loss: 0.2358 - categorical_accuracy: 0.9843
11136/13806 [=======================>......] - ETA: 10s - loss: 0.2349 - categorical_accuracy: 0.9843
11264/13806 [=======================>......] - ETA: 10s - loss: 0.2337 - categorical_accuracy: 0.9845
11392/13806 [=======================>......] - ETA: 9s - loss: 0.2325 - categorical_accuracy: 0.9846 
11520/13806 [========================>.....] - ETA: 9s - loss: 0.2315 - categorical_accuracy: 0.9846
11648/13806 [========================>.....] - ETA: 8s - loss: 0.2305 - categorical_accuracy: 0.9848
11776/13806 [========================>.....] - ETA: 8s - loss: 0.2295 - categorical_accuracy: 0.9847
11904/13806 [========================>.....] - ETA: 7s - loss: 0.2285 - categorical_accuracy: 0.9848
12032/13806 [=========================>....] - ETA: 7s - loss: 0.2276 - categorical_accuracy: 0.9848
12160/13806 [=========================>....] - ETA: 6s - loss: 0.2268 - categorical_accuracy: 0.9848
12288/13806 [=========================>....] - ETA: 6s - loss: 0.2263 - categorical_accuracy: 0.9847
12416/13806 [=========================>....] - ETA: 5s - loss: 0.2257 - categorical_accuracy: 0.9846
12544/13806 [==========================>...] - ETA: 5s - loss: 0.2254 - categorical_accuracy: 0.9847
12672/13806 [==========================>...] - ETA: 4s - loss: 0.2245 - categorical_accuracy: 0.9848
12800/13806 [==========================>...] - ETA: 4s - loss: 0.2234 - categorical_accuracy: 0.9848
12928/13806 [===========================>..] - ETA: 3s - loss: 0.2230 - categorical_accuracy: 0.9848
13056/13806 [===========================>..] - ETA: 3s - loss: 0.2220 - categorical_accuracy: 0.9850
13184/13806 [===========================>..] - ETA: 2s - loss: 0.2211 - categorical_accuracy: 0.9851
13312/13806 [===========================>..] - ETA: 1s - loss: 0.2208 - categorical_accuracy: 0.9851
13440/13806 [============================>.] - ETA: 1s - loss: 0.2206 - categorical_accuracy: 0.9850
13568/13806 [============================>.] - ETA: 0s - loss: 0.2197 - categorical_accuracy: 0.9850
13696/13806 [============================>.] - ETA: 0s - loss: 0.2189 - categorical_accuracy: 0.9850
13806/13806 [==============================] - 57s 4ms/step - loss: 0.2181 - categorical_accuracy: 0.9851 - val_loss: 1.4489 - val_categorical_accuracy: 0.5335

Epoch 00002: val_categorical_accuracy did not improve
Epoch 3/15

  128/13806 [..............................] - ETA: 55s - loss: 0.1841 - categorical_accuracy: 0.9766
  256/13806 [..............................] - ETA: 54s - loss: 0.1521 - categorical_accuracy: 0.9883
  384/13806 [..............................] - ETA: 54s - loss: 0.1501 - categorical_accuracy: 0.9870
  512/13806 [>.............................] - ETA: 55s - loss: 0.1741 - categorical_accuracy: 0.9805
  640/13806 [>.............................] - ETA: 54s - loss: 0.1606 - categorical_accuracy: 0.9844
  768/13806 [>.............................] - ETA: 53s - loss: 0.1552 - categorical_accuracy: 0.9857
  896/13806 [>.............................] - ETA: 53s - loss: 0.1539 - categorical_accuracy: 0.9844
 1024/13806 [=>............................] - ETA: 52s - loss: 0.1520 - categorical_accuracy: 0.9844
 1152/13806 [=>............................] - ETA: 52s - loss: 0.1495 - categorical_accuracy: 0.9844
 1280/13806 [=>............................] - ETA: 51s - loss: 0.1482 - categorical_accuracy: 0.9852
 1408/13806 [==>...........................] - ETA: 51s - loss: 0.1446 - categorical_accuracy: 0.9865
 1536/13806 [==>...........................] - ETA: 50s - loss: 0.1476 - categorical_accuracy: 0.9870
 1664/13806 [==>...........................] - ETA: 50s - loss: 0.1476 - categorical_accuracy: 0.9862
 1792/13806 [==>...........................] - ETA: 49s - loss: 0.1486 - categorical_accuracy: 0.9860
 1920/13806 [===>..........................] - ETA: 49s - loss: 0.1509 - categorical_accuracy: 0.9854
 2048/13806 [===>..........................] - ETA: 48s - loss: 0.1524 - categorical_accuracy: 0.9844
 2176/13806 [===>..........................] - ETA: 47s - loss: 0.1546 - categorical_accuracy: 0.9835
 2304/13806 [====>.........................] - ETA: 47s - loss: 0.1545 - categorical_accuracy: 0.9835
 2432/13806 [====>.........................] - ETA: 47s - loss: 0.1567 - categorical_accuracy: 0.9831
 2560/13806 [====>.........................] - ETA: 46s - loss: 0.1551 - categorical_accuracy: 0.9832
 2688/13806 [====>.........................] - ETA: 45s - loss: 0.1536 - categorical_accuracy: 0.9836
 2816/13806 [=====>........................] - ETA: 45s - loss: 0.1525 - categorical_accuracy: 0.9837
 2944/13806 [=====>........................] - ETA: 44s - loss: 0.1511 - categorical_accuracy: 0.9840
 3072/13806 [=====>........................] - ETA: 44s - loss: 0.1515 - categorical_accuracy: 0.9837
 3200/13806 [=====>........................] - ETA: 43s - loss: 0.1496 - categorical_accuracy: 0.9844
 3328/13806 [======>.......................] - ETA: 43s - loss: 0.1483 - categorical_accuracy: 0.9844
 3456/13806 [======>.......................] - ETA: 42s - loss: 0.1485 - categorical_accuracy: 0.9844
 3584/13806 [======>.......................] - ETA: 42s - loss: 0.1471 - categorical_accuracy: 0.9844
 3712/13806 [=======>......................] - ETA: 41s - loss: 0.1486 - categorical_accuracy: 0.9841
 3840/13806 [=======>......................] - ETA: 41s - loss: 0.1490 - categorical_accuracy: 0.9839
 3968/13806 [=======>......................] - ETA: 40s - loss: 0.1475 - categorical_accuracy: 0.9841
 4096/13806 [=======>......................] - ETA: 40s - loss: 0.1487 - categorical_accuracy: 0.9839
 4224/13806 [========>.....................] - ETA: 39s - loss: 0.1469 - categorical_accuracy: 0.9844
 4352/13806 [========>.....................] - ETA: 39s - loss: 0.1461 - categorical_accuracy: 0.9846
 4480/13806 [========>.....................] - ETA: 38s - loss: 0.1452 - categorical_accuracy: 0.9848
 4608/13806 [=========>....................] - ETA: 38s - loss: 0.1444 - categorical_accuracy: 0.9850
 4736/13806 [=========>....................] - ETA: 37s - loss: 0.1439 - categorical_accuracy: 0.9852
 4864/13806 [=========>....................] - ETA: 37s - loss: 0.1445 - categorical_accuracy: 0.9848
 4992/13806 [=========>....................] - ETA: 36s - loss: 0.1443 - categorical_accuracy: 0.9848
 5120/13806 [==========>...................] - ETA: 36s - loss: 0.1437 - categorical_accuracy: 0.9848
 5248/13806 [==========>...................] - ETA: 35s - loss: 0.1432 - categorical_accuracy: 0.9848
 5376/13806 [==========>...................] - ETA: 34s - loss: 0.1426 - categorical_accuracy: 0.9847
 5504/13806 [==========>...................] - ETA: 34s - loss: 0.1451 - categorical_accuracy: 0.9846
 5632/13806 [===========>..................] - ETA: 33s - loss: 0.1473 - categorical_accuracy: 0.9840
 5760/13806 [===========>..................] - ETA: 33s - loss: 0.1483 - categorical_accuracy: 0.9839
 5888/13806 [===========>..................] - ETA: 32s - loss: 0.1474 - categorical_accuracy: 0.9839
 6016/13806 [============>.................] - ETA: 32s - loss: 0.1464 - categorical_accuracy: 0.9840
 6144/13806 [============>.................] - ETA: 31s - loss: 0.1454 - categorical_accuracy: 0.9844
 6272/13806 [============>.................] - ETA: 31s - loss: 0.1449 - categorical_accuracy: 0.9844
 6400/13806 [============>.................] - ETA: 30s - loss: 0.1440 - categorical_accuracy: 0.9847
 6528/13806 [=============>................] - ETA: 30s - loss: 0.1430 - categorical_accuracy: 0.9850
 6656/13806 [=============>................] - ETA: 29s - loss: 0.1423 - categorical_accuracy: 0.9850
 6784/13806 [=============>................] - ETA: 29s - loss: 0.1413 - categorical_accuracy: 0.9853
 6912/13806 [==============>...............] - ETA: 28s - loss: 0.1411 - categorical_accuracy: 0.9852
 7040/13806 [==============>...............] - ETA: 28s - loss: 0.1414 - categorical_accuracy: 0.9852
 7168/13806 [==============>...............] - ETA: 27s - loss: 0.1413 - categorical_accuracy: 0.9852
 7296/13806 [==============>...............] - ETA: 27s - loss: 0.1407 - categorical_accuracy: 0.9855
 7424/13806 [===============>..............] - ETA: 26s - loss: 0.1407 - categorical_accuracy: 0.9856
 7552/13806 [===============>..............] - ETA: 25s - loss: 0.1408 - categorical_accuracy: 0.9854
 7680/13806 [===============>..............] - ETA: 25s - loss: 0.1402 - categorical_accuracy: 0.9855
 7808/13806 [===============>..............] - ETA: 24s - loss: 0.1411 - categorical_accuracy: 0.9855
 7936/13806 [================>.............] - ETA: 24s - loss: 0.1420 - categorical_accuracy: 0.9853
 8064/13806 [================>.............] - ETA: 23s - loss: 0.1424 - categorical_accuracy: 0.9850
 8192/13806 [================>.............] - ETA: 23s - loss: 0.1445 - categorical_accuracy: 0.9844
 8320/13806 [=================>............] - ETA: 22s - loss: 0.1439 - categorical_accuracy: 0.9845
 8448/13806 [=================>............] - ETA: 22s - loss: 0.1432 - categorical_accuracy: 0.9846
 8576/13806 [=================>............] - ETA: 21s - loss: 0.1432 - categorical_accuracy: 0.9846
 8704/13806 [=================>............] - ETA: 21s - loss: 0.1429 - categorical_accuracy: 0.9846
 8832/13806 [==================>...........] - ETA: 20s - loss: 0.1434 - categorical_accuracy: 0.9846
 8960/13806 [==================>...........] - ETA: 20s - loss: 0.1429 - categorical_accuracy: 0.9845
 9088/13806 [==================>...........] - ETA: 19s - loss: 0.1444 - categorical_accuracy: 0.9839
 9216/13806 [===================>..........] - ETA: 19s - loss: 0.1435 - categorical_accuracy: 0.9842
 9344/13806 [===================>..........] - ETA: 18s - loss: 0.1427 - categorical_accuracy: 0.9844
 9472/13806 [===================>..........] - ETA: 17s - loss: 0.1422 - categorical_accuracy: 0.9845
 9600/13806 [===================>..........] - ETA: 17s - loss: 0.1415 - categorical_accuracy: 0.9847
 9728/13806 [====================>.........] - ETA: 16s - loss: 0.1406 - categorical_accuracy: 0.9849
 9856/13806 [====================>.........] - ETA: 16s - loss: 0.1401 - categorical_accuracy: 0.9850
 9984/13806 [====================>.........] - ETA: 15s - loss: 0.1399 - categorical_accuracy: 0.9851
10112/13806 [====================>.........] - ETA: 15s - loss: 0.1393 - categorical_accuracy: 0.9851
10240/13806 [=====================>........] - ETA: 14s - loss: 0.1398 - categorical_accuracy: 0.9849
10368/13806 [=====================>........] - ETA: 14s - loss: 0.1394 - categorical_accuracy: 0.9849
10496/13806 [=====================>........] - ETA: 13s - loss: 0.1390 - categorical_accuracy: 0.9849
10624/13806 [======================>.......] - ETA: 13s - loss: 0.1387 - categorical_accuracy: 0.9848
10752/13806 [======================>.......] - ETA: 12s - loss: 0.1383 - categorical_accuracy: 0.9847
10880/13806 [======================>.......] - ETA: 12s - loss: 0.1380 - categorical_accuracy: 0.9847
11008/13806 [======================>.......] - ETA: 11s - loss: 0.1377 - categorical_accuracy: 0.9847
11136/13806 [=======================>......] - ETA: 11s - loss: 0.1376 - categorical_accuracy: 0.9848
11264/13806 [=======================>......] - ETA: 10s - loss: 0.1369 - categorical_accuracy: 0.9850
11392/13806 [=======================>......] - ETA: 10s - loss: 0.1367 - categorical_accuracy: 0.9850
11520/13806 [========================>.....] - ETA: 9s - loss: 0.1368 - categorical_accuracy: 0.9851 
11648/13806 [========================>.....] - ETA: 8s - loss: 0.1364 - categorical_accuracy: 0.9851
11776/13806 [========================>.....] - ETA: 8s - loss: 0.1365 - categorical_accuracy: 0.9851
11904/13806 [========================>.....] - ETA: 7s - loss: 0.1358 - categorical_accuracy: 0.9852
12032/13806 [=========================>....] - ETA: 7s - loss: 0.1352 - categorical_accuracy: 0.9853
12160/13806 [=========================>....] - ETA: 6s - loss: 0.1348 - categorical_accuracy: 0.9853
12288/13806 [=========================>....] - ETA: 6s - loss: 0.1346 - categorical_accuracy: 0.9854
12416/13806 [=========================>....] - ETA: 5s - loss: 0.1343 - categorical_accuracy: 0.9853
12544/13806 [==========================>...] - ETA: 5s - loss: 0.1343 - categorical_accuracy: 0.9853
12672/13806 [==========================>...] - ETA: 4s - loss: 0.1342 - categorical_accuracy: 0.9854
12800/13806 [==========================>...] - ETA: 4s - loss: 0.1336 - categorical_accuracy: 0.9855
12928/13806 [===========================>..] - ETA: 3s - loss: 0.1331 - categorical_accuracy: 0.9856
13056/13806 [===========================>..] - ETA: 3s - loss: 0.1328 - categorical_accuracy: 0.9857
13184/13806 [===========================>..] - ETA: 2s - loss: 0.1324 - categorical_accuracy: 0.9857
13312/13806 [===========================>..] - ETA: 2s - loss: 0.1323 - categorical_accuracy: 0.9857
13440/13806 [============================>.] - ETA: 1s - loss: 0.1320 - categorical_accuracy: 0.9857
13568/13806 [============================>.] - ETA: 0s - loss: 0.1317 - categorical_accuracy: 0.9858
13696/13806 [============================>.] - ETA: 0s - loss: 0.1314 - categorical_accuracy: 0.9858
13806/13806 [==============================] - 59s 4ms/step - loss: 0.1312 - categorical_accuracy: 0.9858 - val_loss: 1.4745 - val_categorical_accuracy: 0.5288

Epoch 00003: val_categorical_accuracy did not improve
Epoch 4/15

  128/13806 [..............................] - ETA: 55s - loss: 0.0707 - categorical_accuracy: 1.0000
  256/13806 [..............................] - ETA: 55s - loss: 0.1230 - categorical_accuracy: 0.9922
  384/13806 [..............................] - ETA: 54s - loss: 0.1119 - categorical_accuracy: 0.9922
  512/13806 [>.............................] - ETA: 54s - loss: 0.1066 - categorical_accuracy: 0.9922
  640/13806 [>.............................] - ETA: 54s - loss: 0.1075 - categorical_accuracy: 0.9906
  768/13806 [>.............................] - ETA: 53s - loss: 0.1019 - categorical_accuracy: 0.9922
  896/13806 [>.............................] - ETA: 52s - loss: 0.1000 - categorical_accuracy: 0.9922
 1024/13806 [=>............................] - ETA: 52s - loss: 0.1071 - categorical_accuracy: 0.9912
 1152/13806 [=>............................] - ETA: 51s - loss: 0.1087 - categorical_accuracy: 0.9905
 1280/13806 [=>............................] - ETA: 51s - loss: 0.1054 - categorical_accuracy: 0.9914
 1408/13806 [==>...........................] - ETA: 50s - loss: 0.1063 - categorical_accuracy: 0.9915
 1536/13806 [==>...........................] - ETA: 50s - loss: 0.1057 - categorical_accuracy: 0.9915
 1664/13806 [==>...........................] - ETA: 49s - loss: 0.1061 - categorical_accuracy: 0.9910
 1792/13806 [==>...........................] - ETA: 49s - loss: 0.1080 - categorical_accuracy: 0.9900
 1920/13806 [===>..........................] - ETA: 48s - loss: 0.1104 - categorical_accuracy: 0.9896
 2048/13806 [===>..........................] - ETA: 48s - loss: 0.1091 - categorical_accuracy: 0.9893
 2176/13806 [===>..........................] - ETA: 47s - loss: 0.1086 - categorical_accuracy: 0.9894
 2304/13806 [====>.........................] - ETA: 47s - loss: 0.1139 - categorical_accuracy: 0.9883
 2432/13806 [====>.........................] - ETA: 46s - loss: 0.1141 - categorical_accuracy: 0.9873
 2560/13806 [====>.........................] - ETA: 46s - loss: 0.1155 - categorical_accuracy: 0.9871
 2688/13806 [====>.........................] - ETA: 45s - loss: 0.1143 - categorical_accuracy: 0.9870
 2816/13806 [=====>........................] - ETA: 45s - loss: 0.1159 - categorical_accuracy: 0.9862
 2944/13806 [=====>........................] - ETA: 44s - loss: 0.1157 - categorical_accuracy: 0.9857
 3072/13806 [=====>........................] - ETA: 44s - loss: 0.1139 - categorical_accuracy: 0.9863
 3200/13806 [=====>........................] - ETA: 43s - loss: 0.1130 - categorical_accuracy: 0.9862
 3328/13806 [======>.......................] - ETA: 43s - loss: 0.1111 - categorical_accuracy: 0.9868
 3456/13806 [======>.......................] - ETA: 42s - loss: 0.1116 - categorical_accuracy: 0.9861
 3584/13806 [======>.......................] - ETA: 42s - loss: 0.1103 - categorical_accuracy: 0.9866
 3712/13806 [=======>......................] - ETA: 41s - loss: 0.1114 - categorical_accuracy: 0.9863
 3840/13806 [=======>......................] - ETA: 41s - loss: 0.1111 - categorical_accuracy: 0.9865
 3968/13806 [=======>......................] - ETA: 40s - loss: 0.1118 - categorical_accuracy: 0.9864
 4096/13806 [=======>......................] - ETA: 40s - loss: 0.1126 - categorical_accuracy: 0.9861
 4224/13806 [========>.....................] - ETA: 39s - loss: 0.1119 - categorical_accuracy: 0.9865
 4352/13806 [========>.....................] - ETA: 38s - loss: 0.1124 - categorical_accuracy: 0.9864
 4480/13806 [========>.....................] - ETA: 38s - loss: 0.1136 - categorical_accuracy: 0.9857
 4608/13806 [=========>....................] - ETA: 37s - loss: 0.1122 - categorical_accuracy: 0.9861
 4736/13806 [=========>....................] - ETA: 37s - loss: 0.1131 - categorical_accuracy: 0.9861
 4864/13806 [=========>....................] - ETA: 36s - loss: 0.1129 - categorical_accuracy: 0.9862
 4992/13806 [=========>....................] - ETA: 36s - loss: 0.1126 - categorical_accuracy: 0.9864
 5120/13806 [==========>...................] - ETA: 35s - loss: 0.1133 - categorical_accuracy: 0.9863
 5248/13806 [==========>...................] - ETA: 35s - loss: 0.1129 - categorical_accuracy: 0.9863
 5376/13806 [==========>...................] - ETA: 34s - loss: 0.1141 - categorical_accuracy: 0.9860
 5504/13806 [==========>...................] - ETA: 34s - loss: 0.1135 - categorical_accuracy: 0.9860
 5632/13806 [===========>..................] - ETA: 33s - loss: 0.1130 - categorical_accuracy: 0.9862
 5760/13806 [===========>..................] - ETA: 33s - loss: 0.1121 - categorical_accuracy: 0.9863
 5888/13806 [===========>..................] - ETA: 32s - loss: 0.1110 - categorical_accuracy: 0.9866
 6016/13806 [============>.................] - ETA: 32s - loss: 0.1114 - categorical_accuracy: 0.9865
 6144/13806 [============>.................] - ETA: 31s - loss: 0.1103 - categorical_accuracy: 0.9868
 6272/13806 [============>.................] - ETA: 31s - loss: 0.1098 - categorical_accuracy: 0.9869
 6400/13806 [============>.................] - ETA: 30s - loss: 0.1102 - categorical_accuracy: 0.9869
 6528/13806 [=============>................] - ETA: 30s - loss: 0.1125 - categorical_accuracy: 0.9864
 6656/13806 [=============>................] - ETA: 29s - loss: 0.1121 - categorical_accuracy: 0.9865
 6784/13806 [=============>................] - ETA: 28s - loss: 0.1121 - categorical_accuracy: 0.9864
 6912/13806 [==============>...............] - ETA: 28s - loss: 0.1115 - categorical_accuracy: 0.9865
 7040/13806 [==============>...............] - ETA: 27s - loss: 0.1108 - categorical_accuracy: 0.9868
 7168/13806 [==============>...............] - ETA: 27s - loss: 0.1115 - categorical_accuracy: 0.9866
 7296/13806 [==============>...............] - ETA: 26s - loss: 0.1118 - categorical_accuracy: 0.9863
 7424/13806 [===============>..............] - ETA: 26s - loss: 0.1127 - categorical_accuracy: 0.9863
 7552/13806 [===============>..............] - ETA: 25s - loss: 0.1129 - categorical_accuracy: 0.9864
 7680/13806 [===============>..............] - ETA: 25s - loss: 0.1130 - categorical_accuracy: 0.9862
 7808/13806 [===============>..............] - ETA: 24s - loss: 0.1127 - categorical_accuracy: 0.9863
 7936/13806 [================>.............] - ETA: 24s - loss: 0.1125 - categorical_accuracy: 0.9863
 8064/13806 [================>.............] - ETA: 23s - loss: 0.1126 - categorical_accuracy: 0.9862
 8192/13806 [================>.............] - ETA: 23s - loss: 0.1127 - categorical_accuracy: 0.9863
 8320/13806 [=================>............] - ETA: 22s - loss: 0.1128 - categorical_accuracy: 0.9862
 8448/13806 [=================>............] - ETA: 22s - loss: 0.1124 - categorical_accuracy: 0.9863
 8576/13806 [=================>............] - ETA: 21s - loss: 0.1123 - categorical_accuracy: 0.9864
 8704/13806 [=================>............] - ETA: 21s - loss: 0.1123 - categorical_accuracy: 0.9863
 8832/13806 [==================>...........] - ETA: 20s - loss: 0.1123 - categorical_accuracy: 0.9862
 8960/13806 [==================>...........] - ETA: 20s - loss: 0.1123 - categorical_accuracy: 0.9862
 9088/13806 [==================>...........] - ETA: 19s - loss: 0.1119 - categorical_accuracy: 0.9862
 9216/13806 [===================>..........] - ETA: 18s - loss: 0.1116 - categorical_accuracy: 0.9863
 9344/13806 [===================>..........] - ETA: 18s - loss: 0.1115 - categorical_accuracy: 0.9864
 9472/13806 [===================>..........] - ETA: 17s - loss: 0.1127 - categorical_accuracy: 0.9863
 9600/13806 [===================>..........] - ETA: 17s - loss: 0.1122 - categorical_accuracy: 0.9865
 9728/13806 [====================>.........] - ETA: 16s - loss: 0.1125 - categorical_accuracy: 0.9864
 9856/13806 [====================>.........] - ETA: 16s - loss: 0.1120 - categorical_accuracy: 0.9866
 9984/13806 [====================>.........] - ETA: 15s - loss: 0.1122 - categorical_accuracy: 0.9866
10112/13806 [====================>.........] - ETA: 15s - loss: 0.1120 - categorical_accuracy: 0.9866
10240/13806 [=====================>........] - ETA: 14s - loss: 0.1123 - categorical_accuracy: 0.9863
10368/13806 [=====================>........] - ETA: 14s - loss: 0.1125 - categorical_accuracy: 0.9863
10496/13806 [=====================>........] - ETA: 13s - loss: 0.1120 - categorical_accuracy: 0.9864
10624/13806 [======================>.......] - ETA: 13s - loss: 0.1118 - categorical_accuracy: 0.9864
10752/13806 [======================>.......] - ETA: 12s - loss: 0.1116 - categorical_accuracy: 0.9865
10880/13806 [======================>.......] - ETA: 12s - loss: 0.1118 - categorical_accuracy: 0.9863
11008/13806 [======================>.......] - ETA: 11s - loss: 0.1116 - categorical_accuracy: 0.9864
11136/13806 [=======================>......] - ETA: 11s - loss: 0.1113 - categorical_accuracy: 0.9864
11264/13806 [=======================>......] - ETA: 10s - loss: 0.1110 - categorical_accuracy: 0.9865
11392/13806 [=======================>......] - ETA: 9s - loss: 0.1117 - categorical_accuracy: 0.9863 
11520/13806 [========================>.....] - ETA: 9s - loss: 0.1117 - categorical_accuracy: 0.9863
11648/13806 [========================>.....] - ETA: 8s - loss: 0.1121 - categorical_accuracy: 0.9862
11776/13806 [========================>.....] - ETA: 8s - loss: 0.1117 - categorical_accuracy: 0.9862
11904/13806 [========================>.....] - ETA: 7s - loss: 0.1117 - categorical_accuracy: 0.9863
12032/13806 [=========================>....] - ETA: 7s - loss: 0.1113 - categorical_accuracy: 0.9865
12160/13806 [=========================>....] - ETA: 6s - loss: 0.1110 - categorical_accuracy: 0.9865
12288/13806 [=========================>....] - ETA: 6s - loss: 0.1113 - categorical_accuracy: 0.9865
12416/13806 [=========================>....] - ETA: 5s - loss: 0.1116 - categorical_accuracy: 0.9864
12544/13806 [==========================>...] - ETA: 5s - loss: 0.1113 - categorical_accuracy: 0.9865
12672/13806 [==========================>...] - ETA: 4s - loss: 0.1111 - categorical_accuracy: 0.9865
12800/13806 [==========================>...] - ETA: 4s - loss: 0.1115 - categorical_accuracy: 0.9865
12928/13806 [===========================>..] - ETA: 3s - loss: 0.1113 - categorical_accuracy: 0.9865
13056/13806 [===========================>..] - ETA: 3s - loss: 0.1119 - categorical_accuracy: 0.9863
13184/13806 [===========================>..] - ETA: 2s - loss: 0.1121 - categorical_accuracy: 0.9861
13312/13806 [===========================>..] - ETA: 2s - loss: 0.1118 - categorical_accuracy: 0.9862
13440/13806 [============================>.] - ETA: 1s - loss: 0.1118 - categorical_accuracy: 0.9862
13568/13806 [============================>.] - ETA: 0s - loss: 0.1119 - categorical_accuracy: 0.9861
13696/13806 [============================>.] - ETA: 0s - loss: 0.1121 - categorical_accuracy: 0.9861
13806/13806 [==============================] - 59s 4ms/step - loss: 0.1121 - categorical_accuracy: 0.9859 - val_loss: 1.4711 - val_categorical_accuracy: 0.5315

Epoch 00004: val_categorical_accuracy did not improve
Epoch 5/15

  128/13806 [..............................] - ETA: 56s - loss: 0.0753 - categorical_accuracy: 0.9922
  256/13806 [..............................] - ETA: 55s - loss: 0.0891 - categorical_accuracy: 0.9922
  384/13806 [..............................] - ETA: 55s - loss: 0.1097 - categorical_accuracy: 0.9870
  512/13806 [>.............................] - ETA: 54s - loss: 0.1052 - categorical_accuracy: 0.9883
  640/13806 [>.............................] - ETA: 54s - loss: 0.1031 - categorical_accuracy: 0.9891
  768/13806 [>.............................] - ETA: 54s - loss: 0.0990 - categorical_accuracy: 0.9909
  896/13806 [>.............................] - ETA: 53s - loss: 0.1043 - categorical_accuracy: 0.9888
 1024/13806 [=>............................] - ETA: 52s - loss: 0.1042 - categorical_accuracy: 0.9893
 1152/13806 [=>............................] - ETA: 52s - loss: 0.1023 - categorical_accuracy: 0.9887
 1280/13806 [=>............................] - ETA: 51s - loss: 0.0993 - categorical_accuracy: 0.9891
 1408/13806 [==>...........................] - ETA: 51s - loss: 0.1062 - categorical_accuracy: 0.9872
 1536/13806 [==>...........................] - ETA: 50s - loss: 0.1093 - categorical_accuracy: 0.9870
 1664/13806 [==>...........................] - ETA: 50s - loss: 0.1090 - categorical_accuracy: 0.9868
 1792/13806 [==>...........................] - ETA: 49s - loss: 0.1071 - categorical_accuracy: 0.9866
 1920/13806 [===>..........................] - ETA: 48s - loss: 0.1130 - categorical_accuracy: 0.9854
 2048/13806 [===>..........................] - ETA: 48s - loss: 0.1098 - categorical_accuracy: 0.9863
 2176/13806 [===>..........................] - ETA: 47s - loss: 0.1090 - categorical_accuracy: 0.9858
 2304/13806 [====>.........................] - ETA: 47s - loss: 0.1106 - categorical_accuracy: 0.9857
 2432/13806 [====>.........................] - ETA: 47s - loss: 0.1105 - categorical_accuracy: 0.9856
 2560/13806 [====>.........................] - ETA: 46s - loss: 0.1145 - categorical_accuracy: 0.9855
 2688/13806 [====>.........................] - ETA: 45s - loss: 0.1126 - categorical_accuracy: 0.9859
 2816/13806 [=====>........................] - ETA: 45s - loss: 0.1203 - categorical_accuracy: 0.9833
 2944/13806 [=====>........................] - ETA: 44s - loss: 0.1219 - categorical_accuracy: 0.9830
 3072/13806 [=====>........................] - ETA: 44s - loss: 0.1226 - categorical_accuracy: 0.9827
 3200/13806 [=====>........................] - ETA: 43s - loss: 0.1198 - categorical_accuracy: 0.9834
 3328/13806 [======>.......................] - ETA: 43s - loss: 0.1186 - categorical_accuracy: 0.9832
 3456/13806 [======>.......................] - ETA: 42s - loss: 0.1176 - categorical_accuracy: 0.9832
 3584/13806 [======>.......................] - ETA: 42s - loss: 0.1161 - categorical_accuracy: 0.9838
 3712/13806 [=======>......................] - ETA: 41s - loss: 0.1152 - categorical_accuracy: 0.9841
 3840/13806 [=======>......................] - ETA: 41s - loss: 0.1142 - categorical_accuracy: 0.9844
 3968/13806 [=======>......................] - ETA: 40s - loss: 0.1166 - categorical_accuracy: 0.9839
 4096/13806 [=======>......................] - ETA: 40s - loss: 0.1148 - categorical_accuracy: 0.9844
 4224/13806 [========>.....................] - ETA: 39s - loss: 0.1143 - categorical_accuracy: 0.9844
 4352/13806 [========>.....................] - ETA: 39s - loss: 0.1139 - categorical_accuracy: 0.9844
 4480/13806 [========>.....................] - ETA: 38s - loss: 0.1126 - categorical_accuracy: 0.9848
 4608/13806 [=========>....................] - ETA: 38s - loss: 0.1114 - categorical_accuracy: 0.9852
 4736/13806 [=========>....................] - ETA: 37s - loss: 0.1111 - categorical_accuracy: 0.9852
 4864/13806 [=========>....................] - ETA: 36s - loss: 0.1102 - categorical_accuracy: 0.9854
 4992/13806 [=========>....................] - ETA: 36s - loss: 0.1108 - categorical_accuracy: 0.9854
 5120/13806 [==========>...................] - ETA: 35s - loss: 0.1096 - categorical_accuracy: 0.9857
 5248/13806 [==========>...................] - ETA: 35s - loss: 0.1101 - categorical_accuracy: 0.9855
 5376/13806 [==========>...................] - ETA: 34s - loss: 0.1098 - categorical_accuracy: 0.9855
 5504/13806 [==========>...................] - ETA: 34s - loss: 0.1091 - categorical_accuracy: 0.9856
 5632/13806 [===========>..................] - ETA: 33s - loss: 0.1093 - categorical_accuracy: 0.9856
 5760/13806 [===========>..................] - ETA: 33s - loss: 0.1092 - categorical_accuracy: 0.9856
 5888/13806 [===========>..................] - ETA: 32s - loss: 0.1085 - categorical_accuracy: 0.9857
 6016/13806 [============>.................] - ETA: 32s - loss: 0.1088 - categorical_accuracy: 0.9859
 6144/13806 [============>.................] - ETA: 31s - loss: 0.1078 - categorical_accuracy: 0.9862
 6272/13806 [============>.................] - ETA: 31s - loss: 0.1069 - categorical_accuracy: 0.9863
 6400/13806 [============>.................] - ETA: 30s - loss: 0.1062 - categorical_accuracy: 0.9864
 6528/13806 [=============>................] - ETA: 30s - loss: 0.1060 - categorical_accuracy: 0.9864
 6656/13806 [=============>................] - ETA: 29s - loss: 0.1051 - categorical_accuracy: 0.9866
 6784/13806 [=============>................] - ETA: 29s - loss: 0.1049 - categorical_accuracy: 0.9867
 6912/13806 [==============>...............] - ETA: 28s - loss: 0.1044 - categorical_accuracy: 0.9867
 7040/13806 [==============>...............] - ETA: 27s - loss: 0.1057 - categorical_accuracy: 0.9866
 7168/13806 [==============>...............] - ETA: 27s - loss: 0.1058 - categorical_accuracy: 0.9865
 7296/13806 [==============>...............] - ETA: 26s - loss: 0.1065 - categorical_accuracy: 0.9864
 7424/13806 [===============>..............] - ETA: 26s - loss: 0.1060 - categorical_accuracy: 0.9864
 7552/13806 [===============>..............] - ETA: 25s - loss: 0.1051 - categorical_accuracy: 0.9866
 7680/13806 [===============>..............] - ETA: 25s - loss: 0.1052 - categorical_accuracy: 0.9865
 7808/13806 [===============>..............] - ETA: 24s - loss: 0.1060 - categorical_accuracy: 0.9863
 7936/13806 [================>.............] - ETA: 24s - loss: 0.1062 - categorical_accuracy: 0.9860
 8064/13806 [================>.............] - ETA: 23s - loss: 0.1063 - categorical_accuracy: 0.9859
 8192/13806 [================>.............] - ETA: 23s - loss: 0.1067 - categorical_accuracy: 0.9858
 8320/13806 [=================>............] - ETA: 22s - loss: 0.1067 - categorical_accuracy: 0.9858
 8448/13806 [=================>............] - ETA: 22s - loss: 0.1062 - categorical_accuracy: 0.9859
 8576/13806 [=================>............] - ETA: 21s - loss: 0.1055 - categorical_accuracy: 0.9860
 8704/13806 [=================>............] - ETA: 21s - loss: 0.1057 - categorical_accuracy: 0.9861
 8832/13806 [==================>...........] - ETA: 20s - loss: 0.1057 - categorical_accuracy: 0.9861
 8960/13806 [==================>...........] - ETA: 20s - loss: 0.1055 - categorical_accuracy: 0.9862
 9088/13806 [==================>...........] - ETA: 19s - loss: 0.1052 - categorical_accuracy: 0.9861
 9216/13806 [===================>..........] - ETA: 19s - loss: 0.1053 - categorical_accuracy: 0.9861
 9344/13806 [===================>..........] - ETA: 18s - loss: 0.1049 - categorical_accuracy: 0.9861
 9472/13806 [===================>..........] - ETA: 17s - loss: 0.1046 - categorical_accuracy: 0.9862
 9600/13806 [===================>..........] - ETA: 17s - loss: 0.1056 - categorical_accuracy: 0.9858
 9728/13806 [====================>.........] - ETA: 16s - loss: 0.1053 - categorical_accuracy: 0.9857
 9856/13806 [====================>.........] - ETA: 16s - loss: 0.1054 - categorical_accuracy: 0.9857
 9984/13806 [====================>.........] - ETA: 15s - loss: 0.1051 - categorical_accuracy: 0.9858
10112/13806 [====================>.........] - ETA: 15s - loss: 0.1052 - categorical_accuracy: 0.9858
10240/13806 [=====================>........] - ETA: 14s - loss: 0.1050 - categorical_accuracy: 0.9858
10368/13806 [=====================>........] - ETA: 14s - loss: 0.1047 - categorical_accuracy: 0.9859
10496/13806 [=====================>........] - ETA: 13s - loss: 0.1046 - categorical_accuracy: 0.9859
10624/13806 [======================>.......] - ETA: 13s - loss: 0.1044 - categorical_accuracy: 0.9859
10752/13806 [======================>.......] - ETA: 12s - loss: 0.1044 - categorical_accuracy: 0.9859
10880/13806 [======================>.......] - ETA: 12s - loss: 0.1044 - categorical_accuracy: 0.9858
11008/13806 [======================>.......] - ETA: 11s - loss: 0.1052 - categorical_accuracy: 0.9855
11136/13806 [=======================>......] - ETA: 11s - loss: 0.1051 - categorical_accuracy: 0.9854
11264/13806 [=======================>......] - ETA: 10s - loss: 0.1048 - categorical_accuracy: 0.9855
11392/13806 [=======================>......] - ETA: 10s - loss: 0.1052 - categorical_accuracy: 0.9856
11520/13806 [========================>.....] - ETA: 9s - loss: 0.1052 - categorical_accuracy: 0.9856 
11648/13806 [========================>.....] - ETA: 8s - loss: 0.1049 - categorical_accuracy: 0.9856
11776/13806 [========================>.....] - ETA: 8s - loss: 0.1053 - categorical_accuracy: 0.9855
11904/13806 [========================>.....] - ETA: 7s - loss: 0.1051 - categorical_accuracy: 0.9856
12032/13806 [=========================>....] - ETA: 7s - loss: 0.1048 - categorical_accuracy: 0.9855
12160/13806 [=========================>....] - ETA: 6s - loss: 0.1046 - categorical_accuracy: 0.9855
12288/13806 [=========================>....] - ETA: 6s - loss: 0.1050 - categorical_accuracy: 0.9855
12416/13806 [=========================>....] - ETA: 5s - loss: 0.1053 - categorical_accuracy: 0.9853
12544/13806 [==========================>...] - ETA: 5s - loss: 0.1052 - categorical_accuracy: 0.9853
12672/13806 [==========================>...] - ETA: 4s - loss: 0.1056 - categorical_accuracy: 0.9852
12800/13806 [==========================>...] - ETA: 4s - loss: 0.1053 - categorical_accuracy: 0.9852
12928/13806 [===========================>..] - ETA: 3s - loss: 0.1050 - categorical_accuracy: 0.9853
13056/13806 [===========================>..] - ETA: 3s - loss: 0.1049 - categorical_accuracy: 0.9854
13184/13806 [===========================>..] - ETA: 2s - loss: 0.1047 - categorical_accuracy: 0.9854
13312/13806 [===========================>..] - ETA: 2s - loss: 0.1055 - categorical_accuracy: 0.9853
13440/13806 [============================>.] - ETA: 1s - loss: 0.1057 - categorical_accuracy: 0.9852
13568/13806 [============================>.] - ETA: 0s - loss: 0.1055 - categorical_accuracy: 0.9852
13696/13806 [============================>.] - ETA: 0s - loss: 0.1053 - categorical_accuracy: 0.9852
13806/13806 [==============================] - 59s 4ms/step - loss: 0.1049 - categorical_accuracy: 0.9853 - val_loss: 1.5542 - val_categorical_accuracy: 0.5063

Epoch 00005: val_categorical_accuracy did not improve
Epoch 6/15

  128/13806 [..............................] - ETA: 54s - loss: 0.0734 - categorical_accuracy: 0.9922
  256/13806 [..............................] - ETA: 56s - loss: 0.0760 - categorical_accuracy: 0.9922
  384/13806 [..............................] - ETA: 55s - loss: 0.0839 - categorical_accuracy: 0.9896
  512/13806 [>.............................] - ETA: 55s - loss: 0.1013 - categorical_accuracy: 0.9863
  640/13806 [>.............................] - ETA: 55s - loss: 0.1116 - categorical_accuracy: 0.9828
  768/13806 [>.............................] - ETA: 54s - loss: 0.1100 - categorical_accuracy: 0.9831
  896/13806 [>.............................] - ETA: 53s - loss: 0.1038 - categorical_accuracy: 0.9844
 1024/13806 [=>............................] - ETA: 53s - loss: 0.1004 - categorical_accuracy: 0.9844
 1152/13806 [=>............................] - ETA: 52s - loss: 0.0986 - categorical_accuracy: 0.9852
 1280/13806 [=>............................] - ETA: 51s - loss: 0.1117 - categorical_accuracy: 0.9836
 1408/13806 [==>...........................] - ETA: 51s - loss: 0.1117 - categorical_accuracy: 0.9837
 1536/13806 [==>...........................] - ETA: 50s - loss: 0.1117 - categorical_accuracy: 0.9837
 1664/13806 [==>...........................] - ETA: 49s - loss: 0.1129 - categorical_accuracy: 0.9838
 1792/13806 [==>...........................] - ETA: 49s - loss: 0.1094 - categorical_accuracy: 0.9849
 1920/13806 [===>..........................] - ETA: 48s - loss: 0.1086 - categorical_accuracy: 0.9844
 2048/13806 [===>..........................] - ETA: 48s - loss: 0.1119 - categorical_accuracy: 0.9844
 2176/13806 [===>..........................] - ETA: 47s - loss: 0.1118 - categorical_accuracy: 0.9844
 2304/13806 [====>.........................] - ETA: 47s - loss: 0.1117 - categorical_accuracy: 0.9839
 2432/13806 [====>.........................] - ETA: 46s - loss: 0.1105 - categorical_accuracy: 0.9840
 2560/13806 [====>.........................] - ETA: 46s - loss: 0.1097 - categorical_accuracy: 0.9836
 2688/13806 [====>.........................] - ETA: 45s - loss: 0.1140 - categorical_accuracy: 0.9833
 2816/13806 [=====>........................] - ETA: 45s - loss: 0.1127 - categorical_accuracy: 0.9837
 2944/13806 [=====>........................] - ETA: 44s - loss: 0.1153 - categorical_accuracy: 0.9834
 3072/13806 [=====>........................] - ETA: 43s - loss: 0.1127 - categorical_accuracy: 0.9840
 3200/13806 [=====>........................] - ETA: 43s - loss: 0.1109 - categorical_accuracy: 0.9844
 3328/13806 [======>.......................] - ETA: 42s - loss: 0.1116 - categorical_accuracy: 0.9841
 3456/13806 [======>.......................] - ETA: 42s - loss: 0.1106 - categorical_accuracy: 0.9844
 3584/13806 [======>.......................] - ETA: 41s - loss: 0.1108 - categorical_accuracy: 0.9844
 3712/13806 [=======>......................] - ETA: 41s - loss: 0.1107 - categorical_accuracy: 0.9846
 3840/13806 [=======>......................] - ETA: 40s - loss: 0.1106 - categorical_accuracy: 0.9846
 3968/13806 [=======>......................] - ETA: 40s - loss: 0.1102 - categorical_accuracy: 0.9846
 4096/13806 [=======>......................] - ETA: 39s - loss: 0.1096 - categorical_accuracy: 0.9846
 4224/13806 [========>.....................] - ETA: 39s - loss: 0.1085 - categorical_accuracy: 0.9848
 4352/13806 [========>.....................] - ETA: 38s - loss: 0.1080 - categorical_accuracy: 0.9846
 4480/13806 [========>.....................] - ETA: 38s - loss: 0.1076 - categorical_accuracy: 0.9844
 4608/13806 [=========>....................] - ETA: 37s - loss: 0.1075 - categorical_accuracy: 0.9842
 4736/13806 [=========>....................] - ETA: 37s - loss: 0.1088 - categorical_accuracy: 0.9837
 4864/13806 [=========>....................] - ETA: 36s - loss: 0.1088 - categorical_accuracy: 0.9833
 4992/13806 [=========>....................] - ETA: 36s - loss: 0.1098 - categorical_accuracy: 0.9832
 5120/13806 [==========>...................] - ETA: 35s - loss: 0.1098 - categorical_accuracy: 0.9832
 5248/13806 [==========>...................] - ETA: 35s - loss: 0.1092 - categorical_accuracy: 0.9832
 5376/13806 [==========>...................] - ETA: 34s - loss: 0.1079 - categorical_accuracy: 0.9836
 5504/13806 [==========>...................] - ETA: 34s - loss: 0.1092 - categorical_accuracy: 0.9835
 5632/13806 [===========>..................] - ETA: 33s - loss: 0.1079 - categorical_accuracy: 0.9838
 5760/13806 [===========>..................] - ETA: 33s - loss: 0.1081 - categorical_accuracy: 0.9840
 5888/13806 [===========>..................] - ETA: 32s - loss: 0.1076 - categorical_accuracy: 0.9840
 6016/13806 [============>.................] - ETA: 32s - loss: 0.1069 - categorical_accuracy: 0.9842
 6144/13806 [============>.................] - ETA: 31s - loss: 0.1061 - categorical_accuracy: 0.9844
 6272/13806 [============>.................] - ETA: 31s - loss: 0.1052 - categorical_accuracy: 0.9845
 6400/13806 [============>.................] - ETA: 30s - loss: 0.1042 - categorical_accuracy: 0.9848
 6528/13806 [=============>................] - ETA: 29s - loss: 0.1033 - categorical_accuracy: 0.9851
 6656/13806 [=============>................] - ETA: 29s - loss: 0.1030 - categorical_accuracy: 0.9851
 6784/13806 [=============>................] - ETA: 28s - loss: 0.1025 - categorical_accuracy: 0.9853
 6912/13806 [==============>...............] - ETA: 28s - loss: 0.1023 - categorical_accuracy: 0.9854
 7040/13806 [==============>...............] - ETA: 27s - loss: 0.1018 - categorical_accuracy: 0.9855
 7168/13806 [==============>...............] - ETA: 27s - loss: 0.1016 - categorical_accuracy: 0.9855
 7296/13806 [==============>...............] - ETA: 26s - loss: 0.1020 - categorical_accuracy: 0.9856
 7424/13806 [===============>..............] - ETA: 26s - loss: 0.1016 - categorical_accuracy: 0.9857
 7552/13806 [===============>..............] - ETA: 25s - loss: 0.1010 - categorical_accuracy: 0.9858
 7680/13806 [===============>..............] - ETA: 25s - loss: 0.1008 - categorical_accuracy: 0.9859
 7808/13806 [===============>..............] - ETA: 24s - loss: 0.1004 - categorical_accuracy: 0.9860
 7936/13806 [================>.............] - ETA: 24s - loss: 0.1018 - categorical_accuracy: 0.9858
 8064/13806 [================>.............] - ETA: 23s - loss: 0.1021 - categorical_accuracy: 0.9857
 8192/13806 [================>.............] - ETA: 23s - loss: 0.1013 - categorical_accuracy: 0.9860
 8320/13806 [=================>............] - ETA: 22s - loss: 0.1012 - categorical_accuracy: 0.9858
 8448/13806 [=================>............] - ETA: 22s - loss: 0.1009 - categorical_accuracy: 0.9858
 8576/13806 [=================>............] - ETA: 21s - loss: 0.1009 - categorical_accuracy: 0.9859
 8704/13806 [=================>............] - ETA: 21s - loss: 0.1006 - categorical_accuracy: 0.9859
 8832/13806 [==================>...........] - ETA: 20s - loss: 0.1012 - categorical_accuracy: 0.9858
 8960/13806 [==================>...........] - ETA: 20s - loss: 0.1009 - categorical_accuracy: 0.9860
 9088/13806 [==================>...........] - ETA: 19s - loss: 0.1022 - categorical_accuracy: 0.9858
 9216/13806 [===================>..........] - ETA: 18s - loss: 0.1022 - categorical_accuracy: 0.9859
 9344/13806 [===================>..........] - ETA: 18s - loss: 0.1016 - categorical_accuracy: 0.9860
 9472/13806 [===================>..........] - ETA: 17s - loss: 0.1015 - categorical_accuracy: 0.9861
 9600/13806 [===================>..........] - ETA: 17s - loss: 0.1015 - categorical_accuracy: 0.9860
 9728/13806 [====================>.........] - ETA: 16s - loss: 0.1014 - categorical_accuracy: 0.9860
 9856/13806 [====================>.........] - ETA: 16s - loss: 0.1012 - categorical_accuracy: 0.9860
 9984/13806 [====================>.........] - ETA: 15s - loss: 0.1017 - categorical_accuracy: 0.9860
10112/13806 [====================>.........] - ETA: 15s - loss: 0.1012 - categorical_accuracy: 0.9861
10240/13806 [=====================>........] - ETA: 14s - loss: 0.1018 - categorical_accuracy: 0.9859
10368/13806 [=====================>........] - ETA: 14s - loss: 0.1017 - categorical_accuracy: 0.9859
10496/13806 [=====================>........] - ETA: 13s - loss: 0.1030 - categorical_accuracy: 0.9856
10624/13806 [======================>.......] - ETA: 13s - loss: 0.1030 - categorical_accuracy: 0.9856
10752/13806 [======================>.......] - ETA: 12s - loss: 0.1029 - categorical_accuracy: 0.9857
10880/13806 [======================>.......] - ETA: 12s - loss: 0.1030 - categorical_accuracy: 0.9857
11008/13806 [======================>.......] - ETA: 11s - loss: 0.1031 - categorical_accuracy: 0.9856
11136/13806 [=======================>......] - ETA: 11s - loss: 0.1028 - categorical_accuracy: 0.9856
11264/13806 [=======================>......] - ETA: 10s - loss: 0.1022 - categorical_accuracy: 0.9858
11392/13806 [=======================>......] - ETA: 9s - loss: 0.1023 - categorical_accuracy: 0.9858 
11520/13806 [========================>.....] - ETA: 9s - loss: 0.1024 - categorical_accuracy: 0.9858
11648/13806 [========================>.....] - ETA: 8s - loss: 0.1018 - categorical_accuracy: 0.9859
11776/13806 [========================>.....] - ETA: 8s - loss: 0.1022 - categorical_accuracy: 0.9858
11904/13806 [========================>.....] - ETA: 7s - loss: 0.1023 - categorical_accuracy: 0.9857
12032/13806 [=========================>....] - ETA: 7s - loss: 0.1020 - categorical_accuracy: 0.9858
12160/13806 [=========================>....] - ETA: 6s - loss: 0.1017 - categorical_accuracy: 0.9858
12288/13806 [=========================>....] - ETA: 6s - loss: 0.1016 - categorical_accuracy: 0.9858
12416/13806 [=========================>....] - ETA: 5s - loss: 0.1012 - categorical_accuracy: 0.9859
12544/13806 [==========================>...] - ETA: 5s - loss: 0.1020 - categorical_accuracy: 0.9857
12672/13806 [==========================>...] - ETA: 4s - loss: 0.1018 - categorical_accuracy: 0.9857
12800/13806 [==========================>...] - ETA: 4s - loss: 0.1016 - categorical_accuracy: 0.9857
12928/13806 [===========================>..] - ETA: 3s - loss: 0.1012 - categorical_accuracy: 0.9858
13056/13806 [===========================>..] - ETA: 3s - loss: 0.1010 - categorical_accuracy: 0.9859
13184/13806 [===========================>..] - ETA: 2s - loss: 0.1011 - categorical_accuracy: 0.9858
13312/13806 [===========================>..] - ETA: 2s - loss: 0.1011 - categorical_accuracy: 0.9858
13440/13806 [============================>.] - ETA: 1s - loss: 0.1015 - categorical_accuracy: 0.9856
13568/13806 [============================>.] - ETA: 0s - loss: 0.1016 - categorical_accuracy: 0.9856
13696/13806 [============================>.] - ETA: 0s - loss: 0.1019 - categorical_accuracy: 0.9854
13806/13806 [==============================] - 59s 4ms/step - loss: 0.1022 - categorical_accuracy: 0.9854 - val_loss: 1.5508 - val_categorical_accuracy: 0.5235
2018-03-23 11:05:28.006059: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:03:00.0, compute capability: 6.1)
/home/michon/anaconda2/envs/py35/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.

Epoch 00006: val_categorical_accuracy did not improve
Epoch 00006: early stopping

Final evaluation

f1_score
 0.5143966312012352
accuracy_score
 0.5235255135851558

classification_report
              precision    recall  f1-score   support

        EGY       0.60      0.41      0.48       297
        GLF       0.45      0.58      0.51       259
        LAV       0.39      0.28      0.33       327
        MSA       0.56      0.79      0.65       280
        NOR       0.60      0.59      0.59       346

avg / total       0.52      0.52      0.51      1509


confusion_matrix
 [[121  36  59  54  27]
 [ 15 151  31  37  25]
 [ 33  78  93  57  66]
 [  5  24  11 222  18]
 [ 28  43  42  30 203]]

Evaluation on best model

f1_score
 0.5317091752770047
accuracy_score
 0.5374420145791915

classification_report
              precision    recall  f1-score   support

        EGY       0.53      0.51      0.52       297
        GLF       0.57      0.37      0.45       259
        LAV       0.41      0.45      0.43       327
        MSA       0.60      0.79      0.68       280
        NOR       0.61      0.57      0.59       346

avg / total       0.54      0.54      0.53      1509


confusion_matrix
 [[151  12  70  41  23]
 [ 30  95  74  33  27]
 [ 48  29 147  44  59]
 [ 10   9  21 222  18]
 [ 46  22  50  32 196]]
Closing remaining open files:data/vardial2018/dataset.h5...done
############# train: DONE @ Fri Mar 23 11:05:32 CET 2018
