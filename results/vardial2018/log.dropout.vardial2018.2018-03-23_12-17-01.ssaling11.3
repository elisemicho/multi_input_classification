############# train @ Fri Mar 23 12:17:01 CET 2018 GPUS=3  HOST=ssaling11 PWD=/home/michon/projects/VarDial2018/to_export/multi_input_modular
Loading data
Data Configurations loaded
Loading data
(13806, 8)
(1509, 8)
EGY    3085
LAV    2940
NOR    2866
GLF    2707
MSA    2208
Name: Class, dtype: int64
NOR    346
LAV    327
EGY    297
MSA    280
GLF    259
Name: Class, dtype: int64
Loading vocabularies
Words
48244 48244
Phones
45 45
39 39
61 61
51 51
Generating ids
Preprocessing data
Padding character sequences
(13806, 6830)
Padding phone sequences
(13806, 5885) (13806, 7329) (13806, 6436) (13806, 6837)
Turning labels in one-hot vectors
(13806, 5)
Taking ready-made acoustic embeddings
(13806, 600)
Padding character sequences
(1509, 6830)
Padding phone sequences
(1509, 5885) (1509, 7329) (1509, 6436) (1509, 6837)
Turning labels in one-hot vectors
(1509, 5)
Taking ready-made acoustic embeddings
(1509, 600)
MultiInputCharCNN Configurations loaded
Building the model
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
sent_input (InputLayer)         (None, 6830)         0                                            
__________________________________________________________________________________________________
phone_CZ_input (InputLayer)     (None, 5885)         0                                            
__________________________________________________________________________________________________
phone_EN_input (InputLayer)     (None, 7329)         0                                            
__________________________________________________________________________________________________
phone_HU_input (InputLayer)     (None, 6436)         0                                            
__________________________________________________________________________________________________
phone_RU_input (InputLayer)     (None, 6837)         0                                            
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 6830, 64)     6464        sent_input[0][0]                 
__________________________________________________________________________________________________
embedding_2 (Embedding)         (None, 5885, 64)     2944        phone_CZ_input[0][0]             
__________________________________________________________________________________________________
embedding_3 (Embedding)         (None, 7329, 64)     2560        phone_EN_input[0][0]             
__________________________________________________________________________________________________
embedding_4 (Embedding)         (None, 6436, 64)     3968        phone_HU_input[0][0]             
__________________________________________________________________________________________________
embedding_5 (Embedding)         (None, 6837, 64)     3328        phone_RU_input[0][0]             
__________________________________________________________________________________________________
zero_padding1d_1 (ZeroPadding1D (None, 6834, 64)     0           embedding_1[0][0]                
__________________________________________________________________________________________________
zero_padding1d_2 (ZeroPadding1D (None, 6838, 64)     0           embedding_1[0][0]                
__________________________________________________________________________________________________
zero_padding1d_3 (ZeroPadding1D (None, 5889, 64)     0           embedding_2[0][0]                
__________________________________________________________________________________________________
zero_padding1d_4 (ZeroPadding1D (None, 5893, 64)     0           embedding_2[0][0]                
__________________________________________________________________________________________________
zero_padding1d_5 (ZeroPadding1D (None, 7333, 64)     0           embedding_3[0][0]                
__________________________________________________________________________________________________
zero_padding1d_6 (ZeroPadding1D (None, 7337, 64)     0           embedding_3[0][0]                
__________________________________________________________________________________________________
zero_padding1d_7 (ZeroPadding1D (None, 6440, 64)     0           embedding_4[0][0]                
__________________________________________________________________________________________________
zero_padding1d_8 (ZeroPadding1D (None, 6444, 64)     0           embedding_4[0][0]                
__________________________________________________________________________________________________
zero_padding1d_9 (ZeroPadding1D (None, 6841, 64)     0           embedding_5[0][0]                
__________________________________________________________________________________________________
zero_padding1d_10 (ZeroPadding1 (None, 6845, 64)     0           embedding_5[0][0]                
__________________________________________________________________________________________________
conv1d_1 (Conv1D)               (None, 6834, 16)     3088        zero_padding1d_1[0][0]           
__________________________________________________________________________________________________
conv1d_2 (Conv1D)               (None, 6838, 16)     5136        zero_padding1d_2[0][0]           
__________________________________________________________________________________________________
conv1d_3 (Conv1D)               (None, 5889, 16)     3088        zero_padding1d_3[0][0]           
__________________________________________________________________________________________________
conv1d_4 (Conv1D)               (None, 5893, 16)     5136        zero_padding1d_4[0][0]           
__________________________________________________________________________________________________
conv1d_5 (Conv1D)               (None, 7333, 16)     3088        zero_padding1d_5[0][0]           
__________________________________________________________________________________________________
conv1d_6 (Conv1D)               (None, 7337, 16)     5136        zero_padding1d_6[0][0]           
__________________________________________________________________________________________________
conv1d_7 (Conv1D)               (None, 6440, 16)     3088        zero_padding1d_7[0][0]           
__________________________________________________________________________________________________
conv1d_8 (Conv1D)               (None, 6444, 16)     5136        zero_padding1d_8[0][0]           
__________________________________________________________________________________________________
conv1d_9 (Conv1D)               (None, 6841, 16)     3088        zero_padding1d_9[0][0]           
__________________________________________________________________________________________________
conv1d_10 (Conv1D)              (None, 6845, 16)     5136        zero_padding1d_10[0][0]          
__________________________________________________________________________________________________
global_max_pooling1d_1 (GlobalM (None, 16)           0           conv1d_1[0][0]                   
__________________________________________________________________________________________________
global_max_pooling1d_2 (GlobalM (None, 16)           0           conv1d_2[0][0]                   
__________________________________________________________________________________________________
global_max_pooling1d_3 (GlobalM (None, 16)           0           conv1d_3[0][0]                   
__________________________________________________________________________________________________
global_max_pooling1d_4 (GlobalM (None, 16)           0           conv1d_4[0][0]                   
__________________________________________________________________________________________________
global_max_pooling1d_5 (GlobalM (None, 16)           0           conv1d_5[0][0]                   
__________________________________________________________________________________________________
global_max_pooling1d_6 (GlobalM (None, 16)           0           conv1d_6[0][0]                   
__________________________________________________________________________________________________2018-03-23 12:17:11.126910: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-03-23 12:17:11.353620: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335
pciBusID: 0000:04:00.0
totalMemory: 7.92GiB freeMemory: 7.81GiB
2018-03-23 12:17:11.353653: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:04:00.0, compute capability: 6.1)

global_max_pooling1d_7 (GlobalM (None, 16)           0           conv1d_7[0][0]                   
__________________________________________________________________________________________________
global_max_pooling1d_8 (GlobalM (None, 16)           0           conv1d_8[0][0]                   
__________________________________________________________________________________________________
global_max_pooling1d_9 (GlobalM (None, 16)           0           conv1d_9[0][0]                   
__________________________________________________________________________________________________
global_max_pooling1d_10 (Global (None, 16)           0           conv1d_10[0][0]                  
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 16)           0           global_max_pooling1d_1[0][0]     
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 16)           0           global_max_pooling1d_2[0][0]     
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 16)           0           global_max_pooling1d_3[0][0]     
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 16)           0           global_max_pooling1d_4[0][0]     
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 16)           0           global_max_pooling1d_5[0][0]     
__________________________________________________________________________________________________
dropout_6 (Dropout)             (None, 16)           0           global_max_pooling1d_6[0][0]     
__________________________________________________________________________________________________
dropout_7 (Dropout)             (None, 16)           0           global_max_pooling1d_7[0][0]     
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 16)           0           global_max_pooling1d_8[0][0]     
__________________________________________________________________________________________________
dropout_9 (Dropout)             (None, 16)           0           global_max_pooling1d_9[0][0]     
__________________________________________________________________________________________________
dropout_10 (Dropout)            (None, 16)           0           global_max_pooling1d_10[0][0]    
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 160)          0           dropout_1[0][0]                  
                                                                 dropout_2[0][0]                  
                                                                 dropout_3[0][0]                  
                                                                 dropout_4[0][0]                  
                                                                 dropout_5[0][0]                  
                                                                 dropout_6[0][0]                  
                                                                 dropout_7[0][0]                  
                                                                 dropout_8[0][0]                  
                                                                 dropout_9[0][0]                  
                                                                 dropout_10[0][0]                 
__________________________________________________________________________________________________
embed_input (InputLayer)        (None, 600)          0                                            
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 760)          0           concatenate_1[0][0]              
                                                                 embed_input[0][0]                
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 128)          97408       concatenate_2[0][0]              
__________________________________________________________________________________________________
dropout_11 (Dropout)            (None, 128)          0           dense_1[0][0]                    
__________________________________________________________________________________________________
l_out (Dense)                   (None, 5)            645         dropout_11[0][0]                 
==================================================================================================
Total params: 158,437
Trainable params: 158,437
Non-trainable params: 0
__________________________________________________________________________________________________
Training Configurations loaded
Training the model
no checkpoints available !
Train on 13806 samples, validate on 1509 samples
Epoch 1/15

  128/13806 [..............................] - ETA: 3:20 - loss: 3.9810 - categorical_accuracy: 0.1562
  256/13806 [..............................] - ETA: 2:05 - loss: 3.8327 - categorical_accuracy: 0.2266
  384/13806 [..............................] - ETA: 1:40 - loss: 3.6895 - categorical_accuracy: 0.3542
  512/13806 [>.............................] - ETA: 1:27 - loss: 3.5789 - categorical_accuracy: 0.4102
  640/13806 [>.............................] - ETA: 1:19 - loss: 3.4855 - categorical_accuracy: 0.4703
  768/13806 [>.............................] - ETA: 1:13 - loss: 3.3900 - categorical_accuracy: 0.5391
  896/13806 [>.............................] - ETA: 1:09 - loss: 3.2992 - categorical_accuracy: 0.5871
 1024/13806 [=>............................] - ETA: 1:06 - loss: 3.2268 - categorical_accuracy: 0.6279
 1152/13806 [=>............................] - ETA: 1:04 - loss: 3.1510 - categorical_accuracy: 0.6597
 1280/13806 [=>............................] - ETA: 1:02 - loss: 3.0835 - categorical_accuracy: 0.6852
 1408/13806 [==>...........................] - ETA: 1:00 - loss: 3.0169 - categorical_accuracy: 0.7060
 1536/13806 [==>...........................] - ETA: 58s - loss: 2.9588 - categorical_accuracy: 0.7240 
 1664/13806 [==>...........................] - ETA: 57s - loss: 2.9010 - categorical_accuracy: 0.7410
 1792/13806 [==>...........................] - ETA: 55s - loss: 2.8489 - categorical_accuracy: 0.7533
 1920/13806 [===>..........................] - ETA: 54s - loss: 2.8007 - categorical_accuracy: 0.7667
 2048/13806 [===>..........................] - ETA: 53s - loss: 2.7516 - categorical_accuracy: 0.7788
 2176/13806 [===>..........................] - ETA: 52s - loss: 2.7059 - categorical_accuracy: 0.7904
 2304/13806 [====>.........................] - ETA: 51s - loss: 2.6614 - categorical_accuracy: 0.8008
 2432/13806 [====>.........................] - ETA: 50s - loss: 2.6190 - categorical_accuracy: 0.8100
 2560/13806 [====>.........................] - ETA: 49s - loss: 2.5799 - categorical_accuracy: 0.8184
 2688/13806 [====>.........................] - ETA: 48s - loss: 2.5427 - categorical_accuracy: 0.8248
 2816/13806 [=====>........................] - ETA: 47s - loss: 2.5072 - categorical_accuracy: 0.8313
 2944/13806 [=====>........................] - ETA: 47s - loss: 2.4726 - categorical_accuracy: 0.8373
 3072/13806 [=====>........................] - ETA: 46s - loss: 2.4389 - categorical_accuracy: 0.8421
 3200/13806 [=====>........................] - ETA: 45s - loss: 2.4091 - categorical_accuracy: 0.8469
 3328/13806 [======>.......................] - ETA: 44s - loss: 2.3781 - categorical_accuracy: 0.8519
 3456/13806 [======>.......................] - ETA: 44s - loss: 2.3486 - categorical_accuracy: 0.8562
 3584/13806 [======>.......................] - ETA: 43s - loss: 2.3190 - categorical_accuracy: 0.8602
 3712/13806 [=======>......................] - ETA: 42s - loss: 2.2891 - categorical_accuracy: 0.8648
 3840/13806 [=======>......................] - ETA: 42s - loss: 2.2622 - categorical_accuracy: 0.8682
 3968/13806 [=======>......................] - ETA: 41s - loss: 2.2345 - categorical_accuracy: 0.8720
 4096/13806 [=======>......................] - ETA: 40s - loss: 2.2082 - categorical_accuracy: 0.8752
 4224/13806 [========>.....................] - ETA: 40s - loss: 2.1831 - categorical_accuracy: 0.8786
 4352/13806 [========>.....................] - ETA: 39s - loss: 2.1584 - categorical_accuracy: 0.8812
 4480/13806 [========>.....................] - ETA: 38s - loss: 2.1334 - categorical_accuracy: 0.8842
 4608/13806 [=========>....................] - ETA: 38s - loss: 2.1105 - categorical_accuracy: 0.8867
 4736/13806 [=========>....................] - ETA: 37s - loss: 2.0881 - categorical_accuracy: 0.8891
 4864/13806 [=========>....................] - ETA: 37s - loss: 2.0647 - categorical_accuracy: 0.8914
 4992/13806 [=========>....................] - ETA: 36s - loss: 2.0417 - categorical_accuracy: 0.8938
 5120/13806 [==========>...................] - ETA: 35s - loss: 2.0217 - categorical_accuracy: 0.8953
 5248/13806 [==========>...................] - ETA: 35s - loss: 1.9999 - categorical_accuracy: 0.8973
 5376/13806 [==========>...................] - ETA: 34s - loss: 1.9794 - categorical_accuracy: 0.8990
 5504/13806 [==========>...................] - ETA: 34s - loss: 1.9591 - categorical_accuracy: 0.9010
 5632/13806 [===========>..................] - ETA: 33s - loss: 1.9392 - categorical_accuracy: 0.9027
 5760/13806 [===========>..................] - ETA: 33s - loss: 1.9202 - categorical_accuracy: 0.9043
 5888/13806 [===========>..................] - ETA: 32s - loss: 1.9010 - categorical_accuracy: 0.9059
 6016/13806 [============>.................] - ETA: 31s - loss: 1.8827 - categorical_accuracy: 0.9076
 6144/13806 [============>.................] - ETA: 31s - loss: 1.8635 - categorical_accuracy: 0.9093
 6272/13806 [============>.................] - ETA: 30s - loss: 1.8445 - categorical_accuracy: 0.9109
 6400/13806 [============>.................] - ETA: 30s - loss: 1.8266 - categorical_accuracy: 0.9122
 6528/13806 [=============>................] - ETA: 29s - loss: 1.8092 - categorical_accuracy: 0.9131
 6656/13806 [=============>................] - ETA: 29s - loss: 1.7920 - categorical_accuracy: 0.9144
 6784/13806 [=============>................] - ETA: 28s - loss: 1.7750 - categorical_accuracy: 0.9157
 6912/13806 [==============>...............] - ETA: 28s - loss: 1.7589 - categorical_accuracy: 0.9168
 7040/13806 [==============>...............] - ETA: 27s - loss: 1.7423 - categorical_accuracy: 0.9180
 7168/13806 [==============>...............] - ETA: 26s - loss: 1.7259 - categorical_accuracy: 0.9194
 7296/13806 [==============>...............] - ETA: 26s - loss: 1.7100 - categorical_accuracy: 0.9205
 7424/13806 [===============>..............] - ETA: 25s - loss: 1.6951 - categorical_accuracy: 0.9215
 7552/13806 [===============>..............] - ETA: 25s - loss: 1.6805 - categorical_accuracy: 0.9223
 7680/13806 [===============>..............] - ETA: 24s - loss: 1.6663 - categorical_accuracy: 0.9232
 7808/13806 [===============>..............] - ETA: 24s - loss: 1.6512 - categorical_accuracy: 0.9244
 7936/13806 [================>.............] - ETA: 23s - loss: 1.6368 - categorical_accuracy: 0.9254
 8064/13806 [================>.............] - ETA: 23s - loss: 1.6220 - categorical_accuracy: 0.9265
 8192/13806 [================>.............] - ETA: 22s - loss: 1.6076 - categorical_accuracy: 0.9275
 8320/13806 [=================>............] - ETA: 22s - loss: 1.5938 - categorical_accuracy: 0.9282
 8448/13806 [=================>............] - ETA: 21s - loss: 1.5802 - categorical_accuracy: 0.9290
 8576/13806 [=================>............] - ETA: 21s - loss: 1.5665 - categorical_accuracy: 0.9298
 8704/13806 [=================>............] - ETA: 20s - loss: 1.5543 - categorical_accuracy: 0.9301
 8832/13806 [==================>...........] - ETA: 20s - loss: 1.5412 - categorical_accuracy: 0.9309
 8960/13806 [==================>...........] - ETA: 19s - loss: 1.5287 - categorical_accuracy: 0.9316
 9088/13806 [==================>...........] - ETA: 19s - loss: 1.5169 - categorical_accuracy: 0.9322
 9216/13806 [===================>..........] - ETA: 18s - loss: 1.5045 - categorical_accuracy: 0.9328
 9344/13806 [===================>..........] - ETA: 17s - loss: 1.4922 - categorical_accuracy: 0.9335
 9472/13806 [===================>..........] - ETA: 17s - loss: 1.4798 - categorical_accuracy: 0.9343
 9600/13806 [===================>..........] - ETA: 16s - loss: 1.4679 - categorical_accuracy: 0.9350
 9728/13806 [====================>.........] - ETA: 16s - loss: 1.4562 - categorical_accuracy: 0.9354
 9856/13806 [====================>.........] - ETA: 15s - loss: 1.4449 - categorical_accuracy: 0.9360
 9984/13806 [====================>.........] - ETA: 15s - loss: 1.4341 - categorical_accuracy: 0.9363
10112/13806 [====================>.........] - ETA: 14s - loss: 1.4238 - categorical_accuracy: 0.9368
10240/13806 [=====================>........] - ETA: 14s - loss: 1.4128 - categorical_accuracy: 0.9374
10368/13806 [=====================>........] - ETA: 13s - loss: 1.4021 - categorical_accuracy: 0.9379
10496/13806 [=====================>........] - ETA: 13s - loss: 1.3915 - categorical_accuracy: 0.9384
10624/13806 [======================>.......] - ETA: 12s - loss: 1.3806 - categorical_accuracy: 0.9391
10752/13806 [======================>.......] - ETA: 12s - loss: 1.3712 - categorical_accuracy: 0.9394
10880/13806 [======================>.......] - ETA: 11s - loss: 1.3612 - categorical_accuracy: 0.9398
11008/13806 [======================>.......] - ETA: 11s - loss: 1.3513 - categorical_accuracy: 0.9403
11136/13806 [=======================>......] - ETA: 10s - loss: 1.3419 - categorical_accuracy: 0.9407
11264/13806 [=======================>......] - ETA: 10s - loss: 1.3321 - categorical_accuracy: 0.9412
11392/13806 [=======================>......] - ETA: 9s - loss: 1.3224 - categorical_accuracy: 0.9418 
11520/13806 [========================>.....] - ETA: 9s - loss: 1.3142 - categorical_accuracy: 0.9419
11648/13806 [========================>.....] - ETA: 8s - loss: 1.3053 - categorical_accuracy: 0.9422
11776/13806 [========================>.....] - ETA: 8s - loss: 1.2958 - categorical_accuracy: 0.9428
11904/13806 [========================>.....] - ETA: 7s - loss: 1.2868 - categorical_accuracy: 0.9432
12032/13806 [=========================>....] - ETA: 7s - loss: 1.2783 - categorical_accuracy: 0.9435
12160/13806 [=========================>....] - ETA: 6s - loss: 1.2694 - categorical_accuracy: 0.9439
12288/13806 [=========================>....] - ETA: 6s - loss: 1.2616 - categorical_accuracy: 0.9440
12416/13806 [=========================>....] - ETA: 5s - loss: 1.2528 - categorical_accuracy: 0.9445
12544/13806 [==========================>...] - ETA: 5s - loss: 1.2441 - categorical_accuracy: 0.9449
12672/13806 [==========================>...] - ETA: 4s - loss: 1.2359 - categorical_accuracy: 0.9452
12800/13806 [==========================>...] - ETA: 4s - loss: 1.2277 - categorical_accuracy: 0.9457
12928/13806 [===========================>..] - ETA: 3s - loss: 1.2192 - categorical_accuracy: 0.9462
13056/13806 [===========================>..] - ETA: 2s - loss: 1.2117 - categorical_accuracy: 0.9465
13184/13806 [===========================>..] - ETA: 2s - loss: 1.2032 - categorical_accuracy: 0.9471
13312/13806 [===========================>..] - ETA: 1s - loss: 1.1953 - categorical_accuracy: 0.9475
13440/13806 [============================>.] - ETA: 1s - loss: 1.1877 - categorical_accuracy: 0.9478
13568/13806 [============================>.] - ETA: 0s - loss: 1.1797 - categorical_accuracy: 0.9483
13696/13806 [============================>.] - ETA: 0s - loss: 1.1725 - categorical_accuracy: 0.9484
13806/13806 [==============================] - 57s 4ms/step - loss: 1.1666 - categorical_accuracy: 0.9485 - val_loss: 1.6777 - val_categorical_accuracy: 0.5249

Epoch 00001: val_categorical_accuracy improved from -inf to 0.52485, saving model to results/vardial2018/multi_input_with_dropout/model_weights.hdf5
Epoch 2/15

  128/13806 [..............................] - ETA: 53s - loss: 0.3581 - categorical_accuracy: 0.9766
  256/13806 [..............................] - ETA: 52s - loss: 0.3692 - categorical_accuracy: 0.9766
  384/13806 [..............................] - ETA: 52s - loss: 0.3652 - categorical_accuracy: 0.9740
  512/13806 [>.............................] - ETA: 51s - loss: 0.3788 - categorical_accuracy: 0.9707
  640/13806 [>.............................] - ETA: 51s - loss: 0.3890 - categorical_accuracy: 0.9688
  768/13806 [>.............................] - ETA: 50s - loss: 0.3958 - categorical_accuracy: 0.9701
  896/13806 [>.............................] - ETA: 50s - loss: 0.3820 - categorical_accuracy: 0.9743
 1024/13806 [=>............................] - ETA: 49s - loss: 0.3862 - categorical_accuracy: 0.9717
 1152/13806 [=>............................] - ETA: 49s - loss: 0.3822 - categorical_accuracy: 0.9740
 1280/13806 [=>............................] - ETA: 48s - loss: 0.3831 - categorical_accuracy: 0.9727
 1408/13806 [==>...........................] - ETA: 48s - loss: 0.3775 - categorical_accuracy: 0.9737
 1536/13806 [==>...........................] - ETA: 47s - loss: 0.3766 - categorical_accuracy: 0.9727
 1664/13806 [==>...........................] - ETA: 47s - loss: 0.3758 - categorical_accuracy: 0.9730
 1792/13806 [==>...........................] - ETA: 46s - loss: 0.3737 - categorical_accuracy: 0.9732
 1920/13806 [===>..........................] - ETA: 46s - loss: 0.3711 - categorical_accuracy: 0.9740
 2048/13806 [===>..........................] - ETA: 45s - loss: 0.3665 - categorical_accuracy: 0.9746
 2176/13806 [===>..........................] - ETA: 45s - loss: 0.3663 - categorical_accuracy: 0.9747
 2304/13806 [====>.........................] - ETA: 44s - loss: 0.3653 - categorical_accuracy: 0.9748
 2432/13806 [====>.........................] - ETA: 44s - loss: 0.3639 - categorical_accuracy: 0.9753
 2560/13806 [====>.........................] - ETA: 43s - loss: 0.3584 - categorical_accuracy: 0.9766
 2688/13806 [====>.........................] - ETA: 43s - loss: 0.3583 - categorical_accuracy: 0.9758
 2816/13806 [=====>........................] - ETA: 42s - loss: 0.3578 - categorical_accuracy: 0.9755
 2944/13806 [=====>........................] - ETA: 42s - loss: 0.3573 - categorical_accuracy: 0.9749
 3072/13806 [=====>........................] - ETA: 41s - loss: 0.3549 - categorical_accuracy: 0.9753
 3200/13806 [=====>........................] - ETA: 41s - loss: 0.3564 - categorical_accuracy: 0.9747
 3328/13806 [======>.......................] - ETA: 40s - loss: 0.3535 - categorical_accuracy: 0.9751
 3456/13806 [======>.......................] - ETA: 40s - loss: 0.3498 - categorical_accuracy: 0.9757
 3584/13806 [======>.......................] - ETA: 39s - loss: 0.3485 - categorical_accuracy: 0.9760
 3712/13806 [=======>......................] - ETA: 39s - loss: 0.3469 - categorical_accuracy: 0.9763
 3840/13806 [=======>......................] - ETA: 38s - loss: 0.3447 - categorical_accuracy: 0.9766
 3968/13806 [=======>......................] - ETA: 38s - loss: 0.3437 - categorical_accuracy: 0.9758
 4096/13806 [=======>......................] - ETA: 37s - loss: 0.3423 - categorical_accuracy: 0.9758
 4224/13806 [========>.....................] - ETA: 37s - loss: 0.3438 - categorical_accuracy: 0.9751
 4352/13806 [========>.....................] - ETA: 36s - loss: 0.3406 - categorical_accuracy: 0.9759
 4480/13806 [========>.....................] - ETA: 36s - loss: 0.3384 - categorical_accuracy: 0.9757
 4608/13806 [=========>....................] - ETA: 35s - loss: 0.3373 - categorical_accuracy: 0.9757
 4736/13806 [=========>....................] - ETA: 35s - loss: 0.3364 - categorical_accuracy: 0.9757
 4864/13806 [=========>....................] - ETA: 34s - loss: 0.3332 - categorical_accuracy: 0.9764
 4992/13806 [=========>....................] - ETA: 34s - loss: 0.3315 - categorical_accuracy: 0.9764
 5120/13806 [==========>...................] - ETA: 33s - loss: 0.3302 - categorical_accuracy: 0.9762
 5248/13806 [==========>...................] - ETA: 33s - loss: 0.3284 - categorical_accuracy: 0.9760
 5376/13806 [==========>...................] - ETA: 32s - loss: 0.3268 - categorical_accuracy: 0.9762
 5504/13806 [==========>...................] - ETA: 32s - loss: 0.3247 - categorical_accuracy: 0.9766
 5632/13806 [===========>..................] - ETA: 31s - loss: 0.3243 - categorical_accuracy: 0.9766
 5760/13806 [===========>..................] - ETA: 31s - loss: 0.3217 - categorical_accuracy: 0.9771
 5888/13806 [===========>..................] - ETA: 30s - loss: 0.3207 - categorical_accuracy: 0.9771
 6016/13806 [============>.................] - ETA: 30s - loss: 0.3216 - categorical_accuracy: 0.9769
 6144/13806 [============>.................] - ETA: 29s - loss: 0.3196 - categorical_accuracy: 0.9771
 6272/13806 [============>.................] - ETA: 29s - loss: 0.3191 - categorical_accuracy: 0.9772
 6400/13806 [============>.................] - ETA: 28s - loss: 0.3180 - categorical_accuracy: 0.9770
 6528/13806 [=============>................] - ETA: 28s - loss: 0.3159 - categorical_accuracy: 0.9773
 6656/13806 [=============>................] - ETA: 27s - loss: 0.3152 - categorical_accuracy: 0.9770
 6784/13806 [=============>................] - ETA: 27s - loss: 0.3131 - categorical_accuracy: 0.9773
 6912/13806 [==============>...............] - ETA: 26s - loss: 0.3110 - categorical_accuracy: 0.9777
 7040/13806 [==============>...............] - ETA: 26s - loss: 0.3093 - categorical_accuracy: 0.9777
 7168/13806 [==============>...............] - ETA: 25s - loss: 0.3076 - categorical_accuracy: 0.9780
 7296/13806 [==============>...............] - ETA: 25s - loss: 0.3065 - categorical_accuracy: 0.9779
 7424/13806 [===============>..............] - ETA: 24s - loss: 0.3044 - categorical_accuracy: 0.9782
 7552/13806 [===============>..............] - ETA: 24s - loss: 0.3041 - categorical_accuracy: 0.9779
 7680/13806 [===============>..............] - ETA: 23s - loss: 0.3023 - categorical_accuracy: 0.9781
 7808/13806 [===============>..............] - ETA: 23s - loss: 0.3003 - categorical_accuracy: 0.9784
 7936/13806 [================>.............] - ETA: 22s - loss: 0.2988 - categorical_accuracy: 0.9786
 8064/13806 [================>.............] - ETA: 22s - loss: 0.2973 - categorical_accuracy: 0.9787
 8192/13806 [================>.............] - ETA: 21s - loss: 0.2955 - categorical_accuracy: 0.9790
 8320/13806 [=================>............] - ETA: 21s - loss: 0.2945 - categorical_accuracy: 0.9790
 8448/13806 [=================>............] - ETA: 20s - loss: 0.2950 - categorical_accuracy: 0.9788
 8576/13806 [=================>............] - ETA: 20s - loss: 0.2940 - categorical_accuracy: 0.9789
 8704/13806 [=================>............] - ETA: 19s - loss: 0.2935 - categorical_accuracy: 0.9789
 8832/13806 [==================>...........] - ETA: 19s - loss: 0.2920 - categorical_accuracy: 0.9791
 8960/13806 [==================>...........] - ETA: 18s - loss: 0.2906 - categorical_accuracy: 0.9792
 9088/13806 [==================>...........] - ETA: 18s - loss: 0.2902 - categorical_accuracy: 0.9792
 9216/13806 [===================>..........] - ETA: 17s - loss: 0.2892 - categorical_accuracy: 0.9793
 9344/13806 [===================>..........] - ETA: 17s - loss: 0.2883 - categorical_accuracy: 0.9793
 9472/13806 [===================>..........] - ETA: 16s - loss: 0.2868 - categorical_accuracy: 0.9796
 9600/13806 [===================>..........] - ETA: 16s - loss: 0.2858 - categorical_accuracy: 0.9796
 9728/13806 [====================>.........] - ETA: 15s - loss: 0.2843 - categorical_accuracy: 0.9799
 9856/13806 [====================>.........] - ETA: 15s - loss: 0.2826 - categorical_accuracy: 0.9801
 9984/13806 [====================>.........] - ETA: 14s - loss: 0.2813 - categorical_accuracy: 0.9802
10112/13806 [====================>.........] - ETA: 14s - loss: 0.2801 - categorical_accuracy: 0.9803
10240/13806 [=====================>........] - ETA: 13s - loss: 0.2787 - categorical_accuracy: 0.9804
10368/13806 [=====================>........] - ETA: 13s - loss: 0.2774 - categorical_accuracy: 0.9805
10496/13806 [=====================>........] - ETA: 12s - loss: 0.2761 - categorical_accuracy: 0.9807
10624/13806 [======================>.......] - ETA: 12s - loss: 0.2759 - categorical_accuracy: 0.9807
10752/13806 [======================>.......] - ETA: 11s - loss: 0.2755 - categorical_accuracy: 0.9807
10880/13806 [======================>.......] - ETA: 11s - loss: 0.2748 - categorical_accuracy: 0.9805
11008/13806 [======================>.......] - ETA: 10s - loss: 0.2737 - categorical_accuracy: 0.9806
11136/13806 [=======================>......] - ETA: 10s - loss: 0.2726 - categorical_accuracy: 0.9807
11264/13806 [=======================>......] - ETA: 9s - loss: 0.2731 - categorical_accuracy: 0.9804 
11392/13806 [=======================>......] - ETA: 9s - loss: 0.2721 - categorical_accuracy: 0.9804
11520/13806 [========================>.....] - ETA: 8s - loss: 0.2711 - categorical_accuracy: 0.9805
11648/13806 [========================>.....] - ETA: 8s - loss: 0.2699 - categorical_accuracy: 0.9807
11776/13806 [========================>.....] - ETA: 7s - loss: 0.2689 - categorical_accuracy: 0.9808
11904/13806 [========================>.....] - ETA: 7s - loss: 0.2681 - categorical_accuracy: 0.9809
12032/13806 [=========================>....] - ETA: 6s - loss: 0.2674 - categorical_accuracy: 0.9808
12160/13806 [=========================>....] - ETA: 6s - loss: 0.2660 - categorical_accuracy: 0.9810
12288/13806 [=========================>....] - ETA: 5s - loss: 0.2648 - categorical_accuracy: 0.9812
12416/13806 [=========================>....] - ETA: 5s - loss: 0.2636 - categorical_accuracy: 0.9814
12544/13806 [==========================>...] - ETA: 4s - loss: 0.2626 - categorical_accuracy: 0.9814
12672/13806 [==========================>...] - ETA: 4s - loss: 0.2617 - categorical_accuracy: 0.9815
12800/13806 [==========================>...] - ETA: 3s - loss: 0.2609 - categorical_accuracy: 0.9816
12928/13806 [===========================>..] - ETA: 3s - loss: 0.2598 - categorical_accuracy: 0.9817
13056/13806 [===========================>..] - ETA: 2s - loss: 0.2593 - categorical_accuracy: 0.9817
13184/13806 [===========================>..] - ETA: 2s - loss: 0.2583 - categorical_accuracy: 0.9818
13312/13806 [===========================>..] - ETA: 1s - loss: 0.2578 - categorical_accuracy: 0.9816
13440/13806 [============================>.] - ETA: 1s - loss: 0.2570 - categorical_accuracy: 0.9816
13568/13806 [============================>.] - ETA: 0s - loss: 0.2565 - categorical_accuracy: 0.9816
13696/13806 [============================>.] - ETA: 0s - loss: 0.2556 - categorical_accuracy: 0.9817
13806/13806 [==============================] - 56s 4ms/step - loss: 0.2548 - categorical_accuracy: 0.9817 - val_loss: 1.5188 - val_categorical_accuracy: 0.5162

Epoch 00002: val_categorical_accuracy did not improve
Epoch 3/15

  128/13806 [..............................] - ETA: 55s - loss: 0.1381 - categorical_accuracy: 0.9922
  256/13806 [..............................] - ETA: 54s - loss: 0.1451 - categorical_accuracy: 0.9883
  384/13806 [..............................] - ETA: 54s - loss: 0.1618 - categorical_accuracy: 0.9844
  512/13806 [>.............................] - ETA: 53s - loss: 0.2116 - categorical_accuracy: 0.9805
  640/13806 [>.............................] - ETA: 53s - loss: 0.2000 - categorical_accuracy: 0.9812
  768/13806 [>.............................] - ETA: 52s - loss: 0.2119 - categorical_accuracy: 0.9753
  896/13806 [>.............................] - ETA: 52s - loss: 0.2062 - categorical_accuracy: 0.9777
 1024/13806 [=>............................] - ETA: 51s - loss: 0.2015 - categorical_accuracy: 0.9785
 1152/13806 [=>............................] - ETA: 50s - loss: 0.1944 - categorical_accuracy: 0.9809
 1280/13806 [=>............................] - ETA: 50s - loss: 0.1985 - categorical_accuracy: 0.9805
 1408/13806 [==>...........................] - ETA: 49s - loss: 0.1965 - categorical_accuracy: 0.9815
 1536/13806 [==>...........................] - ETA: 49s - loss: 0.1913 - categorical_accuracy: 0.9831
 1664/13806 [==>...........................] - ETA: 48s - loss: 0.1874 - categorical_accuracy: 0.9832
 1792/13806 [==>...........................] - ETA: 48s - loss: 0.1859 - categorical_accuracy: 0.9838
 1920/13806 [===>..........................] - ETA: 47s - loss: 0.1875 - categorical_accuracy: 0.9844
 2048/13806 [===>..........................] - ETA: 47s - loss: 0.1879 - categorical_accuracy: 0.9844
 2176/13806 [===>..........................] - ETA: 46s - loss: 0.1878 - categorical_accuracy: 0.9839
 2304/13806 [====>.........................] - ETA: 46s - loss: 0.1883 - categorical_accuracy: 0.9839
 2432/13806 [====>.........................] - ETA: 45s - loss: 0.1862 - categorical_accuracy: 0.9840
 2560/13806 [====>.........................] - ETA: 45s - loss: 0.1842 - categorical_accuracy: 0.9848
 2688/13806 [====>.........................] - ETA: 44s - loss: 0.1824 - categorical_accuracy: 0.9851
 2816/13806 [=====>........................] - ETA: 44s - loss: 0.1811 - categorical_accuracy: 0.9854
 2944/13806 [=====>........................] - ETA: 43s - loss: 0.1810 - categorical_accuracy: 0.9854
 3072/13806 [=====>........................] - ETA: 43s - loss: 0.1812 - categorical_accuracy: 0.9854
 3200/13806 [=====>........................] - ETA: 42s - loss: 0.1807 - categorical_accuracy: 0.9853
 3328/13806 [======>.......................] - ETA: 42s - loss: 0.1804 - categorical_accuracy: 0.9850
 3456/13806 [======>.......................] - ETA: 41s - loss: 0.1813 - categorical_accuracy: 0.9850
 3584/13806 [======>.......................] - ETA: 41s - loss: 0.1808 - categorical_accuracy: 0.9849
 3712/13806 [=======>......................] - ETA: 40s - loss: 0.1804 - categorical_accuracy: 0.9849
 3840/13806 [=======>......................] - ETA: 40s - loss: 0.1799 - categorical_accuracy: 0.9849
 3968/13806 [=======>......................] - ETA: 39s - loss: 0.1783 - categorical_accuracy: 0.9851
 4096/13806 [=======>......................] - ETA: 39s - loss: 0.1784 - categorical_accuracy: 0.9849
 4224/13806 [========>.....................] - ETA: 38s - loss: 0.1776 - categorical_accuracy: 0.9844
 4352/13806 [========>.....................] - ETA: 38s - loss: 0.1772 - categorical_accuracy: 0.9841
 4480/13806 [========>.....................] - ETA: 37s - loss: 0.1777 - categorical_accuracy: 0.9842
 4608/13806 [=========>....................] - ETA: 37s - loss: 0.1785 - categorical_accuracy: 0.9837
 4736/13806 [=========>....................] - ETA: 36s - loss: 0.1803 - categorical_accuracy: 0.9831
 4864/13806 [=========>....................] - ETA: 36s - loss: 0.1808 - categorical_accuracy: 0.9827
 4992/13806 [=========>....................] - ETA: 35s - loss: 0.1793 - categorical_accuracy: 0.9830
 5120/13806 [==========>...................] - ETA: 35s - loss: 0.1777 - categorical_accuracy: 0.9832
 5248/13806 [==========>...................] - ETA: 34s - loss: 0.1778 - categorical_accuracy: 0.9834
 5376/13806 [==========>...................] - ETA: 34s - loss: 0.1776 - categorical_accuracy: 0.9829
 5504/13806 [==========>...................] - ETA: 33s - loss: 0.1778 - categorical_accuracy: 0.9822
 5632/13806 [===========>..................] - ETA: 33s - loss: 0.1769 - categorical_accuracy: 0.9824
 5760/13806 [===========>..................] - ETA: 32s - loss: 0.1765 - categorical_accuracy: 0.9825
 5888/13806 [===========>..................] - ETA: 32s - loss: 0.1753 - categorical_accuracy: 0.9828
 6016/13806 [============>.................] - ETA: 31s - loss: 0.1764 - categorical_accuracy: 0.9827
 6144/13806 [============>.................] - ETA: 31s - loss: 0.1759 - categorical_accuracy: 0.9826
 6272/13806 [============>.................] - ETA: 30s - loss: 0.1746 - categorical_accuracy: 0.9829
 6400/13806 [============>.................] - ETA: 29s - loss: 0.1743 - categorical_accuracy: 0.9830
 6528/13806 [=============>................] - ETA: 29s - loss: 0.1750 - categorical_accuracy: 0.9827
 6656/13806 [=============>................] - ETA: 28s - loss: 0.1755 - categorical_accuracy: 0.9826
 6784/13806 [=============>................] - ETA: 28s - loss: 0.1761 - categorical_accuracy: 0.9820
 6912/13806 [==============>...............] - ETA: 27s - loss: 0.1752 - categorical_accuracy: 0.9823
 7040/13806 [==============>...............] - ETA: 27s - loss: 0.1747 - categorical_accuracy: 0.9824
 7168/13806 [==============>...............] - ETA: 26s - loss: 0.1735 - categorical_accuracy: 0.9826
 7296/13806 [==============>...............] - ETA: 26s - loss: 0.1734 - categorical_accuracy: 0.9823
 7424/13806 [===============>..............] - ETA: 25s - loss: 0.1726 - categorical_accuracy: 0.9825
 7552/13806 [===============>..............] - ETA: 25s - loss: 0.1730 - categorical_accuracy: 0.9823
 7680/13806 [===============>..............] - ETA: 24s - loss: 0.1725 - categorical_accuracy: 0.9823
 7808/13806 [===============>..............] - ETA: 24s - loss: 0.1734 - categorical_accuracy: 0.9822
 7936/13806 [================>.............] - ETA: 23s - loss: 0.1750 - categorical_accuracy: 0.9819
 8064/13806 [================>.............] - ETA: 23s - loss: 0.1748 - categorical_accuracy: 0.9819
 8192/13806 [================>.............] - ETA: 22s - loss: 0.1739 - categorical_accuracy: 0.9822
 8320/13806 [=================>............] - ETA: 22s - loss: 0.1735 - categorical_accuracy: 0.9822
 8448/13806 [=================>............] - ETA: 21s - loss: 0.1727 - categorical_accuracy: 0.9825
 8576/13806 [=================>............] - ETA: 21s - loss: 0.1737 - categorical_accuracy: 0.9823
 8704/13806 [=================>............] - ETA: 20s - loss: 0.1734 - categorical_accuracy: 0.9824
 8832/13806 [==================>...........] - ETA: 20s - loss: 0.1732 - categorical_accuracy: 0.9823
 8960/13806 [==================>...........] - ETA: 19s - loss: 0.1726 - categorical_accuracy: 0.9825
 9088/13806 [==================>...........] - ETA: 19s - loss: 0.1718 - categorical_accuracy: 0.9826
 9216/13806 [===================>..........] - ETA: 18s - loss: 0.1714 - categorical_accuracy: 0.9827
 9344/13806 [===================>..........] - ETA: 18s - loss: 0.1718 - categorical_accuracy: 0.9827
 9472/13806 [===================>..........] - ETA: 17s - loss: 0.1709 - categorical_accuracy: 0.9828
 9600/13806 [===================>..........] - ETA: 17s - loss: 0.1707 - categorical_accuracy: 0.9829
 9728/13806 [====================>.........] - ETA: 16s - loss: 0.1700 - categorical_accuracy: 0.9831
 9856/13806 [====================>.........] - ETA: 16s - loss: 0.1699 - categorical_accuracy: 0.9832
 9984/13806 [====================>.........] - ETA: 15s - loss: 0.1711 - categorical_accuracy: 0.9829
10112/13806 [====================>.........] - ETA: 14s - loss: 0.1710 - categorical_accuracy: 0.9827
10240/13806 [=====================>........] - ETA: 14s - loss: 0.1701 - categorical_accuracy: 0.9829
10368/13806 [=====================>........] - ETA: 13s - loss: 0.1700 - categorical_accuracy: 0.9828
10496/13806 [=====================>........] - ETA: 13s - loss: 0.1702 - categorical_accuracy: 0.9827
10624/13806 [======================>.......] - ETA: 12s - loss: 0.1698 - categorical_accuracy: 0.9827
10752/13806 [======================>.......] - ETA: 12s - loss: 0.1704 - categorical_accuracy: 0.9825
10880/13806 [======================>.......] - ETA: 11s - loss: 0.1696 - categorical_accuracy: 0.9826
11008/13806 [======================>.......] - ETA: 11s - loss: 0.1697 - categorical_accuracy: 0.9826
11136/13806 [=======================>......] - ETA: 10s - loss: 0.1702 - categorical_accuracy: 0.9822
11264/13806 [=======================>......] - ETA: 10s - loss: 0.1697 - categorical_accuracy: 0.9822
11392/13806 [=======================>......] - ETA: 9s - loss: 0.1695 - categorical_accuracy: 0.9823 
11520/13806 [========================>.....] - ETA: 9s - loss: 0.1693 - categorical_accuracy: 0.9822
11648/13806 [========================>.....] - ETA: 8s - loss: 0.1694 - categorical_accuracy: 0.9820
11776/13806 [========================>.....] - ETA: 8s - loss: 0.1690 - categorical_accuracy: 0.9820
11904/13806 [========================>.....] - ETA: 7s - loss: 0.1684 - categorical_accuracy: 0.9822
12032/13806 [=========================>....] - ETA: 7s - loss: 0.1676 - categorical_accuracy: 0.9824
12160/13806 [=========================>....] - ETA: 6s - loss: 0.1673 - categorical_accuracy: 0.9824
12288/13806 [=========================>....] - ETA: 6s - loss: 0.1670 - categorical_accuracy: 0.9823
12416/13806 [=========================>....] - ETA: 5s - loss: 0.1667 - categorical_accuracy: 0.9823
12544/13806 [==========================>...] - ETA: 5s - loss: 0.1666 - categorical_accuracy: 0.9821
12672/13806 [==========================>...] - ETA: 4s - loss: 0.1661 - categorical_accuracy: 0.9822
12800/13806 [==========================>...] - ETA: 4s - loss: 0.1658 - categorical_accuracy: 0.9821
12928/13806 [===========================>..] - ETA: 3s - loss: 0.1666 - categorical_accuracy: 0.9820
13056/13806 [===========================>..] - ETA: 3s - loss: 0.1667 - categorical_accuracy: 0.9820
13184/13806 [===========================>..] - ETA: 2s - loss: 0.1670 - categorical_accuracy: 0.9819
13312/13806 [===========================>..] - ETA: 2s - loss: 0.1664 - categorical_accuracy: 0.9821
13440/13806 [============================>.] - ETA: 1s - loss: 0.1666 - categorical_accuracy: 0.9821
13568/13806 [============================>.] - ETA: 0s - loss: 0.1661 - categorical_accuracy: 0.9822
13696/13806 [============================>.] - ETA: 0s - loss: 0.1656 - categorical_accuracy: 0.9823
13806/13806 [==============================] - 58s 4ms/step - loss: 0.1650 - categorical_accuracy: 0.9825 - val_loss: 1.4941 - val_categorical_accuracy: 0.5341

Epoch 00003: val_categorical_accuracy improved from 0.52485 to 0.53413, saving model to results/vardial2018/multi_input_with_dropout/model_weights.hdf5
Epoch 4/15

  128/13806 [..............................] - ETA: 55s - loss: 0.1136 - categorical_accuracy: 0.9922
  256/13806 [..............................] - ETA: 55s - loss: 0.1592 - categorical_accuracy: 0.9805
  384/13806 [..............................] - ETA: 54s - loss: 0.1681 - categorical_accuracy: 0.9792
  512/13806 [>.............................] - ETA: 53s - loss: 0.1677 - categorical_accuracy: 0.9727
  640/13806 [>.............................] - ETA: 53s - loss: 0.1613 - categorical_accuracy: 0.9750
  768/13806 [>.............................] - ETA: 52s - loss: 0.1490 - categorical_accuracy: 0.9792
  896/13806 [>.............................] - ETA: 52s - loss: 0.1518 - categorical_accuracy: 0.9788
 1024/13806 [=>............................] - ETA: 51s - loss: 0.1512 - categorical_accuracy: 0.9795
 1152/13806 [=>............................] - ETA: 51s - loss: 0.1619 - categorical_accuracy: 0.9774
 1280/13806 [=>............................] - ETA: 50s - loss: 0.1553 - categorical_accuracy: 0.9797
 1408/13806 [==>...........................] - ETA: 50s - loss: 0.1517 - categorical_accuracy: 0.9815
 1536/13806 [==>...........................] - ETA: 49s - loss: 0.1518 - categorical_accuracy: 0.9818
 1664/13806 [==>...........................] - ETA: 49s - loss: 0.1510 - categorical_accuracy: 0.9808
 1792/13806 [==>...........................] - ETA: 48s - loss: 0.1472 - categorical_accuracy: 0.9816
 1920/13806 [===>..........................] - ETA: 48s - loss: 0.1503 - categorical_accuracy: 0.9807
 2048/13806 [===>..........................] - ETA: 47s - loss: 0.1494 - categorical_accuracy: 0.9805
 2176/13806 [===>..........................] - ETA: 47s - loss: 0.1467 - categorical_accuracy: 0.9812
 2304/13806 [====>.........................] - ETA: 46s - loss: 0.1455 - categorical_accuracy: 0.9818
 2432/13806 [====>.........................] - ETA: 46s - loss: 0.1466 - categorical_accuracy: 0.9819
 2560/13806 [====>.........................] - ETA: 45s - loss: 0.1485 - categorical_accuracy: 0.9812
 2688/13806 [====>.........................] - ETA: 45s - loss: 0.1467 - categorical_accuracy: 0.9814
 2816/13806 [=====>........................] - ETA: 44s - loss: 0.1467 - categorical_accuracy: 0.9815
 2944/13806 [=====>........................] - ETA: 44s - loss: 0.1446 - categorical_accuracy: 0.9820
 3072/13806 [=====>........................] - ETA: 43s - loss: 0.1460 - categorical_accuracy: 0.9814
 3200/13806 [=====>........................] - ETA: 43s - loss: 0.1451 - categorical_accuracy: 0.9816
 3328/13806 [======>.......................] - ETA: 42s - loss: 0.1447 - categorical_accuracy: 0.9820
 3456/13806 [======>.......................] - ETA: 42s - loss: 0.1485 - categorical_accuracy: 0.9812
 3584/13806 [======>.......................] - ETA: 41s - loss: 0.1492 - categorical_accuracy: 0.9816
 3712/13806 [=======>......................] - ETA: 40s - loss: 0.1481 - categorical_accuracy: 0.9814
 3840/13806 [=======>......................] - ETA: 40s - loss: 0.1466 - categorical_accuracy: 0.9818
 3968/13806 [=======>......................] - ETA: 39s - loss: 0.1461 - categorical_accuracy: 0.9814
 4096/13806 [=======>......................] - ETA: 39s - loss: 0.1458 - categorical_accuracy: 0.9812
 4224/13806 [========>.....................] - ETA: 38s - loss: 0.1450 - categorical_accuracy: 0.9813
 4352/13806 [========>.....................] - ETA: 38s - loss: 0.1453 - categorical_accuracy: 0.9812
 4480/13806 [========>.....................] - ETA: 37s - loss: 0.1441 - categorical_accuracy: 0.9815
 4608/13806 [=========>....................] - ETA: 37s - loss: 0.1437 - categorical_accuracy: 0.9818
 4736/13806 [=========>....................] - ETA: 36s - loss: 0.1452 - categorical_accuracy: 0.9812
 4864/13806 [=========>....................] - ETA: 36s - loss: 0.1445 - categorical_accuracy: 0.9815
 4992/13806 [=========>....................] - ETA: 35s - loss: 0.1457 - categorical_accuracy: 0.9812
 5120/13806 [==========>...................] - ETA: 35s - loss: 0.1447 - categorical_accuracy: 0.9814
 5248/13806 [==========>...................] - ETA: 34s - loss: 0.1447 - categorical_accuracy: 0.9815
 5376/13806 [==========>...................] - ETA: 34s - loss: 0.1457 - categorical_accuracy: 0.9810
 5504/13806 [==========>...................] - ETA: 33s - loss: 0.1453 - categorical_accuracy: 0.9809
 5632/13806 [===========>..................] - ETA: 33s - loss: 0.1444 - categorical_accuracy: 0.9810
 5760/13806 [===========>..................] - ETA: 32s - loss: 0.1443 - categorical_accuracy: 0.9806
 5888/13806 [===========>..................] - ETA: 32s - loss: 0.1436 - categorical_accuracy: 0.9806
 6016/13806 [============>.................] - ETA: 31s - loss: 0.1445 - categorical_accuracy: 0.9804
 6144/13806 [============>.................] - ETA: 31s - loss: 0.1472 - categorical_accuracy: 0.9803
 6272/13806 [============>.................] - ETA: 30s - loss: 0.1463 - categorical_accuracy: 0.9807
 6400/13806 [============>.................] - ETA: 29s - loss: 0.1457 - categorical_accuracy: 0.9809
 6528/13806 [=============>................] - ETA: 29s - loss: 0.1465 - categorical_accuracy: 0.9807
 6656/13806 [=============>................] - ETA: 28s - loss: 0.1471 - categorical_accuracy: 0.9806
 6784/13806 [=============>................] - ETA: 28s - loss: 0.1468 - categorical_accuracy: 0.9808
 6912/13806 [==============>...............] - ETA: 27s - loss: 0.1461 - categorical_accuracy: 0.9809
 7040/13806 [==============>...............] - ETA: 27s - loss: 0.1456 - categorical_accuracy: 0.9810
 7168/13806 [==============>...............] - ETA: 26s - loss: 0.1452 - categorical_accuracy: 0.9809
 7296/13806 [==============>...............] - ETA: 26s - loss: 0.1454 - categorical_accuracy: 0.9809
 7424/13806 [===============>..............] - ETA: 25s - loss: 0.1449 - categorical_accuracy: 0.9810
 7552/13806 [===============>..............] - ETA: 25s - loss: 0.1448 - categorical_accuracy: 0.9811
 7680/13806 [===============>..............] - ETA: 24s - loss: 0.1445 - categorical_accuracy: 0.9810
 7808/13806 [===============>..............] - ETA: 24s - loss: 0.1450 - categorical_accuracy: 0.9809
 7936/13806 [================>.............] - ETA: 23s - loss: 0.1463 - categorical_accuracy: 0.9807
 8064/13806 [================>.............] - ETA: 23s - loss: 0.1457 - categorical_accuracy: 0.9809
 8192/13806 [================>.............] - ETA: 22s - loss: 0.1457 - categorical_accuracy: 0.9808
 8320/13806 [=================>............] - ETA: 22s - loss: 0.1459 - categorical_accuracy: 0.9806
 8448/13806 [=================>............] - ETA: 21s - loss: 0.1472 - categorical_accuracy: 0.9802
 8576/13806 [=================>............] - ETA: 21s - loss: 0.1465 - categorical_accuracy: 0.9804
 8704/13806 [=================>............] - ETA: 20s - loss: 0.1458 - categorical_accuracy: 0.9806
 8832/13806 [==================>...........] - ETA: 20s - loss: 0.1453 - categorical_accuracy: 0.9808
 8960/13806 [==================>...........] - ETA: 19s - loss: 0.1455 - categorical_accuracy: 0.9809
 9088/13806 [==================>...........] - ETA: 19s - loss: 0.1462 - categorical_accuracy: 0.9809
 9216/13806 [===================>..........] - ETA: 18s - loss: 0.1464 - categorical_accuracy: 0.9808
 9344/13806 [===================>..........] - ETA: 18s - loss: 0.1459 - categorical_accuracy: 0.9810
 9472/13806 [===================>..........] - ETA: 17s - loss: 0.1451 - categorical_accuracy: 0.9812
 9600/13806 [===================>..........] - ETA: 17s - loss: 0.1449 - categorical_accuracy: 0.9814
 9728/13806 [====================>.........] - ETA: 16s - loss: 0.1447 - categorical_accuracy: 0.9814
 9856/13806 [====================>.........] - ETA: 16s - loss: 0.1443 - categorical_accuracy: 0.9816
 9984/13806 [====================>.........] - ETA: 15s - loss: 0.1447 - categorical_accuracy: 0.9815
10112/13806 [====================>.........] - ETA: 15s - loss: 0.1443 - categorical_accuracy: 0.9815
10240/13806 [=====================>........] - ETA: 14s - loss: 0.1443 - categorical_accuracy: 0.9816
10368/13806 [=====================>........] - ETA: 13s - loss: 0.1440 - categorical_accuracy: 0.9816
10496/13806 [=====================>........] - ETA: 13s - loss: 0.1435 - categorical_accuracy: 0.9817
10624/13806 [======================>.......] - ETA: 12s - loss: 0.1435 - categorical_accuracy: 0.9817
10752/13806 [======================>.......] - ETA: 12s - loss: 0.1435 - categorical_accuracy: 0.9816
10880/13806 [======================>.......] - ETA: 11s - loss: 0.1439 - categorical_accuracy: 0.9814
11008/13806 [======================>.......] - ETA: 11s - loss: 0.1433 - categorical_accuracy: 0.9816
11136/13806 [=======================>......] - ETA: 10s - loss: 0.1440 - categorical_accuracy: 0.9816
11264/13806 [=======================>......] - ETA: 10s - loss: 0.1439 - categorical_accuracy: 0.9816
11392/13806 [=======================>......] - ETA: 9s - loss: 0.1437 - categorical_accuracy: 0.9817 
11520/13806 [========================>.....] - ETA: 9s - loss: 0.1443 - categorical_accuracy: 0.9815
11648/13806 [========================>.....] - ETA: 8s - loss: 0.1436 - categorical_accuracy: 0.9817
11776/13806 [========================>.....] - ETA: 8s - loss: 0.1433 - categorical_accuracy: 0.9816
11904/13806 [========================>.....] - ETA: 7s - loss: 0.1427 - categorical_accuracy: 0.9818
12032/13806 [=========================>....] - ETA: 7s - loss: 0.1428 - categorical_accuracy: 0.9818
12160/13806 [=========================>....] - ETA: 6s - loss: 0.1427 - categorical_accuracy: 0.9817
12288/13806 [=========================>....] - ETA: 6s - loss: 0.1431 - categorical_accuracy: 0.9818
12416/13806 [=========================>....] - ETA: 5s - loss: 0.1428 - categorical_accuracy: 0.9816
12544/13806 [==========================>...] - ETA: 5s - loss: 0.1424 - categorical_accuracy: 0.9817
12672/13806 [==========================>...] - ETA: 4s - loss: 0.1429 - categorical_accuracy: 0.9817
12800/13806 [==========================>...] - ETA: 4s - loss: 0.1427 - categorical_accuracy: 0.9818
12928/13806 [===========================>..] - ETA: 3s - loss: 0.1422 - categorical_accuracy: 0.9820
13056/13806 [===========================>..] - ETA: 3s - loss: 0.1422 - categorical_accuracy: 0.9818
13184/13806 [===========================>..] - ETA: 2s - loss: 0.1420 - categorical_accuracy: 0.9819
13312/13806 [===========================>..] - ETA: 2s - loss: 0.1419 - categorical_accuracy: 0.9820
13440/13806 [============================>.] - ETA: 1s - loss: 0.1418 - categorical_accuracy: 0.9820
13568/13806 [============================>.] - ETA: 0s - loss: 0.1417 - categorical_accuracy: 0.9820
13696/13806 [============================>.] - ETA: 0s - loss: 0.1413 - categorical_accuracy: 0.9820
13806/13806 [==============================] - 58s 4ms/step - loss: 0.1414 - categorical_accuracy: 0.9820 - val_loss: 1.5354 - val_categorical_accuracy: 0.5308

Epoch 00004: val_categorical_accuracy did not improve
Epoch 5/15

  128/13806 [..............................] - ETA: 55s - loss: 0.2064 - categorical_accuracy: 0.9531
  256/13806 [..............................] - ETA: 54s - loss: 0.1541 - categorical_accuracy: 0.9727
  384/13806 [..............................] - ETA: 54s - loss: 0.1476 - categorical_accuracy: 0.9714
  512/13806 [>.............................] - ETA: 53s - loss: 0.1385 - categorical_accuracy: 0.9746
  640/13806 [>.............................] - ETA: 53s - loss: 0.1543 - categorical_accuracy: 0.9734
  768/13806 [>.............................] - ETA: 53s - loss: 0.1510 - categorical_accuracy: 0.9753
  896/13806 [>.............................] - ETA: 52s - loss: 0.1478 - categorical_accuracy: 0.9754
 1024/13806 [=>............................] - ETA: 52s - loss: 0.1427 - categorical_accuracy: 0.9766
 1152/13806 [=>............................] - ETA: 51s - loss: 0.1481 - categorical_accuracy: 0.9757
 1280/13806 [=>............................] - ETA: 51s - loss: 0.1417 - categorical_accuracy: 0.9781
 1408/13806 [==>...........................] - ETA: 50s - loss: 0.1373 - categorical_accuracy: 0.9794
 1536/13806 [==>...........................] - ETA: 49s - loss: 0.1367 - categorical_accuracy: 0.9798
 1664/13806 [==>...........................] - ETA: 49s - loss: 0.1398 - categorical_accuracy: 0.9778
 1792/13806 [==>...........................] - ETA: 48s - loss: 0.1364 - categorical_accuracy: 0.9788
 1920/13806 [===>..........................] - ETA: 48s - loss: 0.1367 - categorical_accuracy: 0.9786
 2048/13806 [===>..........................] - ETA: 48s - loss: 0.1359 - categorical_accuracy: 0.9790
 2176/13806 [===>..........................] - ETA: 47s - loss: 0.1326 - categorical_accuracy: 0.9802
 2304/13806 [====>.........................] - ETA: 46s - loss: 0.1309 - categorical_accuracy: 0.9805
 2432/13806 [====>.........................] - ETA: 46s - loss: 0.1300 - categorical_accuracy: 0.9807
 2560/13806 [====>.........................] - ETA: 45s - loss: 0.1298 - categorical_accuracy: 0.9809
 2688/13806 [====>.........................] - ETA: 45s - loss: 0.1274 - categorical_accuracy: 0.9814
 2816/13806 [=====>........................] - ETA: 44s - loss: 0.1269 - categorical_accuracy: 0.9819
 2944/13806 [=====>........................] - ETA: 44s - loss: 0.1301 - categorical_accuracy: 0.9813
 3072/13806 [=====>........................] - ETA: 43s - loss: 0.1299 - categorical_accuracy: 0.9814
 3200/13806 [=====>........................] - ETA: 43s - loss: 0.1327 - categorical_accuracy: 0.9806
 3328/13806 [======>.......................] - ETA: 42s - loss: 0.1331 - categorical_accuracy: 0.9805
 3456/13806 [======>.......................] - ETA: 42s - loss: 0.1331 - categorical_accuracy: 0.9800
 3584/13806 [======>.......................] - ETA: 41s - loss: 0.1329 - categorical_accuracy: 0.9796
 3712/13806 [=======>......................] - ETA: 41s - loss: 0.1316 - categorical_accuracy: 0.9803
 3840/13806 [=======>......................] - ETA: 40s - loss: 0.1330 - categorical_accuracy: 0.9802
 3968/13806 [=======>......................] - ETA: 40s - loss: 0.1339 - categorical_accuracy: 0.9801
 4096/13806 [=======>......................] - ETA: 39s - loss: 0.1360 - categorical_accuracy: 0.9797
 4224/13806 [========>.....................] - ETA: 39s - loss: 0.1350 - categorical_accuracy: 0.9804
 4352/13806 [========>.....................] - ETA: 38s - loss: 0.1367 - categorical_accuracy: 0.9802
 4480/13806 [========>.....................] - ETA: 38s - loss: 0.1396 - categorical_accuracy: 0.9797
 4608/13806 [=========>....................] - ETA: 37s - loss: 0.1384 - categorical_accuracy: 0.9803
 4736/13806 [=========>....................] - ETA: 37s - loss: 0.1376 - categorical_accuracy: 0.9804
 4864/13806 [=========>....................] - ETA: 36s - loss: 0.1368 - categorical_accuracy: 0.9807
 4992/13806 [=========>....................] - ETA: 35s - loss: 0.1374 - categorical_accuracy: 0.9806
 5120/13806 [==========>...................] - ETA: 35s - loss: 0.1363 - categorical_accuracy: 0.9809
 5248/13806 [==========>...................] - ETA: 34s - loss: 0.1371 - categorical_accuracy: 0.9809
 5376/13806 [==========>...................] - ETA: 34s - loss: 0.1373 - categorical_accuracy: 0.9810
 5504/13806 [==========>...................] - ETA: 33s - loss: 0.1383 - categorical_accuracy: 0.9811
 5632/13806 [===========>..................] - ETA: 33s - loss: 0.1396 - categorical_accuracy: 0.9808
 5760/13806 [===========>..................] - ETA: 32s - loss: 0.1390 - categorical_accuracy: 0.9809
 5888/13806 [===========>..................] - ETA: 32s - loss: 0.1387 - categorical_accuracy: 0.9806
 6016/13806 [============>.................] - ETA: 31s - loss: 0.1386 - categorical_accuracy: 0.9806
 6144/13806 [============>.................] - ETA: 31s - loss: 0.1389 - categorical_accuracy: 0.9805
 6272/13806 [============>.................] - ETA: 30s - loss: 0.1378 - categorical_accuracy: 0.9809
 6400/13806 [============>.................] - ETA: 30s - loss: 0.1380 - categorical_accuracy: 0.9808
 6528/13806 [=============>................] - ETA: 29s - loss: 0.1379 - categorical_accuracy: 0.9809
 6656/13806 [=============>................] - ETA: 29s - loss: 0.1379 - categorical_accuracy: 0.9809
 6784/13806 [=============>................] - ETA: 28s - loss: 0.1370 - categorical_accuracy: 0.9811
 6912/13806 [==============>...............] - ETA: 28s - loss: 0.1378 - categorical_accuracy: 0.9806
 7040/13806 [==============>...............] - ETA: 27s - loss: 0.1396 - categorical_accuracy: 0.9805
 7168/13806 [==============>...............] - ETA: 27s - loss: 0.1393 - categorical_accuracy: 0.9806
 7296/13806 [==============>...............] - ETA: 26s - loss: 0.1396 - categorical_accuracy: 0.9805
 7424/13806 [===============>..............] - ETA: 26s - loss: 0.1396 - categorical_accuracy: 0.9805
 7552/13806 [===============>..............] - ETA: 25s - loss: 0.1394 - categorical_accuracy: 0.9805
 7680/13806 [===============>..............] - ETA: 25s - loss: 0.1400 - categorical_accuracy: 0.9802
 7808/13806 [===============>..............] - ETA: 24s - loss: 0.1405 - categorical_accuracy: 0.9800
 7936/13806 [================>.............] - ETA: 23s - loss: 0.1406 - categorical_accuracy: 0.9801
 8064/13806 [================>.............] - ETA: 23s - loss: 0.1404 - categorical_accuracy: 0.9799
 8192/13806 [================>.............] - ETA: 22s - loss: 0.1417 - categorical_accuracy: 0.9799
 8320/13806 [=================>............] - ETA: 22s - loss: 0.1413 - categorical_accuracy: 0.9798
 8448/13806 [=================>............] - ETA: 21s - loss: 0.1406 - categorical_accuracy: 0.9800
 8576/13806 [=================>............] - ETA: 21s - loss: 0.1404 - categorical_accuracy: 0.9801
 8704/13806 [=================>............] - ETA: 20s - loss: 0.1417 - categorical_accuracy: 0.9798
 8832/13806 [==================>...........] - ETA: 20s - loss: 0.1418 - categorical_accuracy: 0.9798
 8960/13806 [==================>...........] - ETA: 19s - loss: 0.1413 - categorical_accuracy: 0.9800
 9088/13806 [==================>...........] - ETA: 19s - loss: 0.1411 - categorical_accuracy: 0.9801
 9216/13806 [===================>..........] - ETA: 18s - loss: 0.1430 - categorical_accuracy: 0.9798
 9344/13806 [===================>..........] - ETA: 18s - loss: 0.1423 - categorical_accuracy: 0.9800
 9472/13806 [===================>..........] - ETA: 17s - loss: 0.1422 - categorical_accuracy: 0.9802
 9600/13806 [===================>..........] - ETA: 17s - loss: 0.1416 - categorical_accuracy: 0.9803
 9728/13806 [====================>.........] - ETA: 16s - loss: 0.1410 - categorical_accuracy: 0.9804
 9856/13806 [====================>.........] - ETA: 16s - loss: 0.1418 - categorical_accuracy: 0.9801
 9984/13806 [====================>.........] - ETA: 15s - loss: 0.1420 - categorical_accuracy: 0.9800
10112/13806 [====================>.........] - ETA: 15s - loss: 0.1413 - categorical_accuracy: 0.9801
10240/13806 [=====================>........] - ETA: 14s - loss: 0.1408 - categorical_accuracy: 0.9802
10368/13806 [=====================>........] - ETA: 14s - loss: 0.1408 - categorical_accuracy: 0.9801
10496/13806 [=====================>........] - ETA: 13s - loss: 0.1408 - categorical_accuracy: 0.9802
10624/13806 [======================>.......] - ETA: 13s - loss: 0.1408 - categorical_accuracy: 0.9801
10752/13806 [======================>.......] - ETA: 12s - loss: 0.1404 - categorical_accuracy: 0.9803
10880/13806 [======================>.......] - ETA: 11s - loss: 0.1412 - categorical_accuracy: 0.9803
11008/13806 [======================>.......] - ETA: 11s - loss: 0.1413 - categorical_accuracy: 0.9803
11136/13806 [=======================>......] - ETA: 10s - loss: 0.1413 - categorical_accuracy: 0.9803
11264/13806 [=======================>......] - ETA: 10s - loss: 0.1408 - categorical_accuracy: 0.9805
11392/13806 [=======================>......] - ETA: 9s - loss: 0.1411 - categorical_accuracy: 0.9804 
11520/13806 [========================>.....] - ETA: 9s - loss: 0.1409 - categorical_accuracy: 0.9804
11648/13806 [========================>.....] - ETA: 8s - loss: 0.1405 - categorical_accuracy: 0.9805
11776/13806 [========================>.....] - ETA: 8s - loss: 0.1413 - categorical_accuracy: 0.9803
11904/13806 [========================>.....] - ETA: 7s - loss: 0.1407 - categorical_accuracy: 0.9804
12032/13806 [=========================>....] - ETA: 7s - loss: 0.1406 - categorical_accuracy: 0.9804
12160/13806 [=========================>....] - ETA: 6s - loss: 0.1403 - categorical_accuracy: 0.9804
12288/13806 [=========================>....] - ETA: 6s - loss: 0.1406 - categorical_accuracy: 0.9802
12416/13806 [=========================>....] - ETA: 5s - loss: 0.1411 - categorical_accuracy: 0.9802
12544/13806 [==========================>...] - ETA: 5s - loss: 0.1406 - categorical_accuracy: 0.9803
12672/13806 [==========================>...] - ETA: 4s - loss: 0.1403 - categorical_accuracy: 0.9803
12800/13806 [==========================>...] - ETA: 4s - loss: 0.1399 - categorical_accuracy: 0.9805
12928/13806 [===========================>..] - ETA: 3s - loss: 0.1394 - categorical_accuracy: 0.9807
13056/13806 [===========================>..] - ETA: 3s - loss: 0.1390 - categorical_accuracy: 0.9807
13184/13806 [===========================>..] - ETA: 2s - loss: 0.1386 - categorical_accuracy: 0.9807
13312/13806 [===========================>..] - ETA: 2s - loss: 0.1381 - categorical_accuracy: 0.9808
13440/13806 [============================>.] - ETA: 1s - loss: 0.1375 - categorical_accuracy: 0.9810
13568/13806 [============================>.] - ETA: 0s - loss: 0.1372 - categorical_accuracy: 0.9811
13696/13806 [============================>.] - ETA: 0s - loss: 0.1372 - categorical_accuracy: 0.9810
13806/13806 [==============================] - 58s 4ms/step - loss: 0.1373 - categorical_accuracy: 0.9810 - val_loss: 1.5276 - val_categorical_accuracy: 0.5275

Epoch 00005: val_categorical_accuracy did not improve
Epoch 6/15

  128/13806 [..............................] - ETA: 54s - loss: 0.0978 - categorical_accuracy: 0.9844
  256/13806 [..............................] - ETA: 54s - loss: 0.1110 - categorical_accuracy: 0.9805
  384/13806 [..............................] - ETA: 54s - loss: 0.1147 - categorical_accuracy: 0.9792
  512/13806 [>.............................] - ETA: 53s - loss: 0.1107 - categorical_accuracy: 0.9824
  640/13806 [>.............................] - ETA: 53s - loss: 0.1216 - categorical_accuracy: 0.9828
  768/13806 [>.............................] - ETA: 53s - loss: 0.1159 - categorical_accuracy: 0.9857
  896/13806 [>.............................] - ETA: 52s - loss: 0.1172 - categorical_accuracy: 0.9855
 1024/13806 [=>............................] - ETA: 51s - loss: 0.1233 - categorical_accuracy: 0.9834
 1152/13806 [=>............................] - ETA: 51s - loss: 0.1289 - categorical_accuracy: 0.9826
 1280/13806 [=>............................] - ETA: 50s - loss: 0.1338 - categorical_accuracy: 0.9820
 1408/13806 [==>...........................] - ETA: 50s - loss: 0.1287 - categorical_accuracy: 0.9837
 1536/13806 [==>...........................] - ETA: 49s - loss: 0.1274 - categorical_accuracy: 0.9824
 1664/13806 [==>...........................] - ETA: 49s - loss: 0.1257 - categorical_accuracy: 0.9838
 1792/13806 [==>...........................] - ETA: 48s - loss: 0.1286 - categorical_accuracy: 0.9833
 1920/13806 [===>..........................] - ETA: 48s - loss: 0.1303 - categorical_accuracy: 0.9828
 2048/13806 [===>..........................] - ETA: 47s - loss: 0.1354 - categorical_accuracy: 0.9810
 2176/13806 [===>..........................] - ETA: 47s - loss: 0.1361 - categorical_accuracy: 0.9812
 2304/13806 [====>.........................] - ETA: 46s - loss: 0.1351 - categorical_accuracy: 0.9818
 2432/13806 [====>.........................] - ETA: 46s - loss: 0.1358 - categorical_accuracy: 0.9819
 2560/13806 [====>.........................] - ETA: 45s - loss: 0.1336 - categorical_accuracy: 0.9824
 2688/13806 [====>.........................] - ETA: 45s - loss: 0.1357 - categorical_accuracy: 0.9821
 2816/13806 [=====>........................] - ETA: 44s - loss: 0.1334 - categorical_accuracy: 0.9830
 2944/13806 [=====>........................] - ETA: 44s - loss: 0.1347 - categorical_accuracy: 0.9827
 3072/13806 [=====>........................] - ETA: 43s - loss: 0.1323 - categorical_accuracy: 0.9834
 3200/13806 [=====>........................] - ETA: 43s - loss: 0.1305 - categorical_accuracy: 0.9841
 3328/13806 [======>.......................] - ETA: 42s - loss: 0.1304 - categorical_accuracy: 0.9838
 3456/13806 [======>.......................] - ETA: 42s - loss: 0.1291 - categorical_accuracy: 0.9838
 3584/13806 [======>.......................] - ETA: 41s - loss: 0.1289 - categorical_accuracy: 0.9838
 3712/13806 [=======>......................] - ETA: 41s - loss: 0.1283 - categorical_accuracy: 0.9841
 3840/13806 [=======>......................] - ETA: 40s - loss: 0.1267 - categorical_accuracy: 0.9846
 3968/13806 [=======>......................] - ETA: 40s - loss: 0.1255 - categorical_accuracy: 0.9849
 4096/13806 [=======>......................] - ETA: 39s - loss: 0.1284 - categorical_accuracy: 0.9849
 4224/13806 [========>.....................] - ETA: 39s - loss: 0.1276 - categorical_accuracy: 0.9848
 4352/13806 [========>.....................] - ETA: 38s - loss: 0.1295 - categorical_accuracy: 0.9841
 4480/13806 [========>.....................] - ETA: 37s - loss: 0.1304 - categorical_accuracy: 0.9839
 4608/13806 [=========>....................] - ETA: 37s - loss: 0.1295 - categorical_accuracy: 0.9842
 4736/13806 [=========>....................] - ETA: 36s - loss: 0.1290 - categorical_accuracy: 0.9840
 4864/13806 [=========>....................] - ETA: 36s - loss: 0.1283 - categorical_accuracy: 0.9840
 4992/13806 [=========>....................] - ETA: 35s - loss: 0.1293 - categorical_accuracy: 0.9842
 5120/13806 [==========>...................] - ETA: 35s - loss: 0.1300 - categorical_accuracy: 0.9838
 5248/13806 [==========>...................] - ETA: 34s - loss: 0.1306 - categorical_accuracy: 0.9836
 5376/13806 [==========>...................] - ETA: 34s - loss: 0.1326 - categorical_accuracy: 0.9834
 5504/13806 [==========>...................] - ETA: 33s - loss: 0.1338 - categorical_accuracy: 0.9833
 5632/13806 [===========>..................] - ETA: 33s - loss: 0.1330 - categorical_accuracy: 0.9835
 5760/13806 [===========>..................] - ETA: 32s - loss: 0.1328 - categorical_accuracy: 0.9835
 5888/13806 [===========>..................] - ETA: 32s - loss: 0.1341 - categorical_accuracy: 0.9832
 6016/13806 [============>.................] - ETA: 31s - loss: 0.1330 - categorical_accuracy: 0.9834
 6144/13806 [============>.................] - ETA: 31s - loss: 0.1321 - categorical_accuracy: 0.9837
 6272/13806 [============>.................] - ETA: 30s - loss: 0.1341 - categorical_accuracy: 0.9831
 6400/13806 [============>.................] - ETA: 30s - loss: 0.1331 - categorical_accuracy: 0.9834
 6528/13806 [=============>................] - ETA: 29s - loss: 0.1327 - categorical_accuracy: 0.9836
 6656/13806 [=============>................] - ETA: 29s - loss: 0.1327 - categorical_accuracy: 0.9838
 6784/13806 [=============>................] - ETA: 28s - loss: 0.1324 - categorical_accuracy: 0.9836
 6912/13806 [==============>...............] - ETA: 28s - loss: 0.1320 - categorical_accuracy: 0.9838
 7040/13806 [==============>...............] - ETA: 27s - loss: 0.1312 - categorical_accuracy: 0.9841
 7168/13806 [==============>...............] - ETA: 27s - loss: 0.1320 - categorical_accuracy: 0.9840
 7296/13806 [==============>...............] - ETA: 26s - loss: 0.1322 - categorical_accuracy: 0.9840
 7424/13806 [===============>..............] - ETA: 26s - loss: 0.1346 - categorical_accuracy: 0.9837
 7552/13806 [===============>..............] - ETA: 25s - loss: 0.1350 - categorical_accuracy: 0.9836
 7680/13806 [===============>..............] - ETA: 24s - loss: 0.1348 - categorical_accuracy: 0.9836
 7808/13806 [===============>..............] - ETA: 24s - loss: 0.1340 - categorical_accuracy: 0.9837
 7936/13806 [================>.............] - ETA: 23s - loss: 0.1334 - categorical_accuracy: 0.9839
 8064/13806 [================>.............] - ETA: 23s - loss: 0.1335 - categorical_accuracy: 0.9838
 8192/13806 [================>.............] - ETA: 22s - loss: 0.1341 - categorical_accuracy: 0.9833
 8320/13806 [=================>............] - ETA: 22s - loss: 0.1338 - categorical_accuracy: 0.9834
 8448/13806 [=================>............] - ETA: 21s - loss: 0.1341 - categorical_accuracy: 0.9833
 8576/13806 [=================>............] - ETA: 21s - loss: 0.1336 - categorical_accuracy: 0.9833
 8704/13806 [=================>............] - ETA: 20s - loss: 0.1342 - categorical_accuracy: 0.9832
 8832/13806 [==================>...........] - ETA: 20s - loss: 0.1336 - categorical_accuracy: 0.9834
 8960/13806 [==================>...........] - ETA: 19s - loss: 0.1336 - categorical_accuracy: 0.9831
 9088/13806 [==================>...........] - ETA: 19s - loss: 0.1339 - categorical_accuracy: 0.9832
 9216/13806 [===================>..........] - ETA: 18s - loss: 0.1340 - categorical_accuracy: 0.9831
 9344/13806 [===================>..........] - ETA: 18s - loss: 0.1332 - categorical_accuracy: 0.9833
 9472/13806 [===================>..........] - ETA: 17s - loss: 0.1326 - categorical_accuracy: 0.9834
 9600/13806 [===================>..........] - ETA: 17s - loss: 0.1323 - categorical_accuracy: 0.9834
 9728/13806 [====================>.........] - ETA: 16s - loss: 0.1337 - categorical_accuracy: 0.9831
 9856/13806 [====================>.........] - ETA: 16s - loss: 0.1331 - categorical_accuracy: 0.9834
 9984/13806 [====================>.........] - ETA: 15s - loss: 0.1328 - categorical_accuracy: 0.9834
10112/13806 [====================>.........] - ETA: 15s - loss: 0.1339 - categorical_accuracy: 0.9828
10240/13806 [=====================>........] - ETA: 14s - loss: 0.1340 - categorical_accuracy: 0.9827
10368/13806 [=====================>........] - ETA: 14s - loss: 0.1342 - categorical_accuracy: 0.9826
10496/13806 [=====================>........] - ETA: 13s - loss: 0.1342 - categorical_accuracy: 0.9827
10624/13806 [======================>.......] - ETA: 12s - loss: 0.1346 - categorical_accuracy: 0.9825
10752/13806 [======================>.......] - ETA: 12s - loss: 0.1340 - categorical_accuracy: 0.9827
10880/13806 [======================>.......] - ETA: 11s - loss: 0.1334 - categorical_accuracy: 0.9829
11008/13806 [======================>.......] - ETA: 11s - loss: 0.1334 - categorical_accuracy: 0.9828
11136/13806 [=======================>......] - ETA: 10s - loss: 0.1330 - categorical_accuracy: 0.9829
11264/13806 [=======================>......] - ETA: 10s - loss: 0.1329 - categorical_accuracy: 0.9830
11392/13806 [=======================>......] - ETA: 9s - loss: 0.1338 - categorical_accuracy: 0.9829 
11520/13806 [========================>.....] - ETA: 9s - loss: 0.1337 - categorical_accuracy: 0.9828
11648/13806 [========================>.....] - ETA: 8s - loss: 0.1347 - categorical_accuracy: 0.9827
11776/13806 [========================>.....] - ETA: 8s - loss: 0.1349 - categorical_accuracy: 0.9824
11904/13806 [========================>.....] - ETA: 7s - loss: 0.1346 - categorical_accuracy: 0.9824
12032/13806 [=========================>....] - ETA: 7s - loss: 0.1347 - categorical_accuracy: 0.9822
12160/13806 [=========================>....] - ETA: 6s - loss: 0.1344 - categorical_accuracy: 0.9822
12288/13806 [=========================>....] - ETA: 6s - loss: 0.1352 - categorical_accuracy: 0.9821
12416/13806 [=========================>....] - ETA: 5s - loss: 0.1355 - categorical_accuracy: 0.9820
12544/13806 [==========================>...] - ETA: 5s - loss: 0.1356 - categorical_accuracy: 0.9819
12672/13806 [==========================>...] - ETA: 4s - loss: 0.1355 - categorical_accuracy: 0.9820
12800/13806 [==========================>...] - ETA: 4s - loss: 0.1361 - categorical_accuracy: 0.9820
12928/13806 [===========================>..] - ETA: 3s - loss: 0.1355 - categorical_accuracy: 0.9821
13056/13806 [===========================>..] - ETA: 3s - loss: 0.1353 - categorical_accuracy: 0.9821
13184/13806 [===========================>..] - ETA: 2s - loss: 0.1349 - categorical_accuracy: 0.9821
13312/13806 [===========================>..] - ETA: 2s - loss: 0.1349 - categorical_accuracy: 0.9820
13440/13806 [============================>.] - ETA: 1s - loss: 0.1345 - categorical_accuracy: 0.9822
13568/13806 [============================>.] - ETA: 0s - loss: 0.1344 - categorical_accuracy: 0.9823
13696/13806 [============================>.] - ETA: 0s - loss: 0.1341 - categorical_accuracy: 0.9824
13806/13806 [==============================] - 58s 4ms/step - loss: 0.1337 - categorical_accuracy: 0.9825 - val_loss: 1.5639 - val_categorical_accuracy: 0.5308

Epoch 00006: val_categorical_accuracy did not improve
Epoch 7/15

  128/13806 [..............................] - ETA: 56s - loss: 0.1416 - categorical_accuracy: 0.9844
  256/13806 [..............................] - ETA: 55s - loss: 0.1698 - categorical_accuracy: 0.9805
  384/13806 [..............................] - ETA: 54s - loss: 0.1715 - categorical_accuracy: 0.9792
  512/13806 [>.............................] - ETA: 54s - loss: 0.1648 - categorical_accuracy: 0.9727
  640/13806 [>.............................] - ETA: 53s - loss: 0.1538 - categorical_accuracy: 0.9750
  768/13806 [>.............................] - ETA: 53s - loss: 0.1433 - categorical_accuracy: 0.9766
  896/13806 [>.............................] - ETA: 52s - loss: 0.1392 - categorical_accuracy: 0.9788
 1024/13806 [=>............................] - ETA: 52s - loss: 0.1455 - categorical_accuracy: 0.9775
 1152/13806 [=>............................] - ETA: 51s - loss: 0.1445 - categorical_accuracy: 0.9783
 1280/13806 [=>............................] - ETA: 50s - loss: 0.1405 - categorical_accuracy: 0.9781
 1408/13806 [==>...........................] - ETA: 50s - loss: 0.1407 - categorical_accuracy: 0.9780
 1536/13806 [==>...........................] - ETA: 49s - loss: 0.1443 - categorical_accuracy: 0.9759
 1664/13806 [==>...........................] - ETA: 49s - loss: 0.1430 - categorical_accuracy: 0.9766
 1792/13806 [==>...........................] - ETA: 49s - loss: 0.1426 - categorical_accuracy: 0.9771
 1920/13806 [===>..........................] - ETA: 48s - loss: 0.1429 - categorical_accuracy: 0.9760
 2048/13806 [===>..........................] - ETA: 48s - loss: 0.1416 - categorical_accuracy: 0.9761
 2176/13806 [===>..........................] - ETA: 47s - loss: 0.1379 - categorical_accuracy: 0.9775
 2304/13806 [====>.........................] - ETA: 47s - loss: 0.1351 - categorical_accuracy: 0.9783
 2432/13806 [====>.........................] - ETA: 46s - loss: 0.1386 - categorical_accuracy: 0.9770
 2560/13806 [====>.........................] - ETA: 46s - loss: 0.1393 - categorical_accuracy: 0.9773
 2688/13806 [====>.........................] - ETA: 45s - loss: 0.1392 - categorical_accuracy: 0.9769
 2816/13806 [=====>........................] - ETA: 45s - loss: 0.1391 - categorical_accuracy: 0.9769
 2944/13806 [=====>........................] - ETA: 44s - loss: 0.1380 - categorical_accuracy: 0.9776
 3072/13806 [=====>........................] - ETA: 44s - loss: 0.1374 - categorical_accuracy: 0.9775
 3200/13806 [=====>........................] - ETA: 43s - loss: 0.1397 - categorical_accuracy: 0.9772
 3328/13806 [======>.......................] - ETA: 42s - loss: 0.1444 - categorical_accuracy: 0.9763
 3456/13806 [======>.......................] - ETA: 42s - loss: 0.1435 - categorical_accuracy: 0.9766
 3584/13806 [======>.......................] - ETA: 41s - loss: 0.1412 - categorical_accuracy: 0.9774
 3712/13806 [=======>......................] - ETA: 41s - loss: 0.1405 - categorical_accuracy: 0.9774
 3840/13806 [=======>......................] - ETA: 40s - loss: 0.1407 - categorical_accuracy: 0.9773
 3968/13806 [=======>......................] - ETA: 40s - loss: 0.1409 - categorical_accuracy: 0.9776
 4096/13806 [=======>......................] - ETA: 39s - loss: 0.1402 - categorical_accuracy: 0.9780
 4224/13806 [========>.....................] - ETA: 39s - loss: 0.1409 - categorical_accuracy: 0.9775
 4352/13806 [========>.....................] - ETA: 38s - loss: 0.1415 - categorical_accuracy: 0.9768
 4480/13806 [========>.....................] - ETA: 38s - loss: 0.1399 - categorical_accuracy: 0.9772
 4608/13806 [=========>....................] - ETA: 37s - loss: 0.1384 - categorical_accuracy: 0.9776
 4736/13806 [=========>....................] - ETA: 37s - loss: 0.1394 - categorical_accuracy: 0.9774
 4864/13806 [=========>....................] - ETA: 36s - loss: 0.1380 - categorical_accuracy: 0.9780
 4992/13806 [=========>....................] - ETA: 36s - loss: 0.1368 - categorical_accuracy: 0.9786
 5120/13806 [==========>...................] - ETA: 35s - loss: 0.1361 - categorical_accuracy: 0.9787
 5248/13806 [==========>...................] - ETA: 35s - loss: 0.1345 - categorical_accuracy: 0.9792
 5376/13806 [==========>...................] - ETA: 34s - loss: 0.1349 - categorical_accuracy: 0.9795
 5504/13806 [==========>...................] - ETA: 34s - loss: 0.1337 - categorical_accuracy: 0.9800
 5632/13806 [===========>..................] - ETA: 33s - loss: 0.1348 - categorical_accuracy: 0.9803
 5760/13806 [===========>..................] - ETA: 32s - loss: 0.1358 - categorical_accuracy: 0.9800
 5888/13806 [===========>..................] - ETA: 32s - loss: 0.1353 - categorical_accuracy: 0.9800
 6016/13806 [============>.................] - ETA: 31s - loss: 0.1359 - categorical_accuracy: 0.9794
 6144/13806 [============>.................] - ETA: 31s - loss: 0.1352 - categorical_accuracy: 0.9797
 6272/13806 [============>.................] - ETA: 30s - loss: 0.1349 - categorical_accuracy: 0.9796
 6400/13806 [============>.................] - ETA: 30s - loss: 0.1350 - categorical_accuracy: 0.9795
 6528/13806 [=============>................] - ETA: 29s - loss: 0.1343 - categorical_accuracy: 0.9798
 6656/13806 [=============>................] - ETA: 29s - loss: 0.1341 - categorical_accuracy: 0.9797
 6784/13806 [=============>................] - ETA: 28s - loss: 0.1338 - categorical_accuracy: 0.9794
 6912/13806 [==============>...............] - ETA: 28s - loss: 0.1335 - categorical_accuracy: 0.9795
 7040/13806 [==============>...............] - ETA: 27s - loss: 0.1334 - categorical_accuracy: 0.9794
 7168/13806 [==============>...............] - ETA: 27s - loss: 0.1336 - categorical_accuracy: 0.9795
 7296/13806 [==============>...............] - ETA: 26s - loss: 0.1339 - categorical_accuracy: 0.9794
 7424/13806 [===============>..............] - ETA: 26s - loss: 0.1348 - categorical_accuracy: 0.9793
 7552/13806 [===============>..............] - ETA: 25s - loss: 0.1357 - categorical_accuracy: 0.9791
 7680/13806 [===============>..............] - ETA: 25s - loss: 0.1353 - categorical_accuracy: 0.9792
 7808/13806 [===============>..............] - ETA: 24s - loss: 0.1343 - categorical_accuracy: 0.9795
 7936/13806 [================>.............] - ETA: 24s - loss: 0.1336 - categorical_accuracy: 0.9797
 8064/13806 [================>.............] - ETA: 23s - loss: 0.1336 - categorical_accuracy: 0.9797
 8192/13806 [================>.............] - ETA: 23s - loss: 0.1328 - categorical_accuracy: 0.9800
 8320/13806 [=================>............] - ETA: 22s - loss: 0.1330 - categorical_accuracy: 0.9796
 8448/13806 [=================>............] - ETA: 21s - loss: 0.1323 - categorical_accuracy: 0.9799
 8576/13806 [=================>............] - ETA: 21s - loss: 0.1322 - categorical_accuracy: 0.9799
 8704/13806 [=================>............] - ETA: 20s - loss: 0.1317 - categorical_accuracy: 0.9800
 8832/13806 [==================>...........] - ETA: 20s - loss: 0.1310 - categorical_accuracy: 0.9802
 8960/13806 [==================>...........] - ETA: 19s - loss: 0.1306 - categorical_accuracy: 0.9801
 9088/13806 [==================>...........] - ETA: 19s - loss: 0.1314 - categorical_accuracy: 0.9800
 9216/13806 [===================>..........] - ETA: 18s - loss: 0.1324 - categorical_accuracy: 0.9800
 9344/13806 [===================>..........] - ETA: 18s - loss: 0.1319 - categorical_accuracy: 0.9800
 9472/13806 [===================>..........] - ETA: 17s - loss: 0.1324 - categorical_accuracy: 0.9800
 9600/13806 [===================>..........] - ETA: 17s - loss: 0.1318 - categorical_accuracy: 0.9801
 9728/13806 [====================>.........] - ETA: 16s - loss: 0.1322 - categorical_accuracy: 0.9802
 9856/13806 [====================>.........] - ETA: 16s - loss: 0.1315 - categorical_accuracy: 0.9804
 9984/13806 [====================>.........] - ETA: 15s - loss: 0.1312 - categorical_accuracy: 0.9805
10112/13806 [====================>.........] - ETA: 15s - loss: 0.1312 - categorical_accuracy: 0.9806
10240/13806 [=====================>........] - ETA: 14s - loss: 0.1311 - categorical_accuracy: 0.9805
10368/13806 [=====================>........] - ETA: 14s - loss: 0.1337 - categorical_accuracy: 0.9800
10496/13806 [=====================>........] - ETA: 13s - loss: 0.1339 - categorical_accuracy: 0.9800
10624/13806 [======================>.......] - ETA: 13s - loss: 0.1339 - categorical_accuracy: 0.9800
10752/13806 [======================>.......] - ETA: 12s - loss: 0.1333 - categorical_accuracy: 0.9802
10880/13806 [======================>.......] - ETA: 11s - loss: 0.1338 - categorical_accuracy: 0.9801
11008/13806 [======================>.......] - ETA: 11s - loss: 0.1345 - categorical_accuracy: 0.9800
11136/13806 [=======================>......] - ETA: 10s - loss: 0.1341 - categorical_accuracy: 0.9801
11264/13806 [=======================>......] - ETA: 10s - loss: 0.1344 - categorical_accuracy: 0.9799
11392/13806 [=======================>......] - ETA: 9s - loss: 0.1340 - categorical_accuracy: 0.9801 
11520/13806 [========================>.....] - ETA: 9s - loss: 0.1334 - categorical_accuracy: 0.9802
11648/13806 [========================>.....] - ETA: 8s - loss: 0.1334 - categorical_accuracy: 0.9802
11776/13806 [========================>.....] - ETA: 8s - loss: 0.1331 - categorical_accuracy: 0.9802
11904/13806 [========================>.....] - ETA: 7s - loss: 0.1335 - categorical_accuracy: 0.9801
12032/13806 [=========================>....] - ETA: 7s - loss: 0.1339 - categorical_accuracy: 0.9801
12160/13806 [=========================>....] - ETA: 6s - loss: 0.1335 - categorical_accuracy: 0.9802
12288/13806 [=========================>....] - ETA: 6s - loss: 0.1341 - categorical_accuracy: 0.9801
12416/13806 [=========================>....] - ETA: 5s - loss: 0.1343 - categorical_accuracy: 0.9802
12544/13806 [==========================>...] - ETA: 5s - loss: 0.1337 - categorical_accuracy: 0.9804
12672/13806 [==========================>...] - ETA: 4s - loss: 0.1333 - categorical_accuracy: 0.9805
12800/13806 [==========================>...] - ETA: 4s - loss: 0.1341 - categorical_accuracy: 0.9804
12928/13806 [===========================>..] - ETA: 3s - loss: 0.1344 - categorical_accuracy: 0.9804
13056/13806 [===========================>..] - ETA: 3s - loss: 0.1340 - categorical_accuracy: 0.9805
13184/13806 [===========================>..] - ETA: 2s - loss: 0.1338 - categorical_accuracy: 0.9805
13312/13806 [===========================>..] - ETA: 2s - loss: 0.1339 - categorical_accuracy: 0.9805
13440/13806 [============================>.] - ETA: 1s - loss: 0.1345 - categorical_accuracy: 0.9804
13568/13806 [============================>.] - ETA: 0s - loss: 0.1339 - categorical_accuracy: 0.9805
13696/13806 [============================>.] - ETA: 0s - loss: 0.1340 - categorical_accuracy: 0.9805
13806/13806 [==============================] - 58s 4ms/step - loss: 0.1339 - categorical_accuracy: 0.9806 - val_loss: 1.5366 - val_categorical_accuracy: 0.5421

Epoch 00007: val_categorical_accuracy improved from 0.53413 to 0.54208, saving model to results/vardial2018/multi_input_with_dropout/model_weights.hdf5
Epoch 8/15

  128/13806 [..............................] - ETA: 55s - loss: 0.0971 - categorical_accuracy: 0.9844
  256/13806 [..............................] - ETA: 54s - loss: 0.1018 - categorical_accuracy: 0.9883
  384/13806 [..............................] - ETA: 54s - loss: 0.1042 - categorical_accuracy: 0.9870
  512/13806 [>.............................] - ETA: 53s - loss: 0.1032 - categorical_accuracy: 0.9883
  640/13806 [>.............................] - ETA: 53s - loss: 0.1016 - categorical_accuracy: 0.9891
  768/13806 [>.............................] - ETA: 52s - loss: 0.1167 - categorical_accuracy: 0.9896
  896/13806 [>.............................] - ETA: 52s - loss: 0.1215 - categorical_accuracy: 0.9866
 1024/13806 [=>............................] - ETA: 51s - loss: 0.1236 - categorical_accuracy: 0.9834
 1152/13806 [=>............................] - ETA: 51s - loss: 0.1203 - categorical_accuracy: 0.9852
 1280/13806 [=>............................] - ETA: 51s - loss: 0.1275 - categorical_accuracy: 0.9820
 1408/13806 [==>...........................] - ETA: 50s - loss: 0.1224 - categorical_accuracy: 0.9837
 1536/13806 [==>...........................] - ETA: 49s - loss: 0.1238 - categorical_accuracy: 0.9824
 1664/13806 [==>...........................] - ETA: 49s - loss: 0.1234 - categorical_accuracy: 0.9826
 1792/13806 [==>...........................] - ETA: 48s - loss: 0.1244 - categorical_accuracy: 0.9827
 1920/13806 [===>..........................] - ETA: 48s - loss: 0.1218 - categorical_accuracy: 0.9839
 2048/13806 [===>..........................] - ETA: 47s - loss: 0.1234 - categorical_accuracy: 0.9834
 2176/13806 [===>..........................] - ETA: 47s - loss: 0.1225 - categorical_accuracy: 0.9835
 2304/13806 [====>.........................] - ETA: 46s - loss: 0.1234 - categorical_accuracy: 0.9835
 2432/13806 [====>.........................] - ETA: 46s - loss: 0.1224 - categorical_accuracy: 0.9836
 2560/13806 [====>.........................] - ETA: 45s - loss: 0.1201 - categorical_accuracy: 0.9844
 2688/13806 [====>.........................] - ETA: 45s - loss: 0.1249 - categorical_accuracy: 0.9836
 2816/13806 [=====>........................] - ETA: 44s - loss: 0.1281 - categorical_accuracy: 0.9830
 2944/13806 [=====>........................] - ETA: 44s - loss: 0.1271 - categorical_accuracy: 0.9834
 3072/13806 [=====>........................] - ETA: 43s - loss: 0.1282 - categorical_accuracy: 0.9834
 3200/13806 [=====>........................] - ETA: 43s - loss: 0.1272 - categorical_accuracy: 0.9838
 3328/13806 [======>.......................] - ETA: 42s - loss: 0.1263 - categorical_accuracy: 0.9838
 3456/13806 [======>.......................] - ETA: 42s - loss: 0.1246 - categorical_accuracy: 0.9841
 3584/13806 [======>.......................] - ETA: 41s - loss: 0.1261 - categorical_accuracy: 0.9838
 3712/13806 [=======>......................] - ETA: 41s - loss: 0.1262 - categorical_accuracy: 0.9838
 3840/13806 [=======>......................] - ETA: 40s - loss: 0.1261 - categorical_accuracy: 0.9839
 3968/13806 [=======>......................] - ETA: 40s - loss: 0.1244 - categorical_accuracy: 0.9844
 4096/13806 [=======>......................] - ETA: 39s - loss: 0.1264 - categorical_accuracy: 0.9839
 4224/13806 [========>.....................] - ETA: 39s - loss: 0.1269 - categorical_accuracy: 0.9837
 4352/13806 [========>.....................] - ETA: 38s - loss: 0.1280 - categorical_accuracy: 0.9835
 4480/13806 [========>.....................] - ETA: 38s - loss: 0.1290 - categorical_accuracy: 0.9830
 4608/13806 [=========>....................] - ETA: 37s - loss: 0.1292 - categorical_accuracy: 0.9833
 4736/13806 [=========>....................] - ETA: 37s - loss: 0.1301 - categorical_accuracy: 0.9831
 4864/13806 [=========>....................] - ETA: 36s - loss: 0.1298 - categorical_accuracy: 0.9831
 4992/13806 [=========>....................] - ETA: 35s - loss: 0.1289 - categorical_accuracy: 0.9832
 5120/13806 [==========>...................] - ETA: 35s - loss: 0.1285 - categorical_accuracy: 0.9830
 5248/13806 [==========>...................] - ETA: 34s - loss: 0.1283 - categorical_accuracy: 0.9830
 5376/13806 [==========>...................] - ETA: 34s - loss: 0.1316 - categorical_accuracy: 0.9825
 5504/13806 [==========>...................] - ETA: 33s - loss: 0.1309 - categorical_accuracy: 0.9826
 5632/13806 [===========>..................] - ETA: 33s - loss: 0.1305 - categorical_accuracy: 0.9828
 5760/13806 [===========>..................] - ETA: 32s - loss: 0.1292 - categorical_accuracy: 0.9832
 5888/13806 [===========>..................] - ETA: 32s - loss: 0.1282 - categorical_accuracy: 0.9834
 6016/13806 [============>.................] - ETA: 31s - loss: 0.1280 - categorical_accuracy: 0.9834
 6144/13806 [============>.................] - ETA: 31s - loss: 0.1280 - categorical_accuracy: 0.9834
 6272/13806 [============>.................] - ETA: 30s - loss: 0.1276 - categorical_accuracy: 0.9836
 6400/13806 [============>.................] - ETA: 30s - loss: 0.1267 - categorical_accuracy: 0.9838
 6528/13806 [=============>................] - ETA: 29s - loss: 0.1277 - categorical_accuracy: 0.9835
 6656/13806 [=============>................] - ETA: 29s - loss: 0.1274 - categorical_accuracy: 0.9833
 6784/13806 [=============>................] - ETA: 28s - loss: 0.1276 - categorical_accuracy: 0.9832
 6912/13806 [==============>...............] - ETA: 28s - loss: 0.1285 - categorical_accuracy: 0.9829
 7040/13806 [==============>...............] - ETA: 27s - loss: 0.1300 - categorical_accuracy: 0.9825
 7168/13806 [==============>...............] - ETA: 27s - loss: 0.1294 - categorical_accuracy: 0.9828
 7296/13806 [==============>...............] - ETA: 26s - loss: 0.1289 - categorical_accuracy: 0.9830
 7424/13806 [===============>..............] - ETA: 26s - loss: 0.1285 - categorical_accuracy: 0.9830
 7552/13806 [===============>..............] - ETA: 25s - loss: 0.1280 - categorical_accuracy: 0.9829
 7680/13806 [===============>..............] - ETA: 25s - loss: 0.1287 - categorical_accuracy: 0.9826
 7808/13806 [===============>..............] - ETA: 24s - loss: 0.1296 - categorical_accuracy: 0.9826
 7936/13806 [================>.............] - ETA: 23s - loss: 0.1297 - categorical_accuracy: 0.9826
 8064/13806 [================>.............] - ETA: 23s - loss: 0.1307 - categorical_accuracy: 0.9825
 8192/13806 [================>.............] - ETA: 22s - loss: 0.1304 - categorical_accuracy: 0.9827
 8320/13806 [=================>............] - ETA: 22s - loss: 0.1316 - categorical_accuracy: 0.9823
 8448/13806 [=================>............] - ETA: 21s - loss: 0.1331 - categorical_accuracy: 0.9822
 8576/13806 [=================>............] - ETA: 21s - loss: 0.1332 - categorical_accuracy: 0.9822
 8704/13806 [=================>............] - ETA: 20s - loss: 0.1330 - categorical_accuracy: 0.9820
 8832/13806 [==================>...........] - ETA: 20s - loss: 0.1321 - categorical_accuracy: 0.9821
 8960/13806 [==================>...........] - ETA: 19s - loss: 0.1321 - categorical_accuracy: 0.9821
 9088/13806 [==================>...........] - ETA: 19s - loss: 0.1315 - categorical_accuracy: 0.9822
 9216/13806 [===================>..........] - ETA: 18s - loss: 0.1310 - categorical_accuracy: 0.9823
 9344/13806 [===================>..........] - ETA: 18s - loss: 0.1304 - categorical_accuracy: 0.9824
 9472/13806 [===================>..........] - ETA: 17s - loss: 0.1310 - categorical_accuracy: 0.9825
 9600/13806 [===================>..........] - ETA: 17s - loss: 0.1305 - categorical_accuracy: 0.9826
 9728/13806 [====================>.........] - ETA: 16s - loss: 0.1299 - categorical_accuracy: 0.9828
 9856/13806 [====================>.........] - ETA: 16s - loss: 0.1296 - categorical_accuracy: 0.9830
 9984/13806 [====================>.........] - ETA: 15s - loss: 0.1302 - categorical_accuracy: 0.9829
10112/13806 [====================>.........] - ETA: 15s - loss: 0.1297 - categorical_accuracy: 0.9831
10240/13806 [=====================>........] - ETA: 14s - loss: 0.1296 - categorical_accuracy: 0.9831
10368/13806 [=====================>........] - ETA: 14s - loss: 0.1292 - categorical_accuracy: 0.9830
10496/13806 [=====================>........] - ETA: 13s - loss: 0.1296 - categorical_accuracy: 0.9830
10624/13806 [======================>.......] - ETA: 12s - loss: 0.1300 - categorical_accuracy: 0.9829
10752/13806 [======================>.......] - ETA: 12s - loss: 0.1297 - categorical_accuracy: 0.9830
10880/13806 [======================>.......] - ETA: 11s - loss: 0.1293 - categorical_accuracy: 0.9831
11008/13806 [======================>.......] - ETA: 11s - loss: 0.1295 - categorical_accuracy: 0.9829
11136/13806 [=======================>......] - ETA: 10s - loss: 0.1297 - categorical_accuracy: 0.9828
11264/13806 [=======================>......] - ETA: 10s - loss: 0.1291 - categorical_accuracy: 0.9829
11392/13806 [=======================>......] - ETA: 9s - loss: 0.1292 - categorical_accuracy: 0.9829 
11520/13806 [========================>.....] - ETA: 9s - loss: 0.1298 - categorical_accuracy: 0.9827
11648/13806 [========================>.....] - ETA: 8s - loss: 0.1306 - categorical_accuracy: 0.9825
11776/13806 [========================>.....] - ETA: 8s - loss: 0.1307 - categorical_accuracy: 0.9824
11904/13806 [========================>.....] - ETA: 7s - loss: 0.1308 - categorical_accuracy: 0.9824
12032/13806 [=========================>....] - ETA: 7s - loss: 0.1305 - categorical_accuracy: 0.9825
12160/13806 [=========================>....] - ETA: 6s - loss: 0.1304 - categorical_accuracy: 0.9825
12288/13806 [=========================>....] - ETA: 6s - loss: 0.1302 - categorical_accuracy: 0.9825
12416/13806 [=========================>....] - ETA: 5s - loss: 0.1302 - categorical_accuracy: 0.9824
12544/13806 [==========================>...] - ETA: 5s - loss: 0.1315 - categorical_accuracy: 0.9821
12672/13806 [==========================>...] - ETA: 4s - loss: 0.1324 - categorical_accuracy: 0.9818
12800/13806 [==========================>...] - ETA: 4s - loss: 0.1322 - categorical_accuracy: 0.9819
12928/13806 [===========================>..] - ETA: 3s - loss: 0.1319 - categorical_accuracy: 0.9819
13056/13806 [===========================>..] - ETA: 3s - loss: 0.1314 - categorical_accuracy: 0.9821
13184/13806 [===========================>..] - ETA: 2s - loss: 0.1313 - categorical_accuracy: 0.9821
13312/13806 [===========================>..] - ETA: 2s - loss: 0.1311 - categorical_accuracy: 0.9822
13440/13806 [============================>.] - ETA: 1s - loss: 0.1306 - categorical_accuracy: 0.9823
13568/13806 [============================>.] - ETA: 0s - loss: 0.1301 - categorical_accuracy: 0.9825
13696/13806 [============================>.] - ETA: 0s - loss: 0.1305 - categorical_accuracy: 0.9824
13806/13806 [==============================] - 58s 4ms/step - loss: 0.1307 - categorical_accuracy: 0.9823 - val_loss: 1.6123 - val_categorical_accuracy: 0.5255

Epoch 00008: val_categorical_accuracy did not improve
Epoch 9/15

  128/13806 [..............................] - ETA: 55s - loss: 0.1338 - categorical_accuracy: 0.9844
  256/13806 [..............................] - ETA: 54s - loss: 0.1462 - categorical_accuracy: 0.9805
  384/13806 [..............................] - ETA: 54s - loss: 0.1429 - categorical_accuracy: 0.9818
  512/13806 [>.............................] - ETA: 54s - loss: 0.1383 - categorical_accuracy: 0.9824
  640/13806 [>.............................] - ETA: 53s - loss: 0.1434 - categorical_accuracy: 0.9812
  768/13806 [>.............................] - ETA: 53s - loss: 0.1333 - categorical_accuracy: 0.9818
  896/13806 [>.............................] - ETA: 53s - loss: 0.1293 - categorical_accuracy: 0.9810
 1024/13806 [=>............................] - ETA: 52s - loss: 0.1540 - categorical_accuracy: 0.9775
 1152/13806 [=>............................] - ETA: 52s - loss: 0.1575 - categorical_accuracy: 0.9774
 1280/13806 [=>............................] - ETA: 51s - loss: 0.1516 - categorical_accuracy: 0.9789
 1408/13806 [==>...........................] - ETA: 50s - loss: 0.1551 - categorical_accuracy: 0.9780
 1536/13806 [==>...........................] - ETA: 50s - loss: 0.1583 - categorical_accuracy: 0.9759
 1664/13806 [==>...........................] - ETA: 49s - loss: 0.1533 - categorical_accuracy: 0.9778
 1792/13806 [==>...........................] - ETA: 49s - loss: 0.1490 - categorical_accuracy: 0.9788
 1920/13806 [===>..........................] - ETA: 48s - loss: 0.1460 - categorical_accuracy: 0.9792
 2048/13806 [===>..........................] - ETA: 48s - loss: 0.1441 - categorical_accuracy: 0.9795
 2176/13806 [===>..........................] - ETA: 47s - loss: 0.1431 - categorical_accuracy: 0.9793
 2304/13806 [====>.........................] - ETA: 47s - loss: 0.1391 - categorical_accuracy: 0.9805
 2432/13806 [====>.........................] - ETA: 46s - loss: 0.1451 - categorical_accuracy: 0.9803
 2560/13806 [====>.........................] - ETA: 46s - loss: 0.1433 - categorical_accuracy: 0.9809
 2688/13806 [====>.........................] - ETA: 45s - loss: 0.1438 - categorical_accuracy: 0.9807
 2816/13806 [=====>........................] - ETA: 45s - loss: 0.1442 - categorical_accuracy: 0.9808
 2944/13806 [=====>........................] - ETA: 44s - loss: 0.1448 - categorical_accuracy: 0.9813
 3072/13806 [=====>........................] - ETA: 44s - loss: 0.1455 - categorical_accuracy: 0.9811
 3200/13806 [=====>........................] - ETA: 43s - loss: 0.1455 - categorical_accuracy: 0.9812
 3328/13806 [======>.......................] - ETA: 43s - loss: 0.1462 - categorical_accuracy: 0.9808
 3456/13806 [======>.......................] - ETA: 42s - loss: 0.1451 - categorical_accuracy: 0.9809
 3584/13806 [======>.......................] - ETA: 42s - loss: 0.1437 - categorical_accuracy: 0.9810
 3712/13806 [=======>......................] - ETA: 41s - loss: 0.1420 - categorical_accuracy: 0.9814
 3840/13806 [=======>......................] - ETA: 40s - loss: 0.1404 - categorical_accuracy: 0.9818
 3968/13806 [=======>......................] - ETA: 40s - loss: 0.1389 - categorical_accuracy: 0.9821
 4096/13806 [=======>......................] - ETA: 39s - loss: 0.1402 - categorical_accuracy: 0.9814
 4224/13806 [========>.....................] - ETA: 39s - loss: 0.1387 - categorical_accuracy: 0.9818
 4352/13806 [========>.....................] - ETA: 38s - loss: 0.1385 - categorical_accuracy: 0.9816
 4480/13806 [========>.....................] - ETA: 38s - loss: 0.1364 - categorical_accuracy: 0.9821
 4608/13806 [=========>....................] - ETA: 37s - loss: 0.1368 - categorical_accuracy: 0.9816
 4736/13806 [=========>....................] - ETA: 37s - loss: 0.1351 - categorical_accuracy: 0.9821
 4864/13806 [=========>....................] - ETA: 36s - loss: 0.1335 - categorical_accuracy: 0.9825
 4992/13806 [=========>....................] - ETA: 36s - loss: 0.1330 - categorical_accuracy: 0.9828
 5120/13806 [==========>...................] - ETA: 35s - loss: 0.1318 - categorical_accuracy: 0.9830
 5248/13806 [==========>...................] - ETA: 35s - loss: 0.1306 - categorical_accuracy: 0.9832
 5376/13806 [==========>...................] - ETA: 34s - loss: 0.1301 - categorical_accuracy: 0.9833
 5504/13806 [==========>...................] - ETA: 34s - loss: 0.1302 - categorical_accuracy: 0.9827
 5632/13806 [===========>..................] - ETA: 33s - loss: 0.1305 - categorical_accuracy: 0.9828
 5760/13806 [===========>..................] - ETA: 33s - loss: 0.1305 - categorical_accuracy: 0.9828
 5888/13806 [===========>..................] - ETA: 32s - loss: 0.1296 - categorical_accuracy: 0.9830
 6016/13806 [============>.................] - ETA: 31s - loss: 0.1294 - categorical_accuracy: 0.9825
 6144/13806 [============>.................] - ETA: 31s - loss: 0.1289 - categorical_accuracy: 0.9827
 6272/13806 [============>.................] - ETA: 30s - loss: 0.1285 - categorical_accuracy: 0.9828
 6400/13806 [============>.................] - ETA: 30s - loss: 0.1282 - categorical_accuracy: 0.9828
 6528/13806 [=============>................] - ETA: 29s - loss: 0.1271 - categorical_accuracy: 0.9831
 6656/13806 [=============>................] - ETA: 29s - loss: 0.1266 - categorical_accuracy: 0.9833
 6784/13806 [=============>................] - ETA: 28s - loss: 0.1258 - categorical_accuracy: 0.9835
 6912/13806 [==============>...............] - ETA: 28s - loss: 0.1261 - categorical_accuracy: 0.9832
 7040/13806 [==============>...............] - ETA: 27s - loss: 0.1258 - categorical_accuracy: 0.9831
 7168/13806 [==============>...............] - ETA: 27s - loss: 0.1255 - categorical_accuracy: 0.9833
 7296/13806 [==============>...............] - ETA: 26s - loss: 0.1280 - categorical_accuracy: 0.9829
 7424/13806 [===============>..............] - ETA: 26s - loss: 0.1277 - categorical_accuracy: 0.9829
 7552/13806 [===============>..............] - ETA: 25s - loss: 0.1270 - categorical_accuracy: 0.9829
 7680/13806 [===============>..............] - ETA: 25s - loss: 0.1263 - categorical_accuracy: 0.9831
 7808/13806 [===============>..............] - ETA: 24s - loss: 0.1265 - categorical_accuracy: 0.9831
 7936/13806 [================>.............] - ETA: 24s - loss: 0.1263 - categorical_accuracy: 0.9832
 8064/13806 [================>.............] - ETA: 23s - loss: 0.1258 - categorical_accuracy: 0.9833
 8192/13806 [================>.............] - ETA: 22s - loss: 0.1254 - categorical_accuracy: 0.9833
 8320/13806 [=================>............] - ETA: 22s - loss: 0.1263 - categorical_accuracy: 0.9831
 8448/13806 [=================>............] - ETA: 21s - loss: 0.1266 - categorical_accuracy: 0.9830
 8576/13806 [=================>............] - ETA: 21s - loss: 0.1272 - categorical_accuracy: 0.9829
 8704/13806 [=================>............] - ETA: 20s - loss: 0.1275 - categorical_accuracy: 0.9827
 8832/13806 [==================>...........] - ETA: 20s - loss: 0.1283 - categorical_accuracy: 0.9823
 8960/13806 [==================>...........] - ETA: 19s - loss: 0.1275 - categorical_accuracy: 0.9825
 9088/13806 [==================>...........] - ETA: 19s - loss: 0.1276 - categorical_accuracy: 0.9825
 9216/13806 [===================>..........] - ETA: 18s - loss: 0.1272 - categorical_accuracy: 0.9825
 9344/13806 [===================>..........] - ETA: 18s - loss: 0.1282 - categorical_accuracy: 0.9824
 9472/13806 [===================>..........] - ETA: 17s - loss: 0.1286 - categorical_accuracy: 0.9823
 9600/13806 [===================>..........] - ETA: 17s - loss: 0.1278 - categorical_accuracy: 0.9824
 9728/13806 [====================>.........] - ETA: 16s - loss: 0.1283 - categorical_accuracy: 0.9820
 9856/13806 [====================>.........] - ETA: 16s - loss: 0.1280 - categorical_accuracy: 0.9820
 9984/13806 [====================>.........] - ETA: 15s - loss: 0.1277 - categorical_accuracy: 0.9822
10112/13806 [====================>.........] - ETA: 15s - loss: 0.1275 - categorical_accuracy: 0.9822
10240/13806 [=====================>........] - ETA: 14s - loss: 0.1268 - categorical_accuracy: 0.9824
10368/13806 [=====================>........] - ETA: 14s - loss: 0.1265 - categorical_accuracy: 0.9825
10496/13806 [=====================>........] - ETA: 13s - loss: 0.1260 - categorical_accuracy: 0.9827
10624/13806 [======================>.......] - ETA: 13s - loss: 0.1257 - categorical_accuracy: 0.9827
10752/13806 [======================>.......] - ETA: 12s - loss: 0.1267 - categorical_accuracy: 0.9825
10880/13806 [======================>.......] - ETA: 11s - loss: 0.1267 - categorical_accuracy: 0.9824
11008/13806 [======================>.......] - ETA: 11s - loss: 0.1261 - categorical_accuracy: 0.9825
11136/13806 [=======================>......] - ETA: 10s - loss: 0.1275 - categorical_accuracy: 0.9822
11264/13806 [=======================>......] - ETA: 10s - loss: 0.1274 - categorical_accuracy: 0.9822
11392/13806 [=======================>......] - ETA: 9s - loss: 0.1285 - categorical_accuracy: 0.9819 
11520/13806 [========================>.....] - ETA: 9s - loss: 0.1282 - categorical_accuracy: 0.9820
11648/13806 [========================>.....] - ETA: 8s - loss: 0.1280 - categorical_accuracy: 0.9820
11776/13806 [========================>.....] - ETA: 8s - loss: 0.1281 - categorical_accuracy: 0.9819
11904/13806 [========================>.....] - ETA: 7s - loss: 0.1296 - categorical_accuracy: 0.9814
12032/13806 [=========================>....] - ETA: 7s - loss: 0.1295 - categorical_accuracy: 0.9815
12160/13806 [=========================>....] - ETA: 6s - loss: 0.1300 - categorical_accuracy: 0.9814
12288/13806 [=========================>....] - ETA: 6s - loss: 0.1302 - categorical_accuracy: 0.9814
12416/13806 [=========================>....] - ETA: 5s - loss: 0.1302 - categorical_accuracy: 0.9816
12544/13806 [==========================>...] - ETA: 5s - loss: 0.1311 - categorical_accuracy: 0.9814
12672/13806 [==========================>...] - ETA: 4s - loss: 0.1312 - categorical_accuracy: 0.9813
12800/13806 [==========================>...] - ETA: 4s - loss: 0.1308 - categorical_accuracy: 0.9814
12928/13806 [===========================>..] - ETA: 3s - loss: 0.1306 - categorical_accuracy: 0.9815
13056/13806 [===========================>..] - ETA: 3s - loss: 0.1310 - categorical_accuracy: 0.9815
13184/13806 [===========================>..] - ETA: 2s - loss: 0.1315 - categorical_accuracy: 0.9813
13312/13806 [===========================>..] - ETA: 2s - loss: 0.1321 - categorical_accuracy: 0.9813
13440/13806 [============================>.] - ETA: 1s - loss: 0.1315 - categorical_accuracy: 0.9815
13568/13806 [============================>.] - ETA: 0s - loss: 0.1311 - categorical_accuracy: 0.9814
13696/13806 [============================>.] - ETA: 0s - loss: 0.1309 - categorical_accuracy: 0.9815
13806/13806 [==============================] - 58s 4ms/step - loss: 0.1305 - categorical_accuracy: 0.9817 - val_loss: 1.5456 - val_categorical_accuracy: 0.5302

Epoch 00009: val_categorical_accuracy did not improve
Epoch 10/15

  128/13806 [..............................] - ETA: 54s - loss: 0.1158 - categorical_accuracy: 0.9766
  256/13806 [..............................] - ETA: 54s - loss: 0.1180 - categorical_accuracy: 0.9805
  384/13806 [..............................] - ETA: 54s - loss: 0.1053 - categorical_accuracy: 0.9844
  512/13806 [>.............................] - ETA: 53s - loss: 0.0988 - categorical_accuracy: 0.9863
  640/13806 [>.............................] - ETA: 53s - loss: 0.1137 - categorical_accuracy: 0.9828
  768/13806 [>.............................] - ETA: 53s - loss: 0.1058 - categorical_accuracy: 0.9857
  896/13806 [>.............................] - ETA: 52s - loss: 0.1034 - categorical_accuracy: 0.9866
 1024/13806 [=>............................] - ETA: 52s - loss: 0.1080 - categorical_accuracy: 0.9834
 1152/13806 [=>............................] - ETA: 51s - loss: 0.1041 - categorical_accuracy: 0.9844
 1280/13806 [=>............................] - ETA: 51s - loss: 0.1122 - categorical_accuracy: 0.9812
 1408/13806 [==>...........................] - ETA: 50s - loss: 0.1244 - categorical_accuracy: 0.9808
 1536/13806 [==>...........................] - ETA: 50s - loss: 0.1278 - categorical_accuracy: 0.9805
 1664/13806 [==>...........................] - ETA: 49s - loss: 0.1332 - categorical_accuracy: 0.9784
 1792/13806 [==>...........................] - ETA: 49s - loss: 0.1367 - categorical_accuracy: 0.9777
 1920/13806 [===>..........................] - ETA: 48s - loss: 0.1354 - categorical_accuracy: 0.9781
 2048/13806 [===>..........................] - ETA: 48s - loss: 0.1337 - categorical_accuracy: 0.9780
 2176/13806 [===>..........................] - ETA: 47s - loss: 0.1380 - categorical_accuracy: 0.9779
 2304/13806 [====>.........................] - ETA: 47s - loss: 0.1361 - categorical_accuracy: 0.9783
 2432/13806 [====>.........................] - ETA: 46s - loss: 0.1402 - categorical_accuracy: 0.9778
 2560/13806 [====>.........................] - ETA: 46s - loss: 0.1380 - categorical_accuracy: 0.9781
 2688/13806 [====>.........................] - ETA: 45s - loss: 0.1361 - categorical_accuracy: 0.9788
 2816/13806 [=====>........................] - ETA: 45s - loss: 0.1364 - categorical_accuracy: 0.9787
 2944/13806 [=====>........................] - ETA: 44s - loss: 0.1362 - categorical_accuracy: 0.9789
 3072/13806 [=====>........................] - ETA: 43s - loss: 0.1359 - categorical_accuracy: 0.9792
 3200/13806 [=====>........................] - ETA: 43s - loss: 0.1358 - categorical_accuracy: 0.9791
 3328/13806 [======>.......................] - ETA: 42s - loss: 0.1348 - categorical_accuracy: 0.9793
 3456/13806 [======>.......................] - ETA: 42s - loss: 0.1324 - categorical_accuracy: 0.9797
 3584/13806 [======>.......................] - ETA: 41s - loss: 0.1321 - categorical_accuracy: 0.9799
 3712/13806 [=======>......................] - ETA: 41s - loss: 0.1310 - categorical_accuracy: 0.9801
 3840/13806 [=======>......................] - ETA: 40s - loss: 0.1312 - categorical_accuracy: 0.9797
 3968/13806 [=======>......................] - ETA: 40s - loss: 0.1291 - categorical_accuracy: 0.9803
 4096/13806 [=======>......................] - ETA: 39s - loss: 0.1270 - categorical_accuracy: 0.9810
 4224/13806 [========>.....................] - ETA: 39s - loss: 0.1259 - categorical_accuracy: 0.9811
 4352/13806 [========>.....................] - ETA: 38s - loss: 0.1253 - categorical_accuracy: 0.9809
 4480/13806 [========>.....................] - ETA: 38s - loss: 0.1253 - categorical_accuracy: 0.9812
 4608/13806 [=========>....................] - ETA: 37s - loss: 0.1247 - categorical_accuracy: 0.9813
 4736/13806 [=========>....................] - ETA: 37s - loss: 0.1238 - categorical_accuracy: 0.9814
 4864/13806 [=========>....................] - ETA: 36s - loss: 0.1238 - categorical_accuracy: 0.9815
 4992/13806 [=========>....................] - ETA: 35s - loss: 0.1234 - categorical_accuracy: 0.9816
 5120/13806 [==========>...................] - ETA: 35s - loss: 0.1230 - categorical_accuracy: 0.9816
 5248/13806 [==========>...................] - ETA: 34s - loss: 0.1217 - categorical_accuracy: 0.9821
 5376/13806 [==========>...................] - ETA: 34s - loss: 0.1211 - categorical_accuracy: 0.9821
 5504/13806 [==========>...................] - ETA: 33s - loss: 0.1204 - categorical_accuracy: 0.9820
 5632/13806 [===========>..................] - ETA: 33s - loss: 0.1214 - categorical_accuracy: 0.9817
 5760/13806 [===========>..................] - ETA: 32s - loss: 0.1213 - categorical_accuracy: 0.9819
 5888/13806 [===========>..................] - ETA: 32s - loss: 0.1209 - categorical_accuracy: 0.9820
 6016/13806 [============>.................] - ETA: 31s - loss: 0.1199 - categorical_accuracy: 0.9824
 6144/13806 [============>.................] - ETA: 31s - loss: 0.1189 - categorical_accuracy: 0.9827
 6272/13806 [============>.................] - ETA: 30s - loss: 0.1194 - categorical_accuracy: 0.9826
 6400/13806 [============>.................] - ETA: 30s - loss: 0.1186 - categorical_accuracy: 0.9827
 6528/13806 [=============>................] - ETA: 29s - loss: 0.1189 - categorical_accuracy: 0.9825
 6656/13806 [=============>................] - ETA: 29s - loss: 0.1183 - categorical_accuracy: 0.9827
 6784/13806 [=============>................] - ETA: 28s - loss: 0.1176 - categorical_accuracy: 0.9830
 6912/13806 [==============>...............] - ETA: 28s - loss: 0.1187 - categorical_accuracy: 0.9831
 7040/13806 [==============>...............] - ETA: 27s - loss: 0.1192 - categorical_accuracy: 0.9831
 7168/13806 [==============>...............] - ETA: 27s - loss: 0.1190 - categorical_accuracy: 0.9830
 7296/13806 [==============>...............] - ETA: 26s - loss: 0.1190 - categorical_accuracy: 0.9830
 7424/13806 [===============>..............] - ETA: 25s - loss: 0.1184 - categorical_accuracy: 0.9832
 7552/13806 [===============>..............] - ETA: 25s - loss: 0.1182 - categorical_accuracy: 0.9831
 7680/13806 [===============>..............] - ETA: 24s - loss: 0.1176 - categorical_accuracy: 0.9832
 7808/13806 [===============>..............] - ETA: 24s - loss: 0.1171 - categorical_accuracy: 0.9834
 7936/13806 [================>.............] - ETA: 23s - loss: 0.1177 - categorical_accuracy: 0.9832
 8064/13806 [================>.............] - ETA: 23s - loss: 0.1182 - categorical_accuracy: 0.9833
 8192/13806 [================>.............] - ETA: 22s - loss: 0.1194 - categorical_accuracy: 0.9830
 8320/13806 [=================>............] - ETA: 22s - loss: 0.1196 - categorical_accuracy: 0.9829
 8448/13806 [=================>............] - ETA: 21s - loss: 0.1188 - categorical_accuracy: 0.9832
 8576/13806 [=================>............] - ETA: 21s - loss: 0.1186 - categorical_accuracy: 0.9832
 8704/13806 [=================>............] - ETA: 20s - loss: 0.1191 - categorical_accuracy: 0.9831
 8832/13806 [==================>...........] - ETA: 20s - loss: 0.1196 - categorical_accuracy: 0.9829
 8960/13806 [==================>...........] - ETA: 19s - loss: 0.1190 - categorical_accuracy: 0.9831
 9088/13806 [==================>...........] - ETA: 19s - loss: 0.1199 - categorical_accuracy: 0.9831
 9216/13806 [===================>..........] - ETA: 18s - loss: 0.1219 - categorical_accuracy: 0.9829
 9344/13806 [===================>..........] - ETA: 18s - loss: 0.1219 - categorical_accuracy: 0.9829
 9472/13806 [===================>..........] - ETA: 17s - loss: 0.1216 - categorical_accuracy: 0.9830
 9600/13806 [===================>..........] - ETA: 17s - loss: 0.1215 - categorical_accuracy: 0.9830
 9728/13806 [====================>.........] - ETA: 16s - loss: 0.1228 - categorical_accuracy: 0.9826
 9856/13806 [====================>.........] - ETA: 16s - loss: 0.1227 - categorical_accuracy: 0.9825
 9984/13806 [====================>.........] - ETA: 15s - loss: 0.1235 - categorical_accuracy: 0.9822
10112/13806 [====================>.........] - ETA: 15s - loss: 0.1233 - categorical_accuracy: 0.9822
10240/13806 [=====================>........] - ETA: 14s - loss: 0.1250 - categorical_accuracy: 0.9820
10368/13806 [=====================>........] - ETA: 14s - loss: 0.1251 - categorical_accuracy: 0.9820
10496/13806 [=====================>........] - ETA: 13s - loss: 0.1254 - categorical_accuracy: 0.9819
10624/13806 [======================>.......] - ETA: 12s - loss: 0.1253 - categorical_accuracy: 0.9819
10752/13806 [======================>.......] - ETA: 12s - loss: 0.1254 - categorical_accuracy: 0.9820
10880/13806 [======================>.......] - ETA: 11s - loss: 0.1250 - categorical_accuracy: 0.9822
11008/13806 [======================>.......] - ETA: 11s - loss: 0.1256 - categorical_accuracy: 0.9820
11136/13806 [=======================>......] - ETA: 10s - loss: 0.1262 - categorical_accuracy: 0.9816
11264/13806 [=======================>......] - ETA: 10s - loss: 0.1267 - categorical_accuracy: 0.9815
11392/13806 [=======================>......] - ETA: 9s - loss: 0.1269 - categorical_accuracy: 0.9816 
11520/13806 [========================>.....] - ETA: 9s - loss: 0.1266 - categorical_accuracy: 0.9815
11648/13806 [========================>.....] - ETA: 8s - loss: 0.1278 - categorical_accuracy: 0.9814
11776/13806 [========================>.....] - ETA: 8s - loss: 0.1280 - categorical_accuracy: 0.9811
11904/13806 [========================>.....] - ETA: 7s - loss: 0.1277 - categorical_accuracy: 0.9814
12032/13806 [=========================>....] - ETA: 7s - loss: 0.1274 - categorical_accuracy: 0.9813
12160/13806 [=========================>....] - ETA: 6s - loss: 0.1283 - categorical_accuracy: 0.9810
12288/13806 [=========================>....] - ETA: 6s - loss: 0.1288 - categorical_accuracy: 0.9808
12416/13806 [=========================>....] - ETA: 5s - loss: 0.1286 - categorical_accuracy: 0.9808
12544/13806 [==========================>...] - ETA: 5s - loss: 0.1289 - categorical_accuracy: 0.9805
12672/13806 [==========================>...] - ETA: 4s - loss: 0.1292 - categorical_accuracy: 0.9804
12800/13806 [==========================>...] - ETA: 4s - loss: 0.1292 - categorical_accuracy: 0.9803
12928/13806 [===========================>..] - ETA: 3s - loss: 0.1287 - categorical_accuracy: 0.9804
13056/13806 [===========================>..] - ETA: 3s - loss: 0.1288 - categorical_accuracy: 0.9805
13184/13806 [===========================>..] - ETA: 2s - loss: 0.1292 - categorical_accuracy: 0.9803
13312/13806 [===========================>..] - ETA: 2s - loss: 0.1299 - categorical_accuracy: 0.9801
13440/13806 [============================>.] - ETA: 1s - loss: 0.1300 - categorical_accuracy: 0.9801
13568/13806 [============================>.] - ETA: 0s - loss: 0.1298 - categorical_accuracy: 0.9800
13696/13806 [============================>.] - ETA: 0s - loss: 0.1296 - categorical_accuracy: 0.9801
13806/13806 [==============================] - 58s 4ms/step - loss: 0.1300 - categorical_accuracy: 0.9799 - val_loss: 1.5721 - val_categorical_accuracy: 0.5348

Epoch 00010: val_categorical_accuracy did not improve
Epoch 11/15

  128/13806 [..............................] - ETA: 56s - loss: 0.1760 - categorical_accuracy: 0.9844
  256/13806 [..............................] - ETA: 55s - loss: 0.1332 - categorical_accuracy: 0.9922
  384/13806 [..............................] - ETA: 54s - loss: 0.1562 - categorical_accuracy: 0.9896
  512/13806 [>.............................] - ETA: 54s - loss: 0.1704 - categorical_accuracy: 0.9863
  640/13806 [>.............................] - ETA: 53s - loss: 0.1595 - categorical_accuracy: 0.9859
  768/13806 [>.............................] - ETA: 53s - loss: 0.1639 - categorical_accuracy: 0.9818
  896/13806 [>.............................] - ETA: 53s - loss: 0.1549 - categorical_accuracy: 0.9833
 1024/13806 [=>............................] - ETA: 52s - loss: 0.1481 - categorical_accuracy: 0.9834
 1152/13806 [=>............................] - ETA: 51s - loss: 0.1508 - categorical_accuracy: 0.9818
 1280/13806 [=>............................] - ETA: 51s - loss: 0.1469 - categorical_accuracy: 0.9828
 1408/13806 [==>...........................] - ETA: 50s - loss: 0.1465 - categorical_accuracy: 0.9830
 1536/13806 [==>...........................] - ETA: 50s - loss: 0.1415 - categorical_accuracy: 0.9831
 1664/13806 [==>...........................] - ETA: 49s - loss: 0.1408 - categorical_accuracy: 0.9826
 1792/13806 [==>...........................] - ETA: 49s - loss: 0.1375 - categorical_accuracy: 0.9827
 1920/13806 [===>..........................] - ETA: 48s - loss: 0.1400 - categorical_accuracy: 0.9812
 2048/13806 [===>..........................] - ETA: 48s - loss: 0.1373 - categorical_accuracy: 0.9819
 2176/13806 [===>..........................] - ETA: 47s - loss: 0.1366 - categorical_accuracy: 0.9816
 2304/13806 [====>.........................] - ETA: 46s - loss: 0.1332 - categorical_accuracy: 0.9826
 2432/13806 [====>.........................] - ETA: 46s - loss: 0.1313 - categorical_accuracy: 0.9823
 2560/13806 [====>.........................] - ETA: 45s - loss: 0.1355 - categorical_accuracy: 0.9816
 2688/13806 [====>.........................] - ETA: 45s - loss: 0.1379 - categorical_accuracy: 0.9818
 2816/13806 [=====>........................] - ETA: 44s - loss: 0.1350 - categorical_accuracy: 0.9826
 2944/13806 [=====>........................] - ETA: 44s - loss: 0.1387 - categorical_accuracy: 0.9817
 3072/13806 [=====>........................] - ETA: 43s - loss: 0.1397 - categorical_accuracy: 0.9811
 3200/13806 [=====>........................] - ETA: 43s - loss: 0.1373 - categorical_accuracy: 0.9819
 3328/13806 [======>.......................] - ETA: 42s - loss: 0.1364 - categorical_accuracy: 0.9817
 3456/13806 [======>.......................] - ETA: 42s - loss: 0.1345 - categorical_accuracy: 0.9818
 3584/13806 [======>.......................] - ETA: 41s - loss: 0.1340 - categorical_accuracy: 0.9821
 3712/13806 [=======>......................] - ETA: 41s - loss: 0.1323 - categorical_accuracy: 0.9822
 3840/13806 [=======>......................] - ETA: 40s - loss: 0.1330 - categorical_accuracy: 0.9815
 3968/13806 [=======>......................] - ETA: 40s - loss: 0.1339 - categorical_accuracy: 0.9816
 4096/13806 [=======>......................] - ETA: 39s - loss: 0.1341 - categorical_accuracy: 0.9814
 4224/13806 [========>.....................] - ETA: 39s - loss: 0.1325 - categorical_accuracy: 0.9818
 4352/13806 [========>.....................] - ETA: 38s - loss: 0.1311 - categorical_accuracy: 0.9823
 4480/13806 [========>.....................] - ETA: 37s - loss: 0.1348 - categorical_accuracy: 0.9815
 4608/13806 [=========>....................] - ETA: 37s - loss: 0.1359 - categorical_accuracy: 0.9816
 4736/13806 [=========>....................] - ETA: 37s - loss: 0.1353 - categorical_accuracy: 0.9816
 4864/13806 [=========>....................] - ETA: 36s - loss: 0.1350 - categorical_accuracy: 0.9819
 4992/13806 [=========>....................] - ETA: 35s - loss: 0.1339 - categorical_accuracy: 0.9820
 5120/13806 [==========>...................] - ETA: 35s - loss: 0.1329 - categorical_accuracy: 0.9822
 5248/13806 [==========>...................] - ETA: 34s - loss: 0.1330 - categorical_accuracy: 0.9825
 5376/13806 [==========>...................] - ETA: 34s - loss: 0.1331 - categorical_accuracy: 0.9825
 5504/13806 [==========>...................] - ETA: 33s - loss: 0.1346 - categorical_accuracy: 0.9816
 5632/13806 [===========>..................] - ETA: 33s - loss: 0.1345 - categorical_accuracy: 0.9815
 5760/13806 [===========>..................] - ETA: 32s - loss: 0.1344 - categorical_accuracy: 0.9816
 5888/13806 [===========>..................] - ETA: 32s - loss: 0.1351 - categorical_accuracy: 0.9811
 6016/13806 [============>.................] - ETA: 31s - loss: 0.1347 - categorical_accuracy: 0.9811
 6144/13806 [============>.................] - ETA: 31s - loss: 0.1343 - categorical_accuracy: 0.9811
 6272/13806 [============>.................] - ETA: 30s - loss: 0.1349 - categorical_accuracy: 0.9807
 6400/13806 [============>.................] - ETA: 30s - loss: 0.1360 - categorical_accuracy: 0.9805
 6528/13806 [=============>................] - ETA: 29s - loss: 0.1357 - categorical_accuracy: 0.9804
 6656/13806 [=============>................] - ETA: 29s - loss: 0.1396 - categorical_accuracy: 0.9797
 6784/13806 [=============>................] - ETA: 28s - loss: 0.1388 - categorical_accuracy: 0.9798
 6912/13806 [==============>...............] - ETA: 28s - loss: 0.1385 - categorical_accuracy: 0.9797
 7040/13806 [==============>...............] - ETA: 27s - loss: 0.1374 - categorical_accuracy: 0.9800
 7168/13806 [==============>...............] - ETA: 27s - loss: 0.1377 - categorical_accuracy: 0.9801
 7296/13806 [==============>...............] - ETA: 26s - loss: 0.1372 - categorical_accuracy: 0.9801
 7424/13806 [===============>..............] - ETA: 26s - loss: 0.1379 - categorical_accuracy: 0.9801
 7552/13806 [===============>..............] - ETA: 25s - loss: 0.1372 - categorical_accuracy: 0.9803
 7680/13806 [===============>..............] - ETA: 24s - loss: 0.1363 - categorical_accuracy: 0.9803
 7808/13806 [===============>..............] - ETA: 24s - loss: 0.1359 - categorical_accuracy: 0.9804
 7936/13806 [================>.............] - ETA: 23s - loss: 0.1356 - categorical_accuracy: 0.9803
 8064/13806 [================>.............] - ETA: 23s - loss: 0.1353 - categorical_accuracy: 0.9805
 8192/13806 [================>.............] - ETA: 22s - loss: 0.1347 - categorical_accuracy: 0.9805
 8320/13806 [=================>............] - ETA: 22s - loss: 0.1355 - categorical_accuracy: 0.9805
 8448/13806 [=================>............] - ETA: 21s - loss: 0.1354 - categorical_accuracy: 0.9804
 8576/13806 [=================>............] - ETA: 21s - loss: 0.1347 - categorical_accuracy: 0.9804
 8704/13806 [=================>............] - ETA: 20s - loss: 0.1358 - categorical_accuracy: 0.9805
 8832/13806 [==================>...........] - ETA: 20s - loss: 0.1357 - categorical_accuracy: 0.9806
 8960/13806 [==================>...........] - ETA: 19s - loss: 0.1357 - categorical_accuracy: 0.9806
 9088/13806 [==================>...........] - ETA: 19s - loss: 0.1372 - categorical_accuracy: 0.9803
 9216/13806 [===================>..........] - ETA: 18s - loss: 0.1368 - categorical_accuracy: 0.9804
 9344/13806 [===================>..........] - ETA: 18s - loss: 0.1369 - categorical_accuracy: 0.9803
 9472/13806 [===================>..........] - ETA: 17s - loss: 0.1365 - categorical_accuracy: 0.9804
 9600/13806 [===================>..........] - ETA: 17s - loss: 0.1361 - categorical_accuracy: 0.9804
 9728/13806 [====================>.........] - ETA: 16s - loss: 0.1358 - categorical_accuracy: 0.9806
 9856/13806 [====================>.........] - ETA: 16s - loss: 0.1348 - categorical_accuracy: 0.9808
 9984/13806 [====================>.........] - ETA: 15s - loss: 0.1342 - categorical_accuracy: 0.9811
10112/13806 [====================>.........] - ETA: 15s - loss: 0.1333 - categorical_accuracy: 0.9813
10240/13806 [=====================>........] - ETA: 14s - loss: 0.1328 - categorical_accuracy: 0.9813
10368/13806 [=====================>........] - ETA: 14s - loss: 0.1324 - categorical_accuracy: 0.9814
10496/13806 [=====================>........] - ETA: 13s - loss: 0.1323 - categorical_accuracy: 0.9814
10624/13806 [======================>.......] - ETA: 13s - loss: 0.1324 - categorical_accuracy: 0.9814
10752/13806 [======================>.......] - ETA: 12s - loss: 0.1317 - categorical_accuracy: 0.9815
10880/13806 [======================>.......] - ETA: 11s - loss: 0.1313 - categorical_accuracy: 0.9813
11008/13806 [======================>.......] - ETA: 11s - loss: 0.1310 - categorical_accuracy: 0.9813
11136/13806 [=======================>......] - ETA: 10s - loss: 0.1310 - categorical_accuracy: 0.9813
11264/13806 [=======================>......] - ETA: 10s - loss: 0.1306 - categorical_accuracy: 0.9813
11392/13806 [=======================>......] - ETA: 9s - loss: 0.1301 - categorical_accuracy: 0.9814 
11520/13806 [========================>.....] - ETA: 9s - loss: 0.1301 - categorical_accuracy: 0.9814
11648/13806 [========================>.....] - ETA: 8s - loss: 0.1297 - categorical_accuracy: 0.9815
11776/13806 [========================>.....] - ETA: 8s - loss: 0.1294 - categorical_accuracy: 0.9817
11904/13806 [========================>.....] - ETA: 7s - loss: 0.1293 - categorical_accuracy: 0.9816
12032/13806 [=========================>....] - ETA: 7s - loss: 0.1295 - categorical_accuracy: 0.9816
12160/13806 [=========================>....] - ETA: 6s - loss: 0.1290 - categorical_accuracy: 0.9817
12288/13806 [=========================>....] - ETA: 6s - loss: 0.1299 - categorical_accuracy: 0.9816
12416/13806 [=========================>....] - ETA: 5s - loss: 0.1294 - categorical_accuracy: 0.9817
12544/13806 [==========================>...] - ETA: 5s - loss: 0.1305 - categorical_accuracy: 0.9815
12672/13806 [==========================>...] - ETA: 4s - loss: 0.1308 - categorical_accuracy: 0.9815
12800/13806 [==========================>...] - ETA: 4s - loss: 0.1306 - categorical_accuracy: 0.9816
12928/13806 [===========================>..] - ETA: 3s - loss: 0.1304 - categorical_accuracy: 0.9817
13056/13806 [===========================>..] - ETA: 3s - loss: 0.1310 - categorical_accuracy: 0.9816
13184/13806 [===========================>..] - ETA: 2s - loss: 0.1307 - categorical_accuracy: 0.9817
13312/13806 [===========================>..] - ETA: 2s - loss: 0.1308 - categorical_accuracy: 0.9817
13440/13806 [============================>.] - ETA: 1s - loss: 0.1305 - categorical_accuracy: 0.9817
13568/13806 [============================>.] - ETA: 0s - loss: 0.1301 - categorical_accuracy: 0.9819
13696/13806 [============================>.] - ETA: 0s - loss: 0.1310 - categorical_accuracy: 0.9817
13806/13806 [==============================] - 58s 4ms/step - loss: 0.1307 - categorical_accuracy: 0.9818 - val_loss: 1.5186 - val_categorical_accuracy: 0.5401

Epoch 00011: val_categorical_accuracy did not improve
Epoch 12/15

  128/13806 [..............................] - ETA: 56s - loss: 0.1021 - categorical_accuracy: 0.9844
  256/13806 [..............................] - ETA: 55s - loss: 0.0969 - categorical_accuracy: 0.9844
  384/13806 [..............................] - ETA: 54s - loss: 0.0986 - categorical_accuracy: 0.9844
  512/13806 [>.............................] - ETA: 53s - loss: 0.0920 - categorical_accuracy: 0.9863
  640/13806 [>.............................] - ETA: 53s - loss: 0.0874 - categorical_accuracy: 0.9891
  768/13806 [>.............................] - ETA: 53s - loss: 0.1047 - categorical_accuracy: 0.9844
  896/13806 [>.............................] - ETA: 53s - loss: 0.1155 - categorical_accuracy: 0.9844
 1024/13806 [=>............................] - ETA: 52s - loss: 0.1209 - categorical_accuracy: 0.9824
 1152/13806 [=>............................] - ETA: 51s - loss: 0.1172 - categorical_accuracy: 0.9826
 1280/13806 [=>............................] - ETA: 51s - loss: 0.1165 - categorical_accuracy: 0.9836
 1408/13806 [==>...........................] - ETA: 50s - loss: 0.1143 - categorical_accuracy: 0.9837
 1536/13806 [==>...........................] - ETA: 50s - loss: 0.1144 - categorical_accuracy: 0.9831
 1664/13806 [==>...........................] - ETA: 49s - loss: 0.1137 - categorical_accuracy: 0.9832
 1792/13806 [==>...........................] - ETA: 49s - loss: 0.1113 - categorical_accuracy: 0.9838
 1920/13806 [===>..........................] - ETA: 48s - loss: 0.1097 - categorical_accuracy: 0.9844
 2048/13806 [===>..........................] - ETA: 48s - loss: 0.1079 - categorical_accuracy: 0.9849
 2176/13806 [===>..........................] - ETA: 47s - loss: 0.1103 - categorical_accuracy: 0.9844
 2304/13806 [====>.........................] - ETA: 47s - loss: 0.1080 - categorical_accuracy: 0.9852
 2432/13806 [====>.........................] - ETA: 46s - loss: 0.1083 - categorical_accuracy: 0.9848
 2560/13806 [====>.........................] - ETA: 45s - loss: 0.1068 - categorical_accuracy: 0.9855
 2688/13806 [====>.........................] - ETA: 45s - loss: 0.1066 - categorical_accuracy: 0.9847
 2816/13806 [=====>........................] - ETA: 44s - loss: 0.1068 - categorical_accuracy: 0.9847
 2944/13806 [=====>........................] - ETA: 44s - loss: 0.1069 - categorical_accuracy: 0.9844
 3072/13806 [=====>........................] - ETA: 43s - loss: 0.1128 - categorical_accuracy: 0.9837
 3200/13806 [=====>........................] - ETA: 43s - loss: 0.1131 - categorical_accuracy: 0.9838
 3328/13806 [======>.......................] - ETA: 42s - loss: 0.1136 - categorical_accuracy: 0.9838
 3456/13806 [======>.......................] - ETA: 42s - loss: 0.1127 - categorical_accuracy: 0.9841
 3584/13806 [======>.......................] - ETA: 41s - loss: 0.1157 - categorical_accuracy: 0.9835
 3712/13806 [=======>......................] - ETA: 41s - loss: 0.1146 - categorical_accuracy: 0.9836
 3840/13806 [=======>......................] - ETA: 40s - loss: 0.1150 - categorical_accuracy: 0.9831
 3968/13806 [=======>......................] - ETA: 40s - loss: 0.1155 - categorical_accuracy: 0.9831
 4096/13806 [=======>......................] - ETA: 39s - loss: 0.1154 - categorical_accuracy: 0.9829
 4224/13806 [========>.....................] - ETA: 39s - loss: 0.1159 - categorical_accuracy: 0.9827
 4352/13806 [========>.....................] - ETA: 38s - loss: 0.1182 - categorical_accuracy: 0.9825
 4480/13806 [========>.....................] - ETA: 38s - loss: 0.1209 - categorical_accuracy: 0.9817
 4608/13806 [=========>....................] - ETA: 37s - loss: 0.1218 - categorical_accuracy: 0.9820
 4736/13806 [=========>....................] - ETA: 37s - loss: 0.1216 - categorical_accuracy: 0.9823
 4864/13806 [=========>....................] - ETA: 36s - loss: 0.1211 - categorical_accuracy: 0.9825
 4992/13806 [=========>....................] - ETA: 35s - loss: 0.1210 - categorical_accuracy: 0.9826
 5120/13806 [==========>...................] - ETA: 35s - loss: 0.1206 - categorical_accuracy: 0.9828
 5248/13806 [==========>...................] - ETA: 34s - loss: 0.1218 - categorical_accuracy: 0.9825
 5376/13806 [==========>...................] - ETA: 34s - loss: 0.1213 - categorical_accuracy: 0.9827
 5504/13806 [==========>...................] - ETA: 33s - loss: 0.1208 - categorical_accuracy: 0.9829
 5632/13806 [===========>..................] - ETA: 33s - loss: 0.1223 - categorical_accuracy: 0.9828
 5760/13806 [===========>..................] - ETA: 32s - loss: 0.1220 - categorical_accuracy: 0.9830
 5888/13806 [===========>..................] - ETA: 32s - loss: 0.1214 - categorical_accuracy: 0.9830
 6016/13806 [============>.................] - ETA: 31s - loss: 0.1209 - categorical_accuracy: 0.9832
 6144/13806 [============>.................] - ETA: 31s - loss: 0.1218 - categorical_accuracy: 0.9827
 6272/13806 [============>.................] - ETA: 30s - loss: 0.1212 - categorical_accuracy: 0.9828
 6400/13806 [============>.................] - ETA: 30s - loss: 0.1210 - categorical_accuracy: 0.9828
 6528/13806 [=============>................] - ETA: 29s - loss: 0.1213 - categorical_accuracy: 0.9828
 6656/13806 [=============>................] - ETA: 29s - loss: 0.1208 - categorical_accuracy: 0.9830
 6784/13806 [=============>................] - ETA: 28s - loss: 0.1212 - categorical_accuracy: 0.9826
 6912/13806 [==============>...............] - ETA: 28s - loss: 0.1218 - categorical_accuracy: 0.9826
 7040/13806 [==============>...............] - ETA: 27s - loss: 0.1219 - categorical_accuracy: 0.9828
 7168/13806 [==============>...............] - ETA: 27s - loss: 0.1230 - categorical_accuracy: 0.9828
 7296/13806 [==============>...............] - ETA: 26s - loss: 0.1235 - categorical_accuracy: 0.9826
 7424/13806 [===============>..............] - ETA: 26s - loss: 0.1236 - categorical_accuracy: 0.9822
 7552/13806 [===============>..............] - ETA: 25s - loss: 0.1242 - categorical_accuracy: 0.9819
 7680/13806 [===============>..............] - ETA: 25s - loss: 0.1239 - categorical_accuracy: 0.9820
 7808/13806 [===============>..............] - ETA: 24s - loss: 0.1249 - categorical_accuracy: 0.9818
 7936/13806 [================>.............] - ETA: 23s - loss: 0.1241 - categorical_accuracy: 0.9821
 8064/13806 [================>.............] - ETA: 23s - loss: 0.1234 - categorical_accuracy: 0.9824
 8192/13806 [================>.............] - ETA: 22s - loss: 0.1255 - categorical_accuracy: 0.9817
 8320/13806 [=================>............] - ETA: 22s - loss: 0.1247 - categorical_accuracy: 0.9820
 8448/13806 [=================>............] - ETA: 21s - loss: 0.1242 - categorical_accuracy: 0.9820
 8576/13806 [=================>............] - ETA: 21s - loss: 0.1239 - categorical_accuracy: 0.9820
 8704/13806 [=================>............] - ETA: 20s - loss: 0.1247 - categorical_accuracy: 0.9817
 8832/13806 [==================>...........] - ETA: 20s - loss: 0.1250 - categorical_accuracy: 0.9818
 8960/13806 [==================>...........] - ETA: 19s - loss: 0.1250 - categorical_accuracy: 0.9819
 9088/13806 [==================>...........] - ETA: 19s - loss: 0.1254 - categorical_accuracy: 0.9816
 9216/13806 [===================>..........] - ETA: 18s - loss: 0.1264 - categorical_accuracy: 0.9814
 9344/13806 [===================>..........] - ETA: 18s - loss: 0.1258 - categorical_accuracy: 0.9816
 9472/13806 [===================>..........] - ETA: 17s - loss: 0.1269 - categorical_accuracy: 0.9814
 9600/13806 [===================>..........] - ETA: 17s - loss: 0.1262 - categorical_accuracy: 0.9816
 9728/13806 [====================>.........] - ETA: 16s - loss: 0.1269 - categorical_accuracy: 0.9812
 9856/13806 [====================>.........] - ETA: 16s - loss: 0.1262 - categorical_accuracy: 0.9814
 9984/13806 [====================>.........] - ETA: 15s - loss: 0.1259 - categorical_accuracy: 0.9815
10112/13806 [====================>.........] - ETA: 15s - loss: 0.1262 - categorical_accuracy: 0.9814
10240/13806 [=====================>........] - ETA: 14s - loss: 0.1269 - categorical_accuracy: 0.9811
10368/13806 [=====================>........] - ETA: 14s - loss: 0.1265 - categorical_accuracy: 0.9812
10496/13806 [=====================>........] - ETA: 13s - loss: 0.1260 - categorical_accuracy: 0.9814
10624/13806 [======================>.......] - ETA: 12s - loss: 0.1255 - categorical_accuracy: 0.9815
10752/13806 [======================>.......] - ETA: 12s - loss: 0.1249 - categorical_accuracy: 0.9817
10880/13806 [======================>.......] - ETA: 11s - loss: 0.1249 - categorical_accuracy: 0.9815
11008/13806 [======================>.......] - ETA: 11s - loss: 0.1248 - categorical_accuracy: 0.9816
11136/13806 [=======================>......] - ETA: 10s - loss: 0.1258 - categorical_accuracy: 0.9815
11264/13806 [=======================>......] - ETA: 10s - loss: 0.1251 - categorical_accuracy: 0.9817
11392/13806 [=======================>......] - ETA: 9s - loss: 0.1253 - categorical_accuracy: 0.9818 
11520/13806 [========================>.....] - ETA: 9s - loss: 0.1251 - categorical_accuracy: 0.9819
11648/13806 [========================>.....] - ETA: 8s - loss: 0.1248 - categorical_accuracy: 0.9819
11776/13806 [========================>.....] - ETA: 8s - loss: 0.1244 - categorical_accuracy: 0.9820
11904/13806 [========================>.....] - ETA: 7s - loss: 0.1254 - categorical_accuracy: 0.9818
12032/13806 [=========================>....] - ETA: 7s - loss: 0.1250 - categorical_accuracy: 0.9819
12160/13806 [=========================>....] - ETA: 6s - loss: 0.1244 - categorical_accuracy: 0.9821
12288/13806 [=========================>....] - ETA: 6s - loss: 0.1244 - categorical_accuracy: 0.9822
12416/13806 [=========================>....] - ETA: 5s - loss: 0.1252 - categorical_accuracy: 0.9822
12544/13806 [==========================>...] - ETA: 5s - loss: 0.1252 - categorical_accuracy: 0.9822
12672/13806 [==========================>...] - ETA: 4s - loss: 0.1248 - categorical_accuracy: 0.9822
12800/13806 [==========================>...] - ETA: 4s - loss: 0.1247 - categorical_accuracy: 0.9822
12928/13806 [===========================>..] - ETA: 3s - loss: 0.1247 - categorical_accuracy: 0.9822
13056/13806 [===========================>..] - ETA: 3s - loss: 0.1245 - categorical_accuracy: 0.9822
13184/13806 [===========================>..] - ETA: 2s - loss: 0.1243 - categorical_accuracy: 0.9821
13312/13806 [===========================>..] - ETA: 2s - loss: 0.1252 - categorical_accuracy: 0.9820
13440/13806 [============================>.] - ETA: 1s - loss: 0.1255 - categorical_accuracy: 0.9819
13568/13806 [============================>.] - ETA: 0s - loss: 0.1257 - categorical_accuracy: 0.9818
13696/13806 [============================>.] - ETA: 0s - loss: 0.1263 - categorical_accuracy: 0.9817
13806/13806 [==============================] - 58s 4ms/step - loss: 0.1258 - categorical_accuracy: 0.9817 - val_loss: 1.6007 - val_categorical_accuracy: 0.5361
2018-03-23 12:29:30.424767: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:04:00.0, compute capability: 6.1)
/home/michon/anaconda2/envs/py35/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.

Epoch 00012: val_categorical_accuracy did not improve
Epoch 00012: early stopping

Final evaluation

f1_score
 0.5302301904532972
accuracy_score
 0.5361166335321405

classification_report
              precision    recall  f1-score   support

        EGY       0.52      0.55      0.54       297
        GLF       0.58      0.33      0.42       259
        LAV       0.40      0.46      0.43       327
        MSA       0.61      0.79      0.69       280
        NOR       0.61      0.54      0.57       346

avg / total       0.54      0.54      0.53      1509


confusion_matrix
 [[164   9  74  32  18]
 [ 32  86  75  39  27]
 [ 55  25 152  39  56]
 [ 12   9  21 220  18]
 [ 50  19  62  28 187]]

Evaluation on best model

f1_score
 0.5318597383869473
accuracy_score
 0.5420808482438702

classification_report
              precision    recall  f1-score   support

        EGY       0.52      0.56      0.54       297
        GLF       0.51      0.48      0.49       259
        LAV       0.46      0.28      0.35       327
        MSA       0.60      0.76      0.67       280
        NOR       0.57      0.64      0.60       346

avg / total       0.53      0.54      0.53      1509


confusion_matrix
 [[167  21  43  34  32]
 [ 37 124  31  33  34]
 [ 56  54  93  46  78]
 [ 14  17  10 214  25]
 [ 46  28  24  28 220]]
Closing remaining open files:data/vardial2018/dataset.h5...done
############# train: DONE @ Fri Mar 23 12:29:34 CET 2018
