############# train @ Fri Mar 23 14:48:21 CET 2018 GPUS=3  HOST=ssaling11 PWD=/home/michon/projects/VarDial2018/to_export/multi_input_modular
Loading data
Data Configurations loaded
Loading data
(13806, 8)
(1509, 8)
EGY    3085
LAV    2940
NOR    2866
GLF    2707
MSA    2208
Name: Class, dtype: int64
NOR    346
LAV    327
EGY    297
MSA    280
GLF    259
Name: Class, dtype: int64
Loading vocabularies
Words
48244 48244
Phones
45 45
39 39
61 61
51 51
Generating ids
Preprocessing data
Padding character sequences
(13806, 6830)
Padding phone sequences
(13806, 5885) (13806, 7329) (13806, 6436) (13806, 6837)
Turning labels in one-hot vectors
(13806, 5)
Taking ready-made acoustic embeddings
(13806, 600)
Padding character sequences
(1509, 6830)
Padding phone sequences
(1509, 5885) (1509, 7329) (1509, 6436) (1509, 6837)
Turning labels in one-hot vectors
(1509, 5)
Taking ready-made acoustic embeddings
(1509, 600)
MultiInputCharCNN Configurations loaded
Building the model
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
sent_input (InputLayer)         (None, 6830)         0                                            
__________________________________________________________________________________________________
phone_CZ_input (InputLayer)     (None, 5885)         0                                            
__________________________________________________________________________________________________
phone_EN_input (InputLayer)     (None, 7329)         0                                            
__________________________________________________________________________________________________
phone_HU_input (InputLayer)     (None, 6436)         0                                            
__________________________________________________________________________________________________
phone_RU_input (InputLayer)     (None, 6837)         0                                            
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 6830, 8)      808         sent_input[0][0]                 
__________________________________________________________________________________________________
embedding_2 (Embedding)         (None, 5885, 8)      368         phone_CZ_input[0][0]             
__________________________________________________________________________________________________
embedding_3 (Embedding)         (None, 7329, 8)      320         phone_EN_input[0][0]             
__________________________________________________________________________________________________
embedding_4 (Embedding)         (None, 6436, 8)      496         phone_HU_input[0][0]             
__________________________________________________________________________________________________
embedding_5 (Embedding)         (None, 6837, 8)      416         phone_RU_input[0][0]             
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 6830, 8)      0           embedding_1[0][0]                
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 5885, 8)      0           embedding_2[0][0]                
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 7329, 8)      0           embedding_3[0][0]                
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 6436, 8)      0           embedding_4[0][0]                
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 6837, 8)      0           embedding_5[0][0]                
__________________________________________________________________________________________________
zero_padding1d_1 (ZeroPadding1D (None, 6834, 8)      0           dropout_1[0][0]                  
__________________________________________________________________________________________________
zero_padding1d_2 (ZeroPadding1D (None, 6838, 8)      0           dropout_1[0][0]                  
__________________________________________________________________________________________________
zero_padding1d_3 (ZeroPadding1D (None, 5889, 8)      0           dropout_2[0][0]                  
__________________________________________________________________________________________________
zero_padding1d_4 (ZeroPadding1D (None, 5893, 8)      0           dropout_2[0][0]                  
__________________________________________________________________________________________________
zero_padding1d_5 (ZeroPadding1D (None, 7333, 8)      0           dropout_3[0][0]                  
__________________________________________________________________________________________________
zero_padding1d_6 (ZeroPadding1D (None, 7337, 8)      0           dropout_3[0][0]                  
__________________________________________________________________________________________________
zero_padding1d_7 (ZeroPadding1D (None, 6440, 8)      0           dropout_4[0][0]                  
__________________________________________________________________________________________________
zero_padding1d_8 (ZeroPadding1D (None, 6444, 8)      0           dropout_4[0][0]                  
__________________________________________________________________________________________________
zero_padding1d_9 (ZeroPadding1D (None, 6841, 8)      0           dropout_5[0][0]                  
__________________________________________________________________________________________________
zero_padding1d_10 (ZeroPadding1 (None, 6845, 8)      0           dropout_5[0][0]                  
__________________________________________________________________________________________________
conv1d_1 (Conv1D)               (None, 6834, 4)      100         zero_padding1d_1[0][0]           
__________________________________________________________________________________________________
conv1d_2 (Conv1D)               (None, 6838, 4)      164         zero_padding1d_2[0][0]           
__________________________________________________________________________________________________
conv1d_3 (Conv1D)               (None, 5889, 4)      100         zero_padding1d_3[0][0]           
__________________________________________________________________________________________________
conv1d_4 (Conv1D)               (None, 5893, 4)      164         zero_padding1d_4[0][0]           
__________________________________________________________________________________________________
conv1d_5 (Conv1D)               (None, 7333, 4)      100         zero_padding1d_5[0][0]           
__________________________________________________________________________________________________
conv1d_6 (Conv1D)               (None, 7337, 4)      164         zero_padding1d_6[0][0]           
__________________________________________________________________________________________________
conv1d_7 (Conv1D)               (None, 6440, 4)      100         zero_padding1d_7[0][0]           
__________________________________________________________________________________________________
conv1d_8 (Conv1D)               (None, 6444, 4)      164         zero_padding1d_8[0][0]           
__________________________________________________________________________________________________
conv1d_9 (Conv1D)               (None, 6841, 4)      100         zero_padding1d_9[0][0]           
__________________________________________________________________________________________________
conv1d_10 (Conv1D)              (None, 6845, 4)      164         zero_padding1d_10[0][0]          
__________________________________________________________________________________________________
dropout_6 (Dropout)             (None, 6834, 4)      0           conv1d_1[0][0]                   
__________________________________________________________________________________________________2018-03-23 14:48:31.702853: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-03-23 14:48:31.964267: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335
pciBusID: 0000:04:00.0
totalMemory: 7.92GiB freeMemory: 7.81GiB
2018-03-23 14:48:31.964299: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:04:00.0, compute capability: 6.1)

dropout_7 (Dropout)             (None, 6838, 4)      0           conv1d_2[0][0]                   
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 5889, 4)      0           conv1d_3[0][0]                   
__________________________________________________________________________________________________
dropout_9 (Dropout)             (None, 5893, 4)      0           conv1d_4[0][0]                   
__________________________________________________________________________________________________
dropout_10 (Dropout)            (None, 7333, 4)      0           conv1d_5[0][0]                   
__________________________________________________________________________________________________
dropout_11 (Dropout)            (None, 7337, 4)      0           conv1d_6[0][0]                   
__________________________________________________________________________________________________
dropout_12 (Dropout)            (None, 6440, 4)      0           conv1d_7[0][0]                   
__________________________________________________________________________________________________
dropout_13 (Dropout)            (None, 6444, 4)      0           conv1d_8[0][0]                   
__________________________________________________________________________________________________
dropout_14 (Dropout)            (None, 6841, 4)      0           conv1d_9[0][0]                   
__________________________________________________________________________________________________
dropout_15 (Dropout)            (None, 6845, 4)      0           conv1d_10[0][0]                  
__________________________________________________________________________________________________
global_max_pooling1d_1 (GlobalM (None, 4)            0           dropout_6[0][0]                  
__________________________________________________________________________________________________
global_max_pooling1d_2 (GlobalM (None, 4)            0           dropout_7[0][0]                  
__________________________________________________________________________________________________
global_max_pooling1d_3 (GlobalM (None, 4)            0           dropout_8[0][0]                  
__________________________________________________________________________________________________
global_max_pooling1d_4 (GlobalM (None, 4)            0           dropout_9[0][0]                  
__________________________________________________________________________________________________
global_max_pooling1d_5 (GlobalM (None, 4)            0           dropout_10[0][0]                 
__________________________________________________________________________________________________
global_max_pooling1d_6 (GlobalM (None, 4)            0           dropout_11[0][0]                 
__________________________________________________________________________________________________
global_max_pooling1d_7 (GlobalM (None, 4)            0           dropout_12[0][0]                 
__________________________________________________________________________________________________
global_max_pooling1d_8 (GlobalM (None, 4)            0           dropout_13[0][0]                 
__________________________________________________________________________________________________
global_max_pooling1d_9 (GlobalM (None, 4)            0           dropout_14[0][0]                 
__________________________________________________________________________________________________
global_max_pooling1d_10 (Global (None, 4)            0           dropout_15[0][0]                 
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 40)           0           global_max_pooling1d_1[0][0]     
                                                                 global_max_pooling1d_2[0][0]     
                                                                 global_max_pooling1d_3[0][0]     
                                                                 global_max_pooling1d_4[0][0]     
                                                                 global_max_pooling1d_5[0][0]     
                                                                 global_max_pooling1d_6[0][0]     
                                                                 global_max_pooling1d_7[0][0]     
                                                                 global_max_pooling1d_8[0][0]     
                                                                 global_max_pooling1d_9[0][0]     
                                                                 global_max_pooling1d_10[0][0]    
__________________________________________________________________________________________________
embed_input (InputLayer)        (None, 600)          0                                            
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 640)          0           concatenate_1[0][0]              
                                                                 embed_input[0][0]                
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 8)            5128        concatenate_2[0][0]              
__________________________________________________________________________________________________
dropout_16 (Dropout)            (None, 8)            0           dense_1[0][0]                    
__________________________________________________________________________________________________
l_out (Dense)                   (None, 5)            45          dropout_16[0][0]                 
==================================================================================================
Total params: 8,901
Trainable params: 8,901
Non-trainable params: 0
__________________________________________________________________________________________________
Training Configurations loaded
Training the model
no checkpoints available !
Train on 13806 samples, validate on 1509 samples
Epoch 1/15

  128/13806 [..............................] - ETA: 2:11 - loss: 1.8231 - categorical_accuracy: 0.2734
  256/13806 [..............................] - ETA: 1:15 - loss: 1.7828 - categorical_accuracy: 0.2891
  384/13806 [..............................] - ETA: 56s - loss: 1.7432 - categorical_accuracy: 0.2865 
  512/13806 [>.............................] - ETA: 47s - loss: 1.6930 - categorical_accuracy: 0.3105
  640/13806 [>.............................] - ETA: 40s - loss: 1.6759 - categorical_accuracy: 0.3109
  768/13806 [>.............................] - ETA: 36s - loss: 1.6516 - categorical_accuracy: 0.3216
  896/13806 [>.............................] - ETA: 33s - loss: 1.6276 - categorical_accuracy: 0.3315
 1024/13806 [=>............................] - ETA: 31s - loss: 1.6111 - categorical_accuracy: 0.3379
 1152/13806 [=>............................] - ETA: 29s - loss: 1.5886 - categorical_accuracy: 0.3542
 1280/13806 [=>............................] - ETA: 27s - loss: 1.5715 - categorical_accuracy: 0.3680
 1408/13806 [==>...........................] - ETA: 26s - loss: 1.5504 - categorical_accuracy: 0.3800
 1536/13806 [==>...........................] - ETA: 25s - loss: 1.5364 - categorical_accuracy: 0.3861
 1664/13806 [==>...........................] - ETA: 24s - loss: 1.5214 - categorical_accuracy: 0.3978
 1792/13806 [==>...........................] - ETA: 23s - loss: 1.5086 - categorical_accuracy: 0.4079
 1920/13806 [===>..........................] - ETA: 23s - loss: 1.4987 - categorical_accuracy: 0.4146
 2048/13806 [===>..........................] - ETA: 22s - loss: 1.4832 - categorical_accuracy: 0.4219
 2176/13806 [===>..........................] - ETA: 21s - loss: 1.4729 - categorical_accuracy: 0.4288
 2304/13806 [====>.........................] - ETA: 21s - loss: 1.4572 - categorical_accuracy: 0.4366
 2432/13806 [====>.........................] - ETA: 20s - loss: 1.4449 - categorical_accuracy: 0.4433
 2560/13806 [====>.........................] - ETA: 20s - loss: 1.4327 - categorical_accuracy: 0.4488
 2688/13806 [====>.........................] - ETA: 19s - loss: 1.4232 - categorical_accuracy: 0.4546
 2816/13806 [=====>........................] - ETA: 19s - loss: 1.4175 - categorical_accuracy: 0.4581
 2944/13806 [=====>........................] - ETA: 18s - loss: 1.4064 - categorical_accuracy: 0.4630
 3072/13806 [=====>........................] - ETA: 18s - loss: 1.3981 - categorical_accuracy: 0.4652
 3200/13806 [=====>........................] - ETA: 18s - loss: 1.3872 - categorical_accuracy: 0.4694
 3328/13806 [======>.......................] - ETA: 17s - loss: 1.3756 - categorical_accuracy: 0.4769
 3456/13806 [======>.......................] - ETA: 17s - loss: 1.3636 - categorical_accuracy: 0.4823
 3584/13806 [======>.......................] - ETA: 16s - loss: 1.3579 - categorical_accuracy: 0.4852
 3712/13806 [=======>......................] - ETA: 16s - loss: 1.3524 - categorical_accuracy: 0.4868
 3840/13806 [=======>......................] - ETA: 16s - loss: 1.3453 - categorical_accuracy: 0.4898
 3968/13806 [=======>......................] - ETA: 16s - loss: 1.3399 - categorical_accuracy: 0.4912
 4096/13806 [=======>......................] - ETA: 15s - loss: 1.3328 - categorical_accuracy: 0.4939
 4224/13806 [========>.....................] - ETA: 15s - loss: 1.3254 - categorical_accuracy: 0.4962
 4352/13806 [========>.....................] - ETA: 15s - loss: 1.3216 - categorical_accuracy: 0.4970
 4480/13806 [========>.....................] - ETA: 14s - loss: 1.3155 - categorical_accuracy: 0.4991
 4608/13806 [=========>....................] - ETA: 14s - loss: 1.3113 - categorical_accuracy: 0.5007
 4736/13806 [=========>....................] - ETA: 14s - loss: 1.3042 - categorical_accuracy: 0.5046
 4864/13806 [=========>....................] - ETA: 14s - loss: 1.2959 - categorical_accuracy: 0.5088
 4992/13806 [=========>....................] - ETA: 13s - loss: 1.2906 - categorical_accuracy: 0.5108
 5120/13806 [==========>...................] - ETA: 13s - loss: 1.2879 - categorical_accuracy: 0.5115
 5248/13806 [==========>...................] - ETA: 13s - loss: 1.2807 - categorical_accuracy: 0.5149
 5376/13806 [==========>...................] - ETA: 13s - loss: 1.2761 - categorical_accuracy: 0.5164
 5504/13806 [==========>...................] - ETA: 12s - loss: 1.2703 - categorical_accuracy: 0.5184
 5632/13806 [===========>..................] - ETA: 12s - loss: 1.2642 - categorical_accuracy: 0.5208
 5760/13806 [===========>..................] - ETA: 12s - loss: 1.2567 - categorical_accuracy: 0.5247
 5888/13806 [===========>..................] - ETA: 12s - loss: 1.2548 - categorical_accuracy: 0.5258
 6016/13806 [============>.................] - ETA: 11s - loss: 1.2482 - categorical_accuracy: 0.5293
 6144/13806 [============>.................] - ETA: 11s - loss: 1.2431 - categorical_accuracy: 0.5312
 6272/13806 [============>.................] - ETA: 11s - loss: 1.2388 - categorical_accuracy: 0.5338
 6400/13806 [============>.................] - ETA: 11s - loss: 1.2338 - categorical_accuracy: 0.5355
 6528/13806 [=============>................] - ETA: 11s - loss: 1.2321 - categorical_accuracy: 0.5362
 6656/13806 [=============>................] - ETA: 10s - loss: 1.2288 - categorical_accuracy: 0.5370
 6784/13806 [=============>................] - ETA: 10s - loss: 1.2258 - categorical_accuracy: 0.5379
 6912/13806 [==============>...............] - ETA: 10s - loss: 1.2233 - categorical_accuracy: 0.5394
 7040/13806 [==============>...............] - ETA: 10s - loss: 1.2190 - categorical_accuracy: 0.5408
 7168/13806 [==============>...............] - ETA: 10s - loss: 1.2153 - categorical_accuracy: 0.5427
 7296/13806 [==============>...............] - ETA: 9s - loss: 1.2103 - categorical_accuracy: 0.5458 
 7424/13806 [===============>..............] - ETA: 9s - loss: 1.2072 - categorical_accuracy: 0.5469
 7552/13806 [===============>..............] - ETA: 9s - loss: 1.2031 - categorical_accuracy: 0.5489
 7680/13806 [===============>..............] - ETA: 9s - loss: 1.1988 - categorical_accuracy: 0.5501
 7808/13806 [===============>..............] - ETA: 9s - loss: 1.1956 - categorical_accuracy: 0.5521
 7936/13806 [================>.............] - ETA: 8s - loss: 1.1914 - categorical_accuracy: 0.5541
 8064/13806 [================>.............] - ETA: 8s - loss: 1.1873 - categorical_accuracy: 0.5561
 8192/13806 [================>.............] - ETA: 8s - loss: 1.1834 - categorical_accuracy: 0.5575
 8320/13806 [=================>............] - ETA: 8s - loss: 1.1799 - categorical_accuracy: 0.5589
 8448/13806 [=================>............] - ETA: 8s - loss: 1.1769 - categorical_accuracy: 0.5592
 8576/13806 [=================>............] - ETA: 7s - loss: 1.1739 - categorical_accuracy: 0.5608
 8704/13806 [=================>............] - ETA: 7s - loss: 1.1716 - categorical_accuracy: 0.5617
 8832/13806 [==================>...........] - ETA: 7s - loss: 1.1684 - categorical_accuracy: 0.5624
 8960/13806 [==================>...........] - ETA: 7s - loss: 1.1660 - categorical_accuracy: 0.5638
 9088/13806 [==================>...........] - ETA: 7s - loss: 1.1639 - categorical_accuracy: 0.5647
 9216/13806 [===================>..........] - ETA: 6s - loss: 1.1613 - categorical_accuracy: 0.5661
 9344/13806 [===================>..........] - ETA: 6s - loss: 1.1587 - categorical_accuracy: 0.5664
 9472/13806 [===================>..........] - ETA: 6s - loss: 1.1568 - categorical_accuracy: 0.5664
 9600/13806 [===================>..........] - ETA: 6s - loss: 1.1554 - categorical_accuracy: 0.5664
 9728/13806 [====================>.........] - ETA: 6s - loss: 1.1530 - categorical_accuracy: 0.5670
 9856/13806 [====================>.........] - ETA: 5s - loss: 1.1503 - categorical_accuracy: 0.5676
 9984/13806 [====================>.........] - ETA: 5s - loss: 1.1481 - categorical_accuracy: 0.5683
10112/13806 [====================>.........] - ETA: 5s - loss: 1.1458 - categorical_accuracy: 0.5694
10240/13806 [=====================>........] - ETA: 5s - loss: 1.1429 - categorical_accuracy: 0.5703
10368/13806 [=====================>........] - ETA: 5s - loss: 1.1404 - categorical_accuracy: 0.5705
10496/13806 [=====================>........] - ETA: 4s - loss: 1.1384 - categorical_accuracy: 0.5713
10624/13806 [======================>.......] - ETA: 4s - loss: 1.1373 - categorical_accuracy: 0.5713
10752/13806 [======================>.......] - ETA: 4s - loss: 1.1349 - categorical_accuracy: 0.5722
10880/13806 [======================>.......] - ETA: 4s - loss: 1.1343 - categorical_accuracy: 0.5726
11008/13806 [======================>.......] - ETA: 4s - loss: 1.1331 - categorical_accuracy: 0.5724
11136/13806 [=======================>......] - ETA: 3s - loss: 1.1311 - categorical_accuracy: 0.5726
11264/13806 [=======================>......] - ETA: 3s - loss: 1.1286 - categorical_accuracy: 0.5736
11392/13806 [=======================>......] - ETA: 3s - loss: 1.1258 - categorical_accuracy: 0.5751
11520/13806 [========================>.....] - ETA: 3s - loss: 1.1231 - categorical_accuracy: 0.5764
11648/13806 [========================>.....] - ETA: 3s - loss: 1.1219 - categorical_accuracy: 0.5764
11776/13806 [========================>.....] - ETA: 2s - loss: 1.1208 - categorical_accuracy: 0.5766
11904/13806 [========================>.....] - ETA: 2s - loss: 1.1180 - categorical_accuracy: 0.5781
12032/13806 [=========================>....] - ETA: 2s - loss: 1.1161 - categorical_accuracy: 0.5783
12160/13806 [=========================>....] - ETA: 2s - loss: 1.1141 - categorical_accuracy: 0.5787
12288/13806 [=========================>....] - ETA: 2s - loss: 1.1118 - categorical_accuracy: 0.5800
12416/13806 [=========================>....] - ETA: 2s - loss: 1.1106 - categorical_accuracy: 0.5801
12544/13806 [==========================>...] - ETA: 1s - loss: 1.1091 - categorical_accuracy: 0.5801
12672/13806 [==========================>...] - ETA: 1s - loss: 1.1068 - categorical_accuracy: 0.5812
12800/13806 [==========================>...] - ETA: 1s - loss: 1.1045 - categorical_accuracy: 0.5823
12928/13806 [===========================>..] - ETA: 1s - loss: 1.1033 - categorical_accuracy: 0.5823
13056/13806 [===========================>..] - ETA: 1s - loss: 1.1014 - categorical_accuracy: 0.5830
13184/13806 [===========================>..] - ETA: 0s - loss: 1.0984 - categorical_accuracy: 0.5842
13312/13806 [===========================>..] - ETA: 0s - loss: 1.0966 - categorical_accuracy: 0.5849
13440/13806 [============================>.] - ETA: 0s - loss: 1.0956 - categorical_accuracy: 0.5853
13568/13806 [============================>.] - ETA: 0s - loss: 1.0928 - categorical_accuracy: 0.5863
13696/13806 [============================>.] - ETA: 0s - loss: 1.0912 - categorical_accuracy: 0.5870
13806/13806 [==============================] - 21s 2ms/step - loss: 1.0900 - categorical_accuracy: 0.5871 - val_loss: 1.3775 - val_categorical_accuracy: 0.5262

Epoch 00001: val_categorical_accuracy improved from -inf to 0.52618, saving model to results/vardial2018/multi_input_small_with_dropout/model_weights.hdf5
Epoch 2/15

  128/13806 [..............................] - ETA: 18s - loss: 0.9321 - categorical_accuracy: 0.6250
  256/13806 [..............................] - ETA: 18s - loss: 0.8630 - categorical_accuracy: 0.6680
  384/13806 [..............................] - ETA: 19s - loss: 0.8984 - categorical_accuracy: 0.6302
  512/13806 [>.............................] - ETA: 18s - loss: 0.8916 - categorical_accuracy: 0.6348
  640/13806 [>.............................] - ETA: 18s - loss: 0.8767 - categorical_accuracy: 0.6453
  768/13806 [>.............................] - ETA: 18s - loss: 0.8729 - categorical_accuracy: 0.6536
  896/13806 [>.............................] - ETA: 17s - loss: 0.8828 - categorical_accuracy: 0.6473
 1024/13806 [=>............................] - ETA: 17s - loss: 0.8847 - categorical_accuracy: 0.6484
 1152/13806 [=>............................] - ETA: 17s - loss: 0.8786 - categorical_accuracy: 0.6554
 1280/13806 [=>............................] - ETA: 17s - loss: 0.8912 - categorical_accuracy: 0.6500
 1408/13806 [==>...........................] - ETA: 16s - loss: 0.8834 - categorical_accuracy: 0.6477
 1536/13806 [==>...........................] - ETA: 16s - loss: 0.8855 - categorical_accuracy: 0.6471
 1664/13806 [==>...........................] - ETA: 16s - loss: 0.8859 - categorical_accuracy: 0.6478
 1792/13806 [==>...........................] - ETA: 16s - loss: 0.8917 - categorical_accuracy: 0.6445
 1920/13806 [===>..........................] - ETA: 16s - loss: 0.9003 - categorical_accuracy: 0.6375
 2048/13806 [===>..........................] - ETA: 15s - loss: 0.9041 - categorical_accuracy: 0.6338
 2176/13806 [===>..........................] - ETA: 15s - loss: 0.9013 - categorical_accuracy: 0.6342
 2304/13806 [====>.........................] - ETA: 15s - loss: 0.9029 - categorical_accuracy: 0.6332
 2432/13806 [====>.........................] - ETA: 15s - loss: 0.8991 - categorical_accuracy: 0.6365
 2560/13806 [====>.........................] - ETA: 15s - loss: 0.8958 - categorical_accuracy: 0.6371
 2688/13806 [====>.........................] - ETA: 14s - loss: 0.9020 - categorical_accuracy: 0.6358
 2816/13806 [=====>........................] - ETA: 14s - loss: 0.8997 - categorical_accuracy: 0.6374
 2944/13806 [=====>........................] - ETA: 14s - loss: 0.8995 - categorical_accuracy: 0.6396
 3072/13806 [=====>........................] - ETA: 14s - loss: 0.9027 - categorical_accuracy: 0.6383
 3200/13806 [=====>........................] - ETA: 14s - loss: 0.9011 - categorical_accuracy: 0.6400
 3328/13806 [======>.......................] - ETA: 14s - loss: 0.8975 - categorical_accuracy: 0.6418
 3456/13806 [======>.......................] - ETA: 13s - loss: 0.8969 - categorical_accuracy: 0.6427
 3584/13806 [======>.......................] - ETA: 13s - loss: 0.8956 - categorical_accuracy: 0.6443
 3712/13806 [=======>......................] - ETA: 13s - loss: 0.8954 - categorical_accuracy: 0.6444
 3840/13806 [=======>......................] - ETA: 13s - loss: 0.8964 - categorical_accuracy: 0.6438
 3968/13806 [=======>......................] - ETA: 13s - loss: 0.8930 - categorical_accuracy: 0.6452
 4096/13806 [=======>......................] - ETA: 13s - loss: 0.8938 - categorical_accuracy: 0.6450
 4224/13806 [========>.....................] - ETA: 13s - loss: 0.8909 - categorical_accuracy: 0.6456
 4352/13806 [========>.....................] - ETA: 12s - loss: 0.8911 - categorical_accuracy: 0.6445
 4480/13806 [========>.....................] - ETA: 12s - loss: 0.8907 - categorical_accuracy: 0.6455
 4608/13806 [=========>....................] - ETA: 12s - loss: 0.8899 - categorical_accuracy: 0.6454
 4736/13806 [=========>....................] - ETA: 12s - loss: 0.8905 - categorical_accuracy: 0.6451
 4864/13806 [=========>....................] - ETA: 12s - loss: 0.8909 - categorical_accuracy: 0.6445
 4992/13806 [=========>....................] - ETA: 11s - loss: 0.8891 - categorical_accuracy: 0.6448
 5120/13806 [==========>...................] - ETA: 11s - loss: 0.8866 - categorical_accuracy: 0.6451
 5248/13806 [==========>...................] - ETA: 11s - loss: 0.8853 - categorical_accuracy: 0.6448
 5376/13806 [==========>...................] - ETA: 11s - loss: 0.8848 - categorical_accuracy: 0.6440
 5504/13806 [==========>...................] - ETA: 11s - loss: 0.8836 - categorical_accuracy: 0.6452
 5632/13806 [===========>..................] - ETA: 11s - loss: 0.8846 - categorical_accuracy: 0.6452
 5760/13806 [===========>..................] - ETA: 10s - loss: 0.8833 - categorical_accuracy: 0.6462
 5888/13806 [===========>..................] - ETA: 10s - loss: 0.8832 - categorical_accuracy: 0.6457
 6016/13806 [============>.................] - ETA: 10s - loss: 0.8833 - categorical_accuracy: 0.6456
 6144/13806 [============>.................] - ETA: 10s - loss: 0.8811 - categorical_accuracy: 0.6468
 6272/13806 [============>.................] - ETA: 10s - loss: 0.8827 - categorical_accuracy: 0.6459
 6400/13806 [============>.................] - ETA: 10s - loss: 0.8817 - categorical_accuracy: 0.6464
 6528/13806 [=============>................] - ETA: 9s - loss: 0.8826 - categorical_accuracy: 0.6454 
 6656/13806 [=============>................] - ETA: 9s - loss: 0.8826 - categorical_accuracy: 0.6456
 6784/13806 [=============>................] - ETA: 9s - loss: 0.8808 - categorical_accuracy: 0.6471
 6912/13806 [==============>...............] - ETA: 9s - loss: 0.8798 - categorical_accuracy: 0.6474
 7040/13806 [==============>...............] - ETA: 9s - loss: 0.8794 - categorical_accuracy: 0.6476
 7168/13806 [==============>...............] - ETA: 9s - loss: 0.8797 - categorical_accuracy: 0.6472
 7296/13806 [==============>...............] - ETA: 8s - loss: 0.8783 - categorical_accuracy: 0.6480
 7424/13806 [===============>..............] - ETA: 8s - loss: 0.8773 - categorical_accuracy: 0.6479
 7552/13806 [===============>..............] - ETA: 8s - loss: 0.8756 - categorical_accuracy: 0.6478
 7680/13806 [===============>..............] - ETA: 8s - loss: 0.8760 - categorical_accuracy: 0.6479
 7808/13806 [===============>..............] - ETA: 8s - loss: 0.8747 - categorical_accuracy: 0.6483
 7936/13806 [================>.............] - ETA: 8s - loss: 0.8726 - categorical_accuracy: 0.6489
 8064/13806 [================>.............] - ETA: 7s - loss: 0.8739 - categorical_accuracy: 0.6487
 8192/13806 [================>.............] - ETA: 7s - loss: 0.8712 - categorical_accuracy: 0.6495
 8320/13806 [=================>............] - ETA: 7s - loss: 0.8700 - categorical_accuracy: 0.6505
 8448/13806 [=================>............] - ETA: 7s - loss: 0.8705 - categorical_accuracy: 0.6493
 8576/13806 [=================>............] - ETA: 7s - loss: 0.8698 - categorical_accuracy: 0.6493
 8704/13806 [=================>............] - ETA: 6s - loss: 0.8700 - categorical_accuracy: 0.6490
 8832/13806 [==================>...........] - ETA: 6s - loss: 0.8709 - categorical_accuracy: 0.6486
 8960/13806 [==================>...........] - ETA: 6s - loss: 0.8698 - categorical_accuracy: 0.6492
 9088/13806 [==================>...........] - ETA: 6s - loss: 0.8681 - categorical_accuracy: 0.6505
 9216/13806 [===================>..........] - ETA: 6s - loss: 0.8667 - categorical_accuracy: 0.6509
 9344/13806 [===================>..........] - ETA: 6s - loss: 0.8668 - categorical_accuracy: 0.6513
 9472/13806 [===================>..........] - ETA: 5s - loss: 0.8673 - categorical_accuracy: 0.6503
 9600/13806 [===================>..........] - ETA: 5s - loss: 0.8685 - categorical_accuracy: 0.6505
 9728/13806 [====================>.........] - ETA: 5s - loss: 0.8681 - categorical_accuracy: 0.6502
 9856/13806 [====================>.........] - ETA: 5s - loss: 0.8684 - categorical_accuracy: 0.6499
 9984/13806 [====================>.........] - ETA: 5s - loss: 0.8679 - categorical_accuracy: 0.6499
10112/13806 [====================>.........] - ETA: 5s - loss: 0.8680 - categorical_accuracy: 0.6501
10240/13806 [=====================>........] - ETA: 4s - loss: 0.8675 - categorical_accuracy: 0.6506
10368/13806 [=====================>........] - ETA: 4s - loss: 0.8667 - categorical_accuracy: 0.6509
10496/13806 [=====================>........] - ETA: 4s - loss: 0.8654 - categorical_accuracy: 0.6521
10624/13806 [======================>.......] - ETA: 4s - loss: 0.8639 - categorical_accuracy: 0.6530
10752/13806 [======================>.......] - ETA: 4s - loss: 0.8628 - categorical_accuracy: 0.6537
10880/13806 [======================>.......] - ETA: 4s - loss: 0.8614 - categorical_accuracy: 0.6545
11008/13806 [======================>.......] - ETA: 3s - loss: 0.8613 - categorical_accuracy: 0.6542
11136/13806 [=======================>......] - ETA: 3s - loss: 0.8616 - categorical_accuracy: 0.6536
11264/13806 [=======================>......] - ETA: 3s - loss: 0.8614 - categorical_accuracy: 0.6538
11392/13806 [=======================>......] - ETA: 3s - loss: 0.8613 - categorical_accuracy: 0.6539
11520/13806 [========================>.....] - ETA: 3s - loss: 0.8605 - categorical_accuracy: 0.6543
11648/13806 [========================>.....] - ETA: 2s - loss: 0.8609 - categorical_accuracy: 0.6547
11776/13806 [========================>.....] - ETA: 2s - loss: 0.8603 - categorical_accuracy: 0.6549
11904/13806 [========================>.....] - ETA: 2s - loss: 0.8608 - categorical_accuracy: 0.6547
12032/13806 [=========================>....] - ETA: 2s - loss: 0.8599 - categorical_accuracy: 0.6552
12160/13806 [=========================>....] - ETA: 2s - loss: 0.8594 - categorical_accuracy: 0.6553
12288/13806 [=========================>....] - ETA: 2s - loss: 0.8611 - categorical_accuracy: 0.6543
12416/13806 [=========================>....] - ETA: 1s - loss: 0.8611 - categorical_accuracy: 0.6542
12544/13806 [==========================>...] - ETA: 1s - loss: 0.8609 - categorical_accuracy: 0.6545
12672/13806 [==========================>...] - ETA: 1s - loss: 0.8603 - categorical_accuracy: 0.6545
12800/13806 [==========================>...] - ETA: 1s - loss: 0.8592 - categorical_accuracy: 0.6552
12928/13806 [===========================>..] - ETA: 1s - loss: 0.8592 - categorical_accuracy: 0.6556
13056/13806 [===========================>..] - ETA: 1s - loss: 0.8585 - categorical_accuracy: 0.6562
13184/13806 [===========================>..] - ETA: 0s - loss: 0.8587 - categorical_accuracy: 0.6561
13312/13806 [===========================>..] - ETA: 0s - loss: 0.8576 - categorical_accuracy: 0.6560
13440/13806 [============================>.] - ETA: 0s - loss: 0.8575 - categorical_accuracy: 0.6562
13568/13806 [============================>.] - ETA: 0s - loss: 0.8575 - categorical_accuracy: 0.6562
13696/13806 [============================>.] - ETA: 0s - loss: 0.8578 - categorical_accuracy: 0.6561
13806/13806 [==============================] - 20s 1ms/step - loss: 0.8574 - categorical_accuracy: 0.6567 - val_loss: 1.3347 - val_categorical_accuracy: 0.5242

Epoch 00002: val_categorical_accuracy did not improve
Epoch 3/15

  128/13806 [..............................] - ETA: 18s - loss: 0.8198 - categorical_accuracy: 0.6953
  256/13806 [..............................] - ETA: 18s - loss: 0.7965 - categorical_accuracy: 0.6992
  384/13806 [..............................] - ETA: 18s - loss: 0.8185 - categorical_accuracy: 0.6823
  512/13806 [>.............................] - ETA: 17s - loss: 0.8352 - categorical_accuracy: 0.6582
  640/13806 [>.............................] - ETA: 17s - loss: 0.8557 - categorical_accuracy: 0.6484
  768/13806 [>.............................] - ETA: 17s - loss: 0.8643 - categorical_accuracy: 0.6432
  896/13806 [>.............................] - ETA: 17s - loss: 0.8612 - categorical_accuracy: 0.6440
 1024/13806 [=>............................] - ETA: 17s - loss: 0.8502 - categorical_accuracy: 0.6504
 1152/13806 [=>............................] - ETA: 17s - loss: 0.8496 - categorical_accuracy: 0.6510
 1280/13806 [=>............................] - ETA: 16s - loss: 0.8408 - categorical_accuracy: 0.6562
 1408/13806 [==>...........................] - ETA: 16s - loss: 0.8447 - categorical_accuracy: 0.6506
 1536/13806 [==>...........................] - ETA: 16s - loss: 0.8345 - categorical_accuracy: 0.6517
 1664/13806 [==>...........................] - ETA: 16s - loss: 0.8424 - categorical_accuracy: 0.6526
 1792/13806 [==>...........................] - ETA: 16s - loss: 0.8401 - categorical_accuracy: 0.6501
 1920/13806 [===>..........................] - ETA: 16s - loss: 0.8372 - categorical_accuracy: 0.6526
 2048/13806 [===>..........................] - ETA: 16s - loss: 0.8369 - categorical_accuracy: 0.6528
 2176/13806 [===>..........................] - ETA: 15s - loss: 0.8347 - categorical_accuracy: 0.6530
 2304/13806 [====>.........................] - ETA: 15s - loss: 0.8348 - categorical_accuracy: 0.6515
 2432/13806 [====>.........................] - ETA: 15s - loss: 0.8271 - categorical_accuracy: 0.6542
 2560/13806 [====>.........................] - ETA: 15s - loss: 0.8224 - categorical_accuracy: 0.6590
 2688/13806 [====>.........................] - ETA: 15s - loss: 0.8225 - categorical_accuracy: 0.6603
 2816/13806 [=====>........................] - ETA: 15s - loss: 0.8284 - categorical_accuracy: 0.6591
 2944/13806 [=====>........................] - ETA: 14s - loss: 0.8285 - categorical_accuracy: 0.6590
 3072/13806 [=====>........................] - ETA: 14s - loss: 0.8308 - categorical_accuracy: 0.6582
 3200/13806 [=====>........................] - ETA: 14s - loss: 0.8313 - categorical_accuracy: 0.6584
 3328/13806 [======>.......................] - ETA: 14s - loss: 0.8297 - categorical_accuracy: 0.6569
 3456/13806 [======>.......................] - ETA: 14s - loss: 0.8330 - categorical_accuracy: 0.6539
 3584/13806 [======>.......................] - ETA: 14s - loss: 0.8288 - categorical_accuracy: 0.6560
 3712/13806 [=======>......................] - ETA: 13s - loss: 0.8268 - categorical_accuracy: 0.6565
 3840/13806 [=======>......................] - ETA: 13s - loss: 0.8229 - categorical_accuracy: 0.6581
 3968/13806 [=======>......................] - ETA: 13s - loss: 0.8214 - categorical_accuracy: 0.6585
 4096/13806 [=======>......................] - ETA: 13s - loss: 0.8210 - categorical_accuracy: 0.6592
 4224/13806 [========>.....................] - ETA: 13s - loss: 0.8222 - categorical_accuracy: 0.6596
 4352/13806 [========>.....................] - ETA: 12s - loss: 0.8224 - categorical_accuracy: 0.6599
 4480/13806 [========>.....................] - ETA: 12s - loss: 0.8204 - categorical_accuracy: 0.6614
 4608/13806 [=========>....................] - ETA: 12s - loss: 0.8216 - categorical_accuracy: 0.6595
 4736/13806 [=========>....................] - ETA: 12s - loss: 0.8190 - categorical_accuracy: 0.6617
 4864/13806 [=========>....................] - ETA: 12s - loss: 0.8165 - categorical_accuracy: 0.6628
 4992/13806 [=========>....................] - ETA: 12s - loss: 0.8167 - categorical_accuracy: 0.6627
 5120/13806 [==========>...................] - ETA: 11s - loss: 0.8118 - categorical_accuracy: 0.6650
 5248/13806 [==========>...................] - ETA: 11s - loss: 0.8118 - categorical_accuracy: 0.6662
 5376/13806 [==========>...................] - ETA: 11s - loss: 0.8125 - categorical_accuracy: 0.6654
 5504/13806 [==========>...................] - ETA: 11s - loss: 0.8108 - categorical_accuracy: 0.6661
 5632/13806 [===========>..................] - ETA: 11s - loss: 0.8127 - categorical_accuracy: 0.6657
 5760/13806 [===========>..................] - ETA: 11s - loss: 0.8116 - categorical_accuracy: 0.6663
 5888/13806 [===========>..................] - ETA: 10s - loss: 0.8097 - categorical_accuracy: 0.6668
 6016/13806 [============>.................] - ETA: 10s - loss: 0.8094 - categorical_accuracy: 0.6672
 6144/13806 [============>.................] - ETA: 10s - loss: 0.8101 - categorical_accuracy: 0.6673
 6272/13806 [============>.................] - ETA: 10s - loss: 0.8109 - categorical_accuracy: 0.6669
 6400/13806 [============>.................] - ETA: 10s - loss: 0.8089 - categorical_accuracy: 0.6678
 6528/13806 [=============>................] - ETA: 10s - loss: 0.8072 - categorical_accuracy: 0.6688
 6656/13806 [=============>................] - ETA: 9s - loss: 0.8080 - categorical_accuracy: 0.6686 
 6784/13806 [=============>................] - ETA: 9s - loss: 0.8081 - categorical_accuracy: 0.6689
 6912/13806 [==============>...............] - ETA: 9s - loss: 0.8084 - categorical_accuracy: 0.6688
 7040/13806 [==============>...............] - ETA: 9s - loss: 0.8084 - categorical_accuracy: 0.6687
 7168/13806 [==============>...............] - ETA: 9s - loss: 0.8074 - categorical_accuracy: 0.6695
 7296/13806 [==============>...............] - ETA: 8s - loss: 0.8100 - categorical_accuracy: 0.6682
 7424/13806 [===============>..............] - ETA: 8s - loss: 0.8093 - categorical_accuracy: 0.6681
 7552/13806 [===============>..............] - ETA: 8s - loss: 0.8092 - categorical_accuracy: 0.6680
 7680/13806 [===============>..............] - ETA: 8s - loss: 0.8101 - categorical_accuracy: 0.6671
 7808/13806 [===============>..............] - ETA: 8s - loss: 0.8106 - categorical_accuracy: 0.6673
 7936/13806 [================>.............] - ETA: 8s - loss: 0.8109 - categorical_accuracy: 0.6668
 8064/13806 [================>.............] - ETA: 7s - loss: 0.8118 - categorical_accuracy: 0.6667
 8192/13806 [================>.............] - ETA: 7s - loss: 0.8121 - categorical_accuracy: 0.6669
 8320/13806 [=================>............] - ETA: 7s - loss: 0.8105 - categorical_accuracy: 0.6671
 8448/13806 [=================>............] - ETA: 7s - loss: 0.8089 - categorical_accuracy: 0.6674
 8576/13806 [=================>............] - ETA: 7s - loss: 0.8084 - categorical_accuracy: 0.6680
 8704/13806 [=================>............] - ETA: 7s - loss: 0.8078 - categorical_accuracy: 0.6689
 8832/13806 [==================>...........] - ETA: 6s - loss: 0.8064 - categorical_accuracy: 0.6695
 8960/13806 [==================>...........] - ETA: 6s - loss: 0.8048 - categorical_accuracy: 0.6698
 9088/13806 [==================>...........] - ETA: 6s - loss: 0.8058 - categorical_accuracy: 0.6697
 9216/13806 [===================>..........] - ETA: 6s - loss: 0.8063 - categorical_accuracy: 0.6695
 9344/13806 [===================>..........] - ETA: 6s - loss: 0.8071 - categorical_accuracy: 0.6688
 9472/13806 [===================>..........] - ETA: 5s - loss: 0.8068 - categorical_accuracy: 0.6690
 9600/13806 [===================>..........] - ETA: 5s - loss: 0.8072 - categorical_accuracy: 0.6689
 9728/13806 [====================>.........] - ETA: 5s - loss: 0.8071 - categorical_accuracy: 0.6690
 9856/13806 [====================>.........] - ETA: 5s - loss: 0.8080 - categorical_accuracy: 0.6687
 9984/13806 [====================>.........] - ETA: 5s - loss: 0.8074 - categorical_accuracy: 0.6691
10112/13806 [====================>.........] - ETA: 5s - loss: 0.8056 - categorical_accuracy: 0.6697
10240/13806 [=====================>........] - ETA: 4s - loss: 0.8055 - categorical_accuracy: 0.6695
10368/13806 [=====================>........] - ETA: 4s - loss: 0.8037 - categorical_accuracy: 0.6696
10496/13806 [=====================>........] - ETA: 4s - loss: 0.8023 - categorical_accuracy: 0.6698
10624/13806 [======================>.......] - ETA: 4s - loss: 0.8011 - categorical_accuracy: 0.6701
10752/13806 [======================>.......] - ETA: 4s - loss: 0.8016 - categorical_accuracy: 0.6695
10880/13806 [======================>.......] - ETA: 4s - loss: 0.8022 - categorical_accuracy: 0.6695
11008/13806 [======================>.......] - ETA: 3s - loss: 0.8022 - categorical_accuracy: 0.6690
11136/13806 [=======================>......] - ETA: 3s - loss: 0.8023 - categorical_accuracy: 0.6690
11264/13806 [=======================>......] - ETA: 3s - loss: 0.8005 - categorical_accuracy: 0.6700
11392/13806 [=======================>......] - ETA: 3s - loss: 0.8013 - categorical_accuracy: 0.6691
11520/13806 [========================>.....] - ETA: 3s - loss: 0.8003 - categorical_accuracy: 0.6694
11648/13806 [========================>.....] - ETA: 2s - loss: 0.8010 - categorical_accuracy: 0.6690
11776/13806 [========================>.....] - ETA: 2s - loss: 0.8013 - categorical_accuracy: 0.6686
11904/13806 [========================>.....] - ETA: 2s - loss: 0.8020 - categorical_accuracy: 0.6685
12032/13806 [=========================>....] - ETA: 2s - loss: 0.8016 - categorical_accuracy: 0.6686
12160/13806 [=========================>....] - ETA: 2s - loss: 0.8008 - categorical_accuracy: 0.6697
12288/13806 [=========================>....] - ETA: 2s - loss: 0.7998 - categorical_accuracy: 0.6701
12416/13806 [=========================>....] - ETA: 1s - loss: 0.8006 - categorical_accuracy: 0.6700
12544/13806 [==========================>...] - ETA: 1s - loss: 0.7999 - categorical_accuracy: 0.6708
12672/13806 [==========================>...] - ETA: 1s - loss: 0.7997 - categorical_accuracy: 0.6708
12800/13806 [==========================>...] - ETA: 1s - loss: 0.7991 - categorical_accuracy: 0.6713
12928/13806 [===========================>..] - ETA: 1s - loss: 0.7991 - categorical_accuracy: 0.6711
13056/13806 [===========================>..] - ETA: 1s - loss: 0.7989 - categorical_accuracy: 0.6713
13184/13806 [===========================>..] - ETA: 0s - loss: 0.7984 - categorical_accuracy: 0.6710
13312/13806 [===========================>..] - ETA: 0s - loss: 0.7983 - categorical_accuracy: 0.6705
13440/13806 [============================>.] - ETA: 0s - loss: 0.7980 - categorical_accuracy: 0.6702
13568/13806 [============================>.] - ETA: 0s - loss: 0.7982 - categorical_accuracy: 0.6700
13696/13806 [============================>.] - ETA: 0s - loss: 0.7988 - categorical_accuracy: 0.6700
13806/13806 [==============================] - 20s 1ms/step - loss: 0.7994 - categorical_accuracy: 0.6697 - val_loss: 1.3190 - val_categorical_accuracy: 0.5255

Epoch 00003: val_categorical_accuracy did not improve
Epoch 4/15

  128/13806 [..............................] - ETA: 17s - loss: 0.7138 - categorical_accuracy: 0.7266
  256/13806 [..............................] - ETA: 17s - loss: 0.7302 - categorical_accuracy: 0.6875
  384/13806 [..............................] - ETA: 17s - loss: 0.7597 - categorical_accuracy: 0.6745
  512/13806 [>.............................] - ETA: 17s - loss: 0.7696 - categorical_accuracy: 0.6660
  640/13806 [>.............................] - ETA: 17s - loss: 0.7626 - categorical_accuracy: 0.6703
  768/13806 [>.............................] - ETA: 17s - loss: 0.7890 - categorical_accuracy: 0.6654
  896/13806 [>.............................] - ETA: 17s - loss: 0.7923 - categorical_accuracy: 0.6674
 1024/13806 [=>............................] - ETA: 17s - loss: 0.7856 - categorical_accuracy: 0.6699
 1152/13806 [=>............................] - ETA: 17s - loss: 0.7854 - categorical_accuracy: 0.6684
 1280/13806 [=>............................] - ETA: 17s - loss: 0.7943 - categorical_accuracy: 0.6656
 1408/13806 [==>...........................] - ETA: 16s - loss: 0.7908 - categorical_accuracy: 0.6690
 1536/13806 [==>...........................] - ETA: 16s - loss: 0.7894 - categorical_accuracy: 0.6706
 1664/13806 [==>...........................] - ETA: 16s - loss: 0.7926 - categorical_accuracy: 0.6689
 1792/13806 [==>...........................] - ETA: 16s - loss: 0.7951 - categorical_accuracy: 0.6685
 1920/13806 [===>..........................] - ETA: 16s - loss: 0.7958 - categorical_accuracy: 0.6687
 2048/13806 [===>..........................] - ETA: 15s - loss: 0.8002 - categorical_accuracy: 0.6680
 2176/13806 [===>..........................] - ETA: 15s - loss: 0.7967 - categorical_accuracy: 0.6673
 2304/13806 [====>.........................] - ETA: 15s - loss: 0.7953 - categorical_accuracy: 0.6671
 2432/13806 [====>.........................] - ETA: 15s - loss: 0.7947 - categorical_accuracy: 0.6702
 2560/13806 [====>.........................] - ETA: 15s - loss: 0.7936 - categorical_accuracy: 0.6695
 2688/13806 [====>.........................] - ETA: 15s - loss: 0.7921 - categorical_accuracy: 0.6704
 2816/13806 [=====>........................] - ETA: 14s - loss: 0.7871 - categorical_accuracy: 0.6712
 2944/13806 [=====>........................] - ETA: 14s - loss: 0.7818 - categorical_accuracy: 0.6743
 3072/13806 [=====>........................] - ETA: 14s - loss: 0.7834 - categorical_accuracy: 0.6732
 3200/13806 [=====>........................] - ETA: 14s - loss: 0.7846 - categorical_accuracy: 0.6725
 3328/13806 [======>.......................] - ETA: 14s - loss: 0.7850 - categorical_accuracy: 0.6713
 3456/13806 [======>.......................] - ETA: 13s - loss: 0.7873 - categorical_accuracy: 0.6698
 3584/13806 [======>.......................] - ETA: 13s - loss: 0.7860 - categorical_accuracy: 0.6708
 3712/13806 [=======>......................] - ETA: 13s - loss: 0.7909 - categorical_accuracy: 0.6678
 3840/13806 [=======>......................] - ETA: 13s - loss: 0.7918 - categorical_accuracy: 0.6659
 3968/13806 [=======>......................] - ETA: 13s - loss: 0.7921 - categorical_accuracy: 0.6666
 4096/13806 [=======>......................] - ETA: 13s - loss: 0.7896 - categorical_accuracy: 0.6665
 4224/13806 [========>.....................] - ETA: 12s - loss: 0.7900 - categorical_accuracy: 0.6652
 4352/13806 [========>.....................] - ETA: 12s - loss: 0.7914 - categorical_accuracy: 0.6645
 4480/13806 [========>.....................] - ETA: 12s - loss: 0.7897 - categorical_accuracy: 0.6670
 4608/13806 [=========>....................] - ETA: 12s - loss: 0.7913 - categorical_accuracy: 0.6678
 4736/13806 [=========>....................] - ETA: 12s - loss: 0.7922 - categorical_accuracy: 0.6670
 4864/13806 [=========>....................] - ETA: 12s - loss: 0.7901 - categorical_accuracy: 0.6684
 4992/13806 [=========>....................] - ETA: 11s - loss: 0.7920 - categorical_accuracy: 0.6679
 5120/13806 [==========>...................] - ETA: 11s - loss: 0.7944 - categorical_accuracy: 0.6662
 5248/13806 [==========>...................] - ETA: 11s - loss: 0.7933 - categorical_accuracy: 0.6671
 5376/13806 [==========>...................] - ETA: 11s - loss: 0.7937 - categorical_accuracy: 0.6683
 5504/13806 [==========>...................] - ETA: 11s - loss: 0.7932 - categorical_accuracy: 0.6679
 5632/13806 [===========>..................] - ETA: 11s - loss: 0.7961 - categorical_accuracy: 0.6660
 5760/13806 [===========>..................] - ETA: 10s - loss: 0.7980 - categorical_accuracy: 0.6648
 5888/13806 [===========>..................] - ETA: 10s - loss: 0.7999 - categorical_accuracy: 0.6641
 6016/13806 [============>.................] - ETA: 10s - loss: 0.7999 - categorical_accuracy: 0.6642
 6144/13806 [============>.................] - ETA: 10s - loss: 0.7977 - categorical_accuracy: 0.6650
 6272/13806 [============>.................] - ETA: 10s - loss: 0.7988 - categorical_accuracy: 0.6642
 6400/13806 [============>.................] - ETA: 10s - loss: 0.7971 - categorical_accuracy: 0.6645
 6528/13806 [=============>................] - ETA: 9s - loss: 0.7981 - categorical_accuracy: 0.6641 
 6656/13806 [=============>................] - ETA: 9s - loss: 0.7961 - categorical_accuracy: 0.6650
 6784/13806 [=============>................] - ETA: 9s - loss: 0.7957 - categorical_accuracy: 0.6660
 6912/13806 [==============>...............] - ETA: 9s - loss: 0.7952 - categorical_accuracy: 0.6670
 7040/13806 [==============>...............] - ETA: 9s - loss: 0.7958 - categorical_accuracy: 0.6669
 7168/13806 [==============>...............] - ETA: 9s - loss: 0.7964 - categorical_accuracy: 0.6660
 7296/13806 [==============>...............] - ETA: 8s - loss: 0.7970 - categorical_accuracy: 0.6663
 7424/13806 [===============>..............] - ETA: 8s - loss: 0.7945 - categorical_accuracy: 0.6674
 7552/13806 [===============>..............] - ETA: 8s - loss: 0.7927 - categorical_accuracy: 0.6675
 7680/13806 [===============>..............] - ETA: 8s - loss: 0.7923 - categorical_accuracy: 0.6681
 7808/13806 [===============>..............] - ETA: 8s - loss: 0.7912 - categorical_accuracy: 0.6692
 7936/13806 [================>.............] - ETA: 8s - loss: 0.7917 - categorical_accuracy: 0.6689
 8064/13806 [================>.............] - ETA: 7s - loss: 0.7928 - categorical_accuracy: 0.6679
 8192/13806 [================>.............] - ETA: 7s - loss: 0.7920 - categorical_accuracy: 0.6683
 8320/13806 [=================>............] - ETA: 7s - loss: 0.7927 - categorical_accuracy: 0.6677
 8448/13806 [=================>............] - ETA: 7s - loss: 0.7911 - categorical_accuracy: 0.6683
 8576/13806 [=================>............] - ETA: 7s - loss: 0.7908 - categorical_accuracy: 0.6680
 8704/13806 [=================>............] - ETA: 6s - loss: 0.7934 - categorical_accuracy: 0.6662
 8832/13806 [==================>...........] - ETA: 6s - loss: 0.7929 - categorical_accuracy: 0.6663
 8960/13806 [==================>...........] - ETA: 6s - loss: 0.7904 - categorical_accuracy: 0.6680
 9088/13806 [==================>...........] - ETA: 6s - loss: 0.7903 - categorical_accuracy: 0.6678
 9216/13806 [===================>..........] - ETA: 6s - loss: 0.7884 - categorical_accuracy: 0.6688
 9344/13806 [===================>..........] - ETA: 6s - loss: 0.7869 - categorical_accuracy: 0.6695
 9472/13806 [===================>..........] - ETA: 5s - loss: 0.7870 - categorical_accuracy: 0.6696
 9600/13806 [===================>..........] - ETA: 5s - loss: 0.7871 - categorical_accuracy: 0.6697
 9728/13806 [====================>.........] - ETA: 5s - loss: 0.7865 - categorical_accuracy: 0.6699
 9856/13806 [====================>.........] - ETA: 5s - loss: 0.7863 - categorical_accuracy: 0.6698
 9984/13806 [====================>.........] - ETA: 5s - loss: 0.7849 - categorical_accuracy: 0.6707
10112/13806 [====================>.........] - ETA: 5s - loss: 0.7851 - categorical_accuracy: 0.6705
10240/13806 [=====================>........] - ETA: 4s - loss: 0.7848 - categorical_accuracy: 0.6706
10368/13806 [=====================>........] - ETA: 4s - loss: 0.7846 - categorical_accuracy: 0.6707
10496/13806 [=====================>........] - ETA: 4s - loss: 0.7835 - categorical_accuracy: 0.6705
10624/13806 [======================>.......] - ETA: 4s - loss: 0.7830 - categorical_accuracy: 0.6707
10752/13806 [======================>.......] - ETA: 4s - loss: 0.7832 - categorical_accuracy: 0.6699
10880/13806 [======================>.......] - ETA: 3s - loss: 0.7827 - categorical_accuracy: 0.6701
11008/13806 [======================>.......] - ETA: 3s - loss: 0.7836 - categorical_accuracy: 0.6692
11136/13806 [=======================>......] - ETA: 3s - loss: 0.7825 - categorical_accuracy: 0.6697
11264/13806 [=======================>......] - ETA: 3s - loss: 0.7836 - categorical_accuracy: 0.6686
11392/13806 [=======================>......] - ETA: 3s - loss: 0.7844 - categorical_accuracy: 0.6685
11520/13806 [========================>.....] - ETA: 3s - loss: 0.7853 - categorical_accuracy: 0.6681
11648/13806 [========================>.....] - ETA: 2s - loss: 0.7851 - categorical_accuracy: 0.6680
11776/13806 [========================>.....] - ETA: 2s - loss: 0.7856 - categorical_accuracy: 0.6674
11904/13806 [========================>.....] - ETA: 2s - loss: 0.7841 - categorical_accuracy: 0.6676
12032/13806 [=========================>....] - ETA: 2s - loss: 0.7838 - categorical_accuracy: 0.6676
12160/13806 [=========================>....] - ETA: 2s - loss: 0.7833 - categorical_accuracy: 0.6681
12288/13806 [=========================>....] - ETA: 2s - loss: 0.7823 - categorical_accuracy: 0.6687
12416/13806 [=========================>....] - ETA: 1s - loss: 0.7820 - categorical_accuracy: 0.6694
12544/13806 [==========================>...] - ETA: 1s - loss: 0.7813 - categorical_accuracy: 0.6699
12672/13806 [==========================>...] - ETA: 1s - loss: 0.7811 - categorical_accuracy: 0.6699
12800/13806 [==========================>...] - ETA: 1s - loss: 0.7813 - categorical_accuracy: 0.6694
12928/13806 [===========================>..] - ETA: 1s - loss: 0.7804 - categorical_accuracy: 0.6697
13056/13806 [===========================>..] - ETA: 1s - loss: 0.7805 - categorical_accuracy: 0.6698
13184/13806 [===========================>..] - ETA: 0s - loss: 0.7800 - categorical_accuracy: 0.6703
13312/13806 [===========================>..] - ETA: 0s - loss: 0.7798 - categorical_accuracy: 0.6702
13440/13806 [============================>.] - ETA: 0s - loss: 0.7799 - categorical_accuracy: 0.6704
13568/13806 [============================>.] - ETA: 0s - loss: 0.7795 - categorical_accuracy: 0.6708
13696/13806 [============================>.] - ETA: 0s - loss: 0.7795 - categorical_accuracy: 0.6709
13806/13806 [==============================] - 20s 1ms/step - loss: 0.7788 - categorical_accuracy: 0.6712 - val_loss: 1.3130 - val_categorical_accuracy: 0.5176

Epoch 00004: val_categorical_accuracy did not improve
Epoch 5/15

  128/13806 [..............................] - ETA: 18s - loss: 0.8117 - categorical_accuracy: 0.6797
  256/13806 [..............................] - ETA: 18s - loss: 0.7612 - categorical_accuracy: 0.7031
  384/13806 [..............................] - ETA: 17s - loss: 0.7027 - categorical_accuracy: 0.7135
  512/13806 [>.............................] - ETA: 17s - loss: 0.7114 - categorical_accuracy: 0.6973
  640/13806 [>.............................] - ETA: 17s - loss: 0.7240 - categorical_accuracy: 0.6922
  768/13806 [>.............................] - ETA: 17s - loss: 0.7177 - categorical_accuracy: 0.6992
  896/13806 [>.............................] - ETA: 17s - loss: 0.7280 - categorical_accuracy: 0.6953
 1024/13806 [=>............................] - ETA: 17s - loss: 0.7333 - categorical_accuracy: 0.6943
 1152/13806 [=>............................] - ETA: 17s - loss: 0.7217 - categorical_accuracy: 0.6962
 1280/13806 [=>............................] - ETA: 16s - loss: 0.7236 - categorical_accuracy: 0.6992
 1408/13806 [==>...........................] - ETA: 16s - loss: 0.7279 - categorical_accuracy: 0.6953
 1536/13806 [==>...........................] - ETA: 17s - loss: 0.7322 - categorical_accuracy: 0.6940
 1664/13806 [==>...........................] - ETA: 16s - loss: 0.7349 - categorical_accuracy: 0.6887
 1792/13806 [==>...........................] - ETA: 16s - loss: 0.7367 - categorical_accuracy: 0.6853
 1920/13806 [===>..........................] - ETA: 16s - loss: 0.7443 - categorical_accuracy: 0.6797
 2048/13806 [===>..........................] - ETA: 16s - loss: 0.7452 - categorical_accuracy: 0.6802
 2176/13806 [===>..........................] - ETA: 16s - loss: 0.7437 - categorical_accuracy: 0.6811
 2304/13806 [====>.........................] - ETA: 15s - loss: 0.7435 - categorical_accuracy: 0.6801
 2432/13806 [====>.........................] - ETA: 15s - loss: 0.7399 - categorical_accuracy: 0.6846
 2560/13806 [====>.........................] - ETA: 15s - loss: 0.7416 - categorical_accuracy: 0.6840
 2688/13806 [====>.........................] - ETA: 15s - loss: 0.7430 - categorical_accuracy: 0.6827
 2816/13806 [=====>........................] - ETA: 14s - loss: 0.7444 - categorical_accuracy: 0.6832
 2944/13806 [=====>........................] - ETA: 14s - loss: 0.7480 - categorical_accuracy: 0.6810
 3072/13806 [=====>........................] - ETA: 14s - loss: 0.7480 - categorical_accuracy: 0.6829
 3200/13806 [=====>........................] - ETA: 14s - loss: 0.7481 - categorical_accuracy: 0.6828
 3328/13806 [======>.......................] - ETA: 14s - loss: 0.7483 - categorical_accuracy: 0.6836
 3456/13806 [======>.......................] - ETA: 14s - loss: 0.7481 - categorical_accuracy: 0.6829
 3584/13806 [======>.......................] - ETA: 13s - loss: 0.7518 - categorical_accuracy: 0.6811
 3712/13806 [=======>......................] - ETA: 13s - loss: 0.7493 - categorical_accuracy: 0.6827
 3840/13806 [=======>......................] - ETA: 13s - loss: 0.7512 - categorical_accuracy: 0.6826
 3968/13806 [=======>......................] - ETA: 13s - loss: 0.7525 - categorical_accuracy: 0.6812
 4096/13806 [=======>......................] - ETA: 13s - loss: 0.7535 - categorical_accuracy: 0.6802
 4224/13806 [========>.....................] - ETA: 13s - loss: 0.7539 - categorical_accuracy: 0.6792
 4352/13806 [========>.....................] - ETA: 13s - loss: 0.7535 - categorical_accuracy: 0.6792
 4480/13806 [========>.....................] - ETA: 12s - loss: 0.7522 - categorical_accuracy: 0.6790
 4608/13806 [=========>....................] - ETA: 12s - loss: 0.7540 - categorical_accuracy: 0.6771
 4736/13806 [=========>....................] - ETA: 12s - loss: 0.7549 - categorical_accuracy: 0.6774
 4864/13806 [=========>....................] - ETA: 12s - loss: 0.7543 - categorical_accuracy: 0.6768
 4992/13806 [=========>....................] - ETA: 12s - loss: 0.7552 - categorical_accuracy: 0.6759
 5120/13806 [==========>...................] - ETA: 11s - loss: 0.7553 - categorical_accuracy: 0.6760
 5248/13806 [==========>...................] - ETA: 11s - loss: 0.7565 - categorical_accuracy: 0.6759
 5376/13806 [==========>...................] - ETA: 11s - loss: 0.7562 - categorical_accuracy: 0.6769
 5504/13806 [==========>...................] - ETA: 11s - loss: 0.7571 - categorical_accuracy: 0.6764
 5632/13806 [===========>..................] - ETA: 11s - loss: 0.7577 - categorical_accuracy: 0.6754
 5760/13806 [===========>..................] - ETA: 11s - loss: 0.7563 - categorical_accuracy: 0.6755
 5888/13806 [===========>..................] - ETA: 10s - loss: 0.7572 - categorical_accuracy: 0.6749
 6016/13806 [============>.................] - ETA: 10s - loss: 0.7566 - categorical_accuracy: 0.6757
 6144/13806 [============>.................] - ETA: 10s - loss: 0.7556 - categorical_accuracy: 0.6769
 6272/13806 [============>.................] - ETA: 10s - loss: 0.7545 - categorical_accuracy: 0.6771
 6400/13806 [============>.................] - ETA: 10s - loss: 0.7556 - categorical_accuracy: 0.6755
 6528/13806 [=============>................] - ETA: 9s - loss: 0.7566 - categorical_accuracy: 0.6754 
 6656/13806 [=============>................] - ETA: 9s - loss: 0.7564 - categorical_accuracy: 0.6762
 6784/13806 [=============>................] - ETA: 9s - loss: 0.7572 - categorical_accuracy: 0.6759
 6912/13806 [==============>...............] - ETA: 9s - loss: 0.7568 - categorical_accuracy: 0.6769
 7040/13806 [==============>...............] - ETA: 9s - loss: 0.7560 - categorical_accuracy: 0.6778
 7168/13806 [==============>...............] - ETA: 9s - loss: 0.7572 - categorical_accuracy: 0.6783
 7296/13806 [==============>...............] - ETA: 8s - loss: 0.7572 - categorical_accuracy: 0.6790
 7424/13806 [===============>..............] - ETA: 8s - loss: 0.7566 - categorical_accuracy: 0.6800
 7552/13806 [===============>..............] - ETA: 8s - loss: 0.7576 - categorical_accuracy: 0.6797
 7680/13806 [===============>..............] - ETA: 8s - loss: 0.7583 - categorical_accuracy: 0.6784
 7808/13806 [===============>..............] - ETA: 8s - loss: 0.7577 - categorical_accuracy: 0.6785
 7936/13806 [================>.............] - ETA: 8s - loss: 0.7569 - categorical_accuracy: 0.6787
 8064/13806 [================>.............] - ETA: 7s - loss: 0.7561 - categorical_accuracy: 0.6794
 8192/13806 [================>.............] - ETA: 7s - loss: 0.7559 - categorical_accuracy: 0.6790
 8320/13806 [=================>............] - ETA: 7s - loss: 0.7548 - categorical_accuracy: 0.6796
 8448/13806 [=================>............] - ETA: 7s - loss: 0.7547 - categorical_accuracy: 0.6804
 8576/13806 [=================>............] - ETA: 7s - loss: 0.7534 - categorical_accuracy: 0.6809
 8704/13806 [=================>............] - ETA: 6s - loss: 0.7537 - categorical_accuracy: 0.6808
 8832/13806 [==================>...........] - ETA: 6s - loss: 0.7531 - categorical_accuracy: 0.6815
 8960/13806 [==================>...........] - ETA: 6s - loss: 0.7524 - categorical_accuracy: 0.6811
 9088/13806 [==================>...........] - ETA: 6s - loss: 0.7520 - categorical_accuracy: 0.6811
 9216/13806 [===================>..........] - ETA: 6s - loss: 0.7519 - categorical_accuracy: 0.6809
 9344/13806 [===================>..........] - ETA: 6s - loss: 0.7507 - categorical_accuracy: 0.6817
 9472/13806 [===================>..........] - ETA: 5s - loss: 0.7506 - categorical_accuracy: 0.6826
 9600/13806 [===================>..........] - ETA: 5s - loss: 0.7504 - categorical_accuracy: 0.6822
 9728/13806 [====================>.........] - ETA: 5s - loss: 0.7505 - categorical_accuracy: 0.6825
 9856/13806 [====================>.........] - ETA: 5s - loss: 0.7496 - categorical_accuracy: 0.6824
 9984/13806 [====================>.........] - ETA: 5s - loss: 0.7494 - categorical_accuracy: 0.6823
10112/13806 [====================>.........] - ETA: 5s - loss: 0.7501 - categorical_accuracy: 0.6822
10240/13806 [=====================>........] - ETA: 4s - loss: 0.7497 - categorical_accuracy: 0.6821
10368/13806 [=====================>........] - ETA: 4s - loss: 0.7516 - categorical_accuracy: 0.6809
10496/13806 [=====================>........] - ETA: 4s - loss: 0.7514 - categorical_accuracy: 0.6807
10624/13806 [======================>.......] - ETA: 4s - loss: 0.7508 - categorical_accuracy: 0.6815
10752/13806 [======================>.......] - ETA: 4s - loss: 0.7505 - categorical_accuracy: 0.6819
10880/13806 [======================>.......] - ETA: 3s - loss: 0.7517 - categorical_accuracy: 0.6813
11008/13806 [======================>.......] - ETA: 3s - loss: 0.7520 - categorical_accuracy: 0.6811
11136/13806 [=======================>......] - ETA: 3s - loss: 0.7512 - categorical_accuracy: 0.6819
11264/13806 [=======================>......] - ETA: 3s - loss: 0.7500 - categorical_accuracy: 0.6825
11392/13806 [=======================>......] - ETA: 3s - loss: 0.7511 - categorical_accuracy: 0.6821
11520/13806 [========================>.....] - ETA: 3s - loss: 0.7513 - categorical_accuracy: 0.6817
11648/13806 [========================>.....] - ETA: 2s - loss: 0.7510 - categorical_accuracy: 0.6817
11776/13806 [========================>.....] - ETA: 2s - loss: 0.7500 - categorical_accuracy: 0.6822
11904/13806 [========================>.....] - ETA: 2s - loss: 0.7506 - categorical_accuracy: 0.6817
12032/13806 [=========================>....] - ETA: 2s - loss: 0.7508 - categorical_accuracy: 0.6815
12160/13806 [=========================>....] - ETA: 2s - loss: 0.7501 - categorical_accuracy: 0.6823
12288/13806 [=========================>....] - ETA: 2s - loss: 0.7504 - categorical_accuracy: 0.6823
12416/13806 [=========================>....] - ETA: 1s - loss: 0.7512 - categorical_accuracy: 0.6821
12544/13806 [==========================>...] - ETA: 1s - loss: 0.7516 - categorical_accuracy: 0.6818
12672/13806 [==========================>...] - ETA: 1s - loss: 0.7513 - categorical_accuracy: 0.6820
12800/13806 [==========================>...] - ETA: 1s - loss: 0.7525 - categorical_accuracy: 0.6811
12928/13806 [===========================>..] - ETA: 1s - loss: 0.7526 - categorical_accuracy: 0.6808
13056/13806 [===========================>..] - ETA: 1s - loss: 0.7534 - categorical_accuracy: 0.6803
13184/13806 [===========================>..] - ETA: 0s - loss: 0.7540 - categorical_accuracy: 0.6807
13312/13806 [===========================>..] - ETA: 0s - loss: 0.7543 - categorical_accuracy: 0.6807
13440/13806 [============================>.] - ETA: 0s - loss: 0.7545 - categorical_accuracy: 0.6808
13568/13806 [============================>.] - ETA: 0s - loss: 0.7542 - categorical_accuracy: 0.6812
13696/13806 [============================>.] - ETA: 0s - loss: 0.7533 - categorical_accuracy: 0.6814
13806/13806 [==============================] - 20s 1ms/step - loss: 0.7535 - categorical_accuracy: 0.6809 - val_loss: 1.3110 - val_categorical_accuracy: 0.5083

Epoch 00005: val_categorical_accuracy did not improve
Epoch 6/15

  128/13806 [..............................] - ETA: 18s - loss: 0.6936 - categorical_accuracy: 0.7344
  256/13806 [..............................] - ETA: 18s - loss: 0.7319 - categorical_accuracy: 0.7148
  384/13806 [..............................] - ETA: 18s - loss: 0.7418 - categorical_accuracy: 0.6953
  512/13806 [>.............................] - ETA: 18s - loss: 0.7574 - categorical_accuracy: 0.6895
  640/13806 [>.............................] - ETA: 18s - loss: 0.7349 - categorical_accuracy: 0.7000
  768/13806 [>.............................] - ETA: 17s - loss: 0.7465 - categorical_accuracy: 0.6966
  896/13806 [>.............................] - ETA: 17s - loss: 0.7440 - categorical_accuracy: 0.6953
 1024/13806 [=>............................] - ETA: 17s - loss: 0.7559 - categorical_accuracy: 0.6924
 1152/13806 [=>............................] - ETA: 17s - loss: 0.7509 - categorical_accuracy: 0.6962
 1280/13806 [=>............................] - ETA: 17s - loss: 0.7474 - categorical_accuracy: 0.6984
 1408/13806 [==>...........................] - ETA: 17s - loss: 0.7434 - categorical_accuracy: 0.6974
 1536/13806 [==>...........................] - ETA: 16s - loss: 0.7420 - categorical_accuracy: 0.6973
 1664/13806 [==>...........................] - ETA: 16s - loss: 0.7458 - categorical_accuracy: 0.6959
 1792/13806 [==>...........................] - ETA: 16s - loss: 0.7503 - categorical_accuracy: 0.6936
 1920/13806 [===>..........................] - ETA: 16s - loss: 0.7521 - categorical_accuracy: 0.6927
 2048/13806 [===>..........................] - ETA: 16s - loss: 0.7515 - categorical_accuracy: 0.6919
 2176/13806 [===>..........................] - ETA: 15s - loss: 0.7503 - categorical_accuracy: 0.6916
 2304/13806 [====>.........................] - ETA: 15s - loss: 0.7570 - categorical_accuracy: 0.6871
 2432/13806 [====>.........................] - ETA: 15s - loss: 0.7601 - categorical_accuracy: 0.6871
 2560/13806 [====>.........................] - ETA: 15s - loss: 0.7636 - categorical_accuracy: 0.6871
 2688/13806 [====>.........................] - ETA: 15s - loss: 0.7668 - categorical_accuracy: 0.6849
 2816/13806 [=====>........................] - ETA: 14s - loss: 0.7635 - categorical_accuracy: 0.6839
 2944/13806 [=====>........................] - ETA: 14s - loss: 0.7628 - categorical_accuracy: 0.6844
 3072/13806 [=====>........................] - ETA: 14s - loss: 0.7618 - categorical_accuracy: 0.6855
 3200/13806 [=====>........................] - ETA: 14s - loss: 0.7602 - categorical_accuracy: 0.6859
 3328/13806 [======>.......................] - ETA: 14s - loss: 0.7637 - categorical_accuracy: 0.6869
 3456/13806 [======>.......................] - ETA: 14s - loss: 0.7607 - categorical_accuracy: 0.6889
 3584/13806 [======>.......................] - ETA: 13s - loss: 0.7633 - categorical_accuracy: 0.6869
 3712/13806 [=======>......................] - ETA: 13s - loss: 0.7626 - categorical_accuracy: 0.6862
 3840/13806 [=======>......................] - ETA: 13s - loss: 0.7624 - categorical_accuracy: 0.6839
 3968/13806 [=======>......................] - ETA: 13s - loss: 0.7633 - categorical_accuracy: 0.6837
 4096/13806 [=======>......................] - ETA: 13s - loss: 0.7598 - categorical_accuracy: 0.6843
 4224/13806 [========>.....................] - ETA: 13s - loss: 0.7579 - categorical_accuracy: 0.6839
 4352/13806 [========>.....................] - ETA: 12s - loss: 0.7539 - categorical_accuracy: 0.6864
 4480/13806 [========>.....................] - ETA: 12s - loss: 0.7549 - categorical_accuracy: 0.6862
 4608/13806 [=========>....................] - ETA: 12s - loss: 0.7574 - categorical_accuracy: 0.6840
 4736/13806 [=========>....................] - ETA: 12s - loss: 0.7588 - categorical_accuracy: 0.6831
 4864/13806 [=========>....................] - ETA: 12s - loss: 0.7593 - categorical_accuracy: 0.6824
 4992/13806 [=========>....................] - ETA: 11s - loss: 0.7587 - categorical_accuracy: 0.6827
 5120/13806 [==========>...................] - ETA: 11s - loss: 0.7604 - categorical_accuracy: 0.6814
 5248/13806 [==========>...................] - ETA: 11s - loss: 0.7598 - categorical_accuracy: 0.6804
 5376/13806 [==========>...................] - ETA: 11s - loss: 0.7595 - categorical_accuracy: 0.6808
 5504/13806 [==========>...................] - ETA: 11s - loss: 0.7583 - categorical_accuracy: 0.6820
 5632/13806 [===========>..................] - ETA: 11s - loss: 0.7603 - categorical_accuracy: 0.6815
 5760/13806 [===========>..................] - ETA: 10s - loss: 0.7590 - categorical_accuracy: 0.6807
 5888/13806 [===========>..................] - ETA: 10s - loss: 0.7587 - categorical_accuracy: 0.6814
 6016/13806 [============>.................] - ETA: 10s - loss: 0.7576 - categorical_accuracy: 0.6810
 6144/13806 [============>.................] - ETA: 10s - loss: 0.7590 - categorical_accuracy: 0.6799
 6272/13806 [============>.................] - ETA: 10s - loss: 0.7603 - categorical_accuracy: 0.6800
 6400/13806 [============>.................] - ETA: 10s - loss: 0.7603 - categorical_accuracy: 0.6797
 6528/13806 [=============>................] - ETA: 9s - loss: 0.7589 - categorical_accuracy: 0.6795 
 6656/13806 [=============>................] - ETA: 9s - loss: 0.7591 - categorical_accuracy: 0.6791
 6784/13806 [=============>................] - ETA: 9s - loss: 0.7597 - categorical_accuracy: 0.6788
 6912/13806 [==============>...............] - ETA: 9s - loss: 0.7591 - categorical_accuracy: 0.6795
 7040/13806 [==============>...............] - ETA: 9s - loss: 0.7597 - categorical_accuracy: 0.6793
 7168/13806 [==============>...............] - ETA: 9s - loss: 0.7579 - categorical_accuracy: 0.6804
 7296/13806 [==============>...............] - ETA: 8s - loss: 0.7567 - categorical_accuracy: 0.6804
 7424/13806 [===============>..............] - ETA: 8s - loss: 0.7563 - categorical_accuracy: 0.6801
 7552/13806 [===============>..............] - ETA: 8s - loss: 0.7571 - categorical_accuracy: 0.6797
 7680/13806 [===============>..............] - ETA: 8s - loss: 0.7575 - categorical_accuracy: 0.6798
 7808/13806 [===============>..............] - ETA: 8s - loss: 0.7604 - categorical_accuracy: 0.6793
 7936/13806 [================>.............] - ETA: 8s - loss: 0.7597 - categorical_accuracy: 0.6802
 8064/13806 [================>.............] - ETA: 7s - loss: 0.7578 - categorical_accuracy: 0.6809
 8192/13806 [================>.............] - ETA: 7s - loss: 0.7581 - categorical_accuracy: 0.6797
 8320/13806 [=================>............] - ETA: 7s - loss: 0.7584 - categorical_accuracy: 0.6793
 8448/13806 [=================>............] - ETA: 7s - loss: 0.7571 - categorical_accuracy: 0.6803
 8576/13806 [=================>............] - ETA: 7s - loss: 0.7576 - categorical_accuracy: 0.6799
 8704/13806 [=================>............] - ETA: 7s - loss: 0.7585 - categorical_accuracy: 0.6797
 8832/13806 [==================>...........] - ETA: 6s - loss: 0.7578 - categorical_accuracy: 0.6799
 8960/13806 [==================>...........] - ETA: 6s - loss: 0.7561 - categorical_accuracy: 0.6799
 9088/13806 [==================>...........] - ETA: 6s - loss: 0.7548 - categorical_accuracy: 0.6803
 9216/13806 [===================>..........] - ETA: 6s - loss: 0.7568 - categorical_accuracy: 0.6782
 9344/13806 [===================>..........] - ETA: 6s - loss: 0.7564 - categorical_accuracy: 0.6786
 9472/13806 [===================>..........] - ETA: 5s - loss: 0.7566 - categorical_accuracy: 0.6782
 9600/13806 [===================>..........] - ETA: 5s - loss: 0.7564 - categorical_accuracy: 0.6779
 9728/13806 [====================>.........] - ETA: 5s - loss: 0.7553 - categorical_accuracy: 0.6781
 9856/13806 [====================>.........] - ETA: 5s - loss: 0.7548 - categorical_accuracy: 0.6787
 9984/13806 [====================>.........] - ETA: 5s - loss: 0.7545 - categorical_accuracy: 0.6792
10112/13806 [====================>.........] - ETA: 5s - loss: 0.7540 - categorical_accuracy: 0.6796
10240/13806 [=====================>........] - ETA: 4s - loss: 0.7544 - categorical_accuracy: 0.6788
10368/13806 [=====================>........] - ETA: 4s - loss: 0.7537 - categorical_accuracy: 0.6792
10496/13806 [=====================>........] - ETA: 4s - loss: 0.7538 - categorical_accuracy: 0.6788
10624/13806 [======================>.......] - ETA: 4s - loss: 0.7531 - categorical_accuracy: 0.6793
10752/13806 [======================>.......] - ETA: 4s - loss: 0.7532 - categorical_accuracy: 0.6795
10880/13806 [======================>.......] - ETA: 4s - loss: 0.7530 - categorical_accuracy: 0.6793
11008/13806 [======================>.......] - ETA: 3s - loss: 0.7527 - categorical_accuracy: 0.6801
11136/13806 [=======================>......] - ETA: 3s - loss: 0.7502 - categorical_accuracy: 0.6814
11264/13806 [=======================>......] - ETA: 3s - loss: 0.7504 - categorical_accuracy: 0.6809
11392/13806 [=======================>......] - ETA: 3s - loss: 0.7514 - categorical_accuracy: 0.6807
11520/13806 [========================>.....] - ETA: 3s - loss: 0.7520 - categorical_accuracy: 0.6811
11648/13806 [========================>.....] - ETA: 2s - loss: 0.7511 - categorical_accuracy: 0.6816
11776/13806 [========================>.....] - ETA: 2s - loss: 0.7507 - categorical_accuracy: 0.6816
11904/13806 [========================>.....] - ETA: 2s - loss: 0.7507 - categorical_accuracy: 0.6813
12032/13806 [=========================>....] - ETA: 2s - loss: 0.7506 - categorical_accuracy: 0.6812
12160/13806 [=========================>....] - ETA: 2s - loss: 0.7508 - categorical_accuracy: 0.6813
12288/13806 [=========================>....] - ETA: 2s - loss: 0.7503 - categorical_accuracy: 0.6822
12416/13806 [=========================>....] - ETA: 1s - loss: 0.7494 - categorical_accuracy: 0.6827
12544/13806 [==========================>...] - ETA: 1s - loss: 0.7498 - categorical_accuracy: 0.6825
12672/13806 [==========================>...] - ETA: 1s - loss: 0.7498 - categorical_accuracy: 0.6828
12800/13806 [==========================>...] - ETA: 1s - loss: 0.7498 - categorical_accuracy: 0.6827
12928/13806 [===========================>..] - ETA: 1s - loss: 0.7495 - categorical_accuracy: 0.6830
13056/13806 [===========================>..] - ETA: 1s - loss: 0.7492 - categorical_accuracy: 0.6832
13184/13806 [===========================>..] - ETA: 0s - loss: 0.7495 - categorical_accuracy: 0.6828
13312/13806 [===========================>..] - ETA: 0s - loss: 0.7502 - categorical_accuracy: 0.6822
13440/13806 [============================>.] - ETA: 0s - loss: 0.7491 - categorical_accuracy: 0.6832
13568/13806 [============================>.] - ETA: 0s - loss: 0.7497 - categorical_accuracy: 0.6829
13696/13806 [============================>.] - ETA: 0s - loss: 0.7509 - categorical_accuracy: 0.6822
13806/13806 [==============================] - 20s 1ms/step - loss: 0.7507 - categorical_accuracy: 0.6822 - val_loss: 1.3105 - val_categorical_accuracy: 0.5142
2018-03-23 14:50:44.195604: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:04:00.0, compute capability: 6.1)
/home/michon/anaconda2/envs/py35/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.

Epoch 00006: val_categorical_accuracy did not improve
Epoch 00006: early stopping

Final evaluation

f1_score
 0.5103293676173448
accuracy_score
 0.5142478462557986

classification_report
              precision    recall  f1-score   support

        EGY       0.49      0.60      0.54       297
        GLF       0.57      0.32      0.41       259
        LAV       0.37      0.42      0.39       327
        MSA       0.61      0.70      0.65       280
        NOR       0.60      0.53      0.56       346

avg / total       0.52      0.51      0.51      1509


confusion_matrix
 [[177   7  66  26  21]
 [ 35  83  93  25  23]
 [ 67  31 137  39  53]
 [ 19   3  37 197  24]
 [ 66  22  41  35 182]]

Evaluation on best model

f1_score
 0.5217669102899573
accuracy_score
 0.5261762756792577

classification_report
              precision    recall  f1-score   support

        EGY       0.51      0.56      0.53       297
        GLF       0.44      0.47      0.46       259
        LAV       0.43      0.33      0.37       327
        MSA       0.62      0.72      0.66       280
        NOR       0.60      0.57      0.58       346

avg / total       0.52      0.53      0.52      1509


confusion_matrix
 [[167  35  42  29  24]
 [ 34 123  54  24  24]
 [ 57  65 107  39  59]
 [ 14  23  17 201  25]
 [ 57  34  27  32 196]]
Closing remaining open files:data/vardial2018/dataset.h5...done
############# train: DONE @ Fri Mar 23 14:50:47 CET 2018
