Loading data
Data Configurations loaded
Loading data
(48, 8)
(49, 8)
LAV    15
EGY    10
MSA     8
NOR     8
GLF     7
Name: Class, dtype: int64
LAV    14
NOR    11
MSA     9
GLF     8
EGY     7
Name: Class, dtype: int64
Loading vocabularies
Words
1786 1786
Phones
45 45
38 38
53 53
50 50
Generating ids
Preprocessing data
Padding character sequences
(48, 1018)
Padding phone sequences
(48, 1031) (48, 1365) (48, 1119) (48, 1268)
Turning labels in one-hot vectors
(48, 5)
Taking ready-made acoustic embeddings
(48, 600)
Padding character sequences
(49, 1018)
Padding phone sequences
(49, 1031) (49, 1365) (49, 1119) (49, 1268)
Turning labels in one-hot vectors
(49, 5)
Taking ready-made acoustic embeddings
(49, 600)
MultiInputCharCNN Configurations loaded
Building the model
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
sent_input (InputLayer)         (None, 1018)         0                                            
__________________________________________________________________________________________________
phone_CZ_input (InputLayer)     (None, 1031)         0                                            
__________________________________________________________________________________________________
phone_EN_input (InputLayer)     (None, 1365)         0                                            
__________________________________________________________________________________________________
phone_HU_input (InputLayer)     (None, 1119)         0                                            
__________________________________________________________________________________________________
phone_RU_input (InputLayer)     (None, 1268)         0                                            
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 1018, 64)     6464        sent_input[0][0]                 
__________________________________________________________________________________________________
embedding_2 (Embedding)         (None, 1031, 64)     2944        phone_CZ_input[0][0]             
__________________________________________________________________________________________________
embedding_3 (Embedding)         (None, 1365, 64)     2496        phone_EN_input[0][0]             
__________________________________________________________________________________________________
embedding_4 (Embedding)         (None, 1119, 64)     3456        phone_HU_input[0][0]             
__________________________________________________________________________________________________
embedding_5 (Embedding)         (None, 1268, 64)     3264        phone_RU_input[0][0]             
__________________________________________________________________________________________________
zero_padding1d_1 (ZeroPadding1D (None, 1022, 64)     0           embedding_1[0][0]                
__________________________________________________________________________________________________
zero_padding1d_2 (ZeroPadding1D (None, 1026, 64)     0           embedding_1[0][0]                
__________________________________________________________________________________________________
zero_padding1d_3 (ZeroPadding1D (None, 1035, 64)     0           embedding_2[0][0]                
__________________________________________________________________________________________________
zero_padding1d_4 (ZeroPadding1D (None, 1039, 64)     0           embedding_2[0][0]                
__________________________________________________________________________________________________
zero_padding1d_5 (ZeroPadding1D (None, 1369, 64)     0           embedding_3[0][0]                
__________________________________________________________________________________________________
zero_padding1d_6 (ZeroPadding1D (None, 1373, 64)     0           embedding_3[0][0]                
__________________________________________________________________________________________________
zero_padding1d_7 (ZeroPadding1D (None, 1123, 64)     0           embedding_4[0][0]                
__________________________________________________________________________________________________
zero_padding1d_8 (ZeroPadding1D (None, 1127, 64)     0           embedding_4[0][0]                
__________________________________________________________________________________________________
zero_padding1d_9 (ZeroPadding1D (None, 1272, 64)     0           embedding_5[0][0]                
__________________________________________________________________________________________________
zero_padding1d_10 (ZeroPadding1 (None, 1276, 64)     0           embedding_5[0][0]                
__________________________________________________________________________________________________
conv1d_1 (Conv1D)               (None, 1022, 16)     3088        zero_padding1d_1[0][0]           
__________________________________________________________________________________________________
conv1d_2 (Conv1D)               (None, 1026, 16)     5136        zero_padding1d_2[0][0]           
__________________________________________________________________________________________________
conv1d_3 (Conv1D)               (None, 1035, 16)     3088        zero_padding1d_3[0][0]           
__________________________________________________________________________________________________
conv1d_4 (Conv1D)               (None, 1039, 16)     5136        zero_padding1d_4[0][0]           
__________________________________________________________________________________________________
conv1d_5 (Conv1D)               (None, 1369, 16)     3088        zero_padding1d_5[0][0]           
__________________________________________________________________________________________________
conv1d_6 (Conv1D)               (None, 1373, 16)     5136        zero_padding1d_6[0][0]           
__________________________________________________________________________________________________
conv1d_7 (Conv1D)               (None, 1123, 16)     3088        zero_padding1d_7[0][0]           
__________________________________________________________________________________________________
conv1d_8 (Conv1D)               (None, 1127, 16)     5136        zero_padding1d_8[0][0]           
__________________________________________________________________________________________________
conv1d_9 (Conv1D)               (None, 1272, 16)     3088        zero_padding1d_9[0][0]           
__________________________________________________________________________________________________
conv1d_10 (Conv1D)              (None, 1276, 16)     5136        zero_padding1d_10[0][0]          
__________________________________________________________________________________________________
global_max_pooling1d_1 (GlobalM (None, 16)           0           conv1d_1[0][0]                   
__________________________________________________________________________________________________
global_max_pooling1d_2 (GlobalM (None, 16)           0           conv1d_2[0][0]                   
__________________________________________________________________________________________________
global_max_pooling1d_3 (GlobalM (None, 16)           0           conv1d_3[0][0]                   
__________________________________________________________________________________________________
global_max_pooling1d_4 (GlobalM (None, 16)           0           conv1d_4[0][0]                   
__________________________________________________________________________________________________
global_max_pooling1d_5 (GlobalM (None, 16)           0           conv1d_5[0][0]                   
__________________________________________________________________________________________________
global_max_pooling1d_6 (GlobalM (None, 16)           0           conv1d_6[0][0]                   
__________________________________________________________________________________________________
global_max_pooling1d_7 (GlobalM (None, 16)           0           conv1d_7[0][0]                   
__________________________________________________________________________________________________
global_max_pooling1d_8 (GlobalM (None, 16)           0           conv1d_8[0][0]                   
__________________________________________________________________________________________________
global_max_pooling1d_9 (GlobalM (None, 16)           0           conv1d_9[0][0]                   
__________________________________________________________________________________________________
global_max_pooling1d_10 (Global (None, 16)           0           conv1d_10[0][0]                  
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 160)          0           global_max_pooling1d_1[0][0]     
                                                                 global_max_pooling1d_2[0][0]     
                                                                 global_max_pooling1d_3[0][0]     
                                                                 global_max_pooling1d_4[0][0]     
                                                                 global_max_pooling1d_5[0][0]     
                                                                 global_max_pooling1d_6[0][0]     
                                                                 global_max_pooling1d_7[0][0]     
                                                                 global_max_pooling1d_8[0][0]     
                                                                 global_max_pooling1d_9[0][0]     
                                                                 global_max_pooling1d_10[0][0]    
__________________________________________________________________________________________________
embed_input (InputLayer)        (None, 600)          0                                            
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 760)          0           concatenate_1[0][0]              
                                                                 embed_input[0][0]                
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 128)          97408       concatenate_2[0][0]              
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 128)          0           dense_1[0][0]                    
__________________________________________________________________________________________________
l_out (Dense)                   (None, 5)            645         dropout_1[0][0]                  
==================================================================================================
Total params: 157,797
Trainable params: 157,797
Non-trainable params: 0
__________________________________________________________________________________________________
Training Configurations loaded
Training the model
no checkpoints available !
Train on 48 samples, validate on 49 samples
Epoch 1/15

Epoch 00001: val_categorical_accuracy improved from -inf to 0.32653, saving model to results/vardial2018-completesample/multi_input_with_dropout/model_weights.hdf5

48/48 [==============================] - 2s 42ms/step - loss: 3.8837 - categorical_accuracy: 0.0417 - val_loss: 3.6882 - val_categorical_accuracy: 0.3265
Epoch 2/15

Epoch 00002: val_categorical_accuracy improved from 0.32653 to 0.38776, saving model to results/vardial2018-completesample/multi_input_with_dropout/model_weights.hdf5

48/48 [==============================] - 1s 21ms/step - loss: 3.6037 - categorical_accuracy: 0.3542 - val_loss: 3.5725 - val_categorical_accuracy: 0.3878
Epoch 3/15

Epoch 00003: val_categorical_accuracy improved from 0.38776 to 0.42857, saving model to results/vardial2018-completesample/multi_input_with_dropout/model_weights.hdf5

48/48 [==============================] - 1s 21ms/step - loss: 3.3779 - categorical_accuracy: 0.6250 - val_loss: 3.4765 - val_categorical_accuracy: 0.4286
Epoch 4/15

Epoch 00004: val_categorical_accuracy improved from 0.42857 to 0.46939, saving model to results/vardial2018-completesample/multi_input_with_dropout/model_weights.hdf5

48/48 [==============================] - 1s 21ms/step - loss: 3.2030 - categorical_accuracy: 0.8333 - val_loss: 3.3960 - val_categorical_accuracy: 0.4694
Epoch 5/15

Epoch 00005: val_categorical_accuracy did not improve

48/48 [==============================] - 1s 20ms/step - loss: 3.0244 - categorical_accuracy: 0.9375 - val_loss: 3.3152 - val_categorical_accuracy: 0.4694
Epoch 6/15

Epoch 00006: val_categorical_accuracy improved from 0.46939 to 0.51020, saving model to results/vardial2018-completesample/multi_input_with_dropout/model_weights.hdf5

48/48 [==============================] - 1s 21ms/step - loss: 2.8731 - categorical_accuracy: 0.9792 - val_loss: 3.2364 - val_categorical_accuracy: 0.5102
Epoch 7/15

Epoch 00007: val_categorical_accuracy improved from 0.51020 to 0.53061, saving model to results/vardial2018-completesample/multi_input_with_dropout/model_weights.hdf5

48/48 [==============================] - 1s 21ms/step - loss: 2.7495 - categorical_accuracy: 1.0000 - val_loss: 3.1592 - val_categorical_accuracy: 0.5306
Epoch 8/15

Epoch 00008: val_categorical_accuracy improved from 0.53061 to 0.55102, saving model to results/vardial2018-completesample/multi_input_with_dropout/model_weights.hdf5

48/48 [==============================] - 1s 21ms/step - loss: 2.6055 - categorical_accuracy: 1.0000 - val_loss: 3.0879 - val_categorical_accuracy: 0.5510
Epoch 9/15

Epoch 00009: val_categorical_accuracy improved from 0.55102 to 0.61224, saving model to results/vardial2018-completesample/multi_input_with_dropout/model_weights.hdf5

48/48 [==============================] - 1s 21ms/step - loss: 2.5067 - categorical_accuracy: 1.0000 - val_loss: 3.0222 - val_categorical_accuracy: 0.6122
Epoch 10/15

Epoch 00010: val_categorical_accuracy did not improve

48/48 [==============================] - 1s 20ms/step - loss: 2.3926 - categorical_accuracy: 1.0000 - val_loss: 2.9619 - val_categorical_accuracy: 0.6122
Epoch 11/15

Epoch 00011: val_categorical_accuracy did not improve

48/48 [==============================] - 1s 20ms/step - loss: 2.2668 - categorical_accuracy: 1.0000 - val_loss: 2.9065 - val_categorical_accuracy: 0.6122
Epoch 12/15

Epoch 00012: val_categorical_accuracy did not improve

48/48 [==============================] - 1s 20ms/step - loss: 2.1885 - categorical_accuracy: 1.0000 - val_loss: 2.8556 - val_categorical_accuracy: 0.6122
Epoch 13/15

Epoch 00013: val_categorical_accuracy did not improve

48/48 [==============================] - 1s 21ms/step - loss: 2.0982 - categorical_accuracy: 1.0000 - val_loss: 2.8090 - val_categorical_accuracy: 0.6122
Epoch 14/15

Epoch 00014: val_categorical_accuracy did not improve

48/48 [==============================] - 1s 21ms/step - loss: 2.0082 - categorical_accuracy: 1.0000 - val_loss: 2.7640 - val_categorical_accuracy: 0.6122
Epoch 00014: early stopping

Final evaluation

f1_score
 0.5975867269984917
accuracy_score
 0.6122448979591837

classification_report
              precision    recall  f1-score   support

        EGY       0.60      0.43      0.50         7
        GLF       0.80      0.50      0.62         8
        LAV       0.55      0.86      0.67        14
        MSA       0.75      0.67      0.71         9
        NOR       0.56      0.45      0.50        11

avg / total       0.63      0.61      0.60        49


confusion_matrix
 [[ 3  1  2  0  1]
 [ 1  4  3  0  0]
 [ 0  0 12  1  1]
 [ 0  0  1  6  2]
 [ 1  0  4  1  5]]

Evaluation on best model

f1_score
 0.5975867269984917
accuracy_score
 0.6122448979591837

classification_report
              precision    recall  f1-score   support

        EGY       0.60      0.43      0.50         7
        GLF       0.80      0.50      0.62         8
        LAV       0.55      0.86      0.67        14
        MSA       0.75      0.67      0.71         9
        NOR       0.56      0.45      0.50        11

avg / total       0.63      0.61      0.60        49


confusion_matrix
 [[ 3  1  2  0  1]
 [ 1  4  3  0  0]
 [ 0  0 12  1  1]
 [ 0  0  1  6  2]
 [ 1  0  4  1  5]]
